{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import re\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded knowledge base (619144 characters)\n"
     ]
    }
   ],
   "source": [
    "with open(\"knowledge_base.md\", \"r\", encoding=\"utf-8\") as f:\n",
    "    full_kb = f.read()\n",
    "\n",
    "print(f\"Loaded knowledge base ({len(full_kb)} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1345 total questions\n"
     ]
    }
   ],
   "source": [
    "# Load generated questions\n",
    "with open(\"generated_questions_smaller_set.json\", \"r\") as f:\n",
    "    all_questions = json.load(f)\n",
    "\n",
    "print(f\"Loaded {sum(len(ctx['questions']) for ctx in all_questions)} total questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load generated questions\n",
    "with open(\"merged_contexts.json\", \"r\") as f:\n",
    "    contexts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 89 total contexts\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(contexts)} total contexts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = \"API KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gemini\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    # model=\"gemini-2.0-flash\",\n",
    "    model=\"gemini-2.0-pro-exp-02-05\",\n",
    "    temperature=0.1,\n",
    "    max_retries=3,\n",
    "    top_p = 0.95,\n",
    "    timeout = 360,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"# You are TurboML Expert - a senior ML engineer specializing in real-time machine learning with TurboML. You answer technical questions using ONLY the documentation provided below.\n",
    "\n",
    "Documentation:\n",
    "{full_kb}\n",
    "\n",
    "## Response Requirements\n",
    "\n",
    "### 1. Content Accuracy & Documentation Fidelity\n",
    "- Use EXACT class/method names from docs (e.g., `FeatureEngineering.create_sql_features()`)\n",
    "- Include verbatim code snippets when available\n",
    "- Reference specific sections like `Feature Engineering/UDAF` or `BYOM/ONNX`\n",
    "- Cite documentation with section headers in brackets (e.g., [Feature Engineering - Python UDFs])\n",
    "- For code references, include source: [File: feature_engineering.py]\n",
    "\n",
    "### 2. Real-Time ML Focus\n",
    "- Emphasize TurboML's streaming capabilities: `OnlineDataset`, windowed aggregates, continuous training\n",
    "- Highlight key differentiators: Ibis integration, ONNX deployment, Python UDF support\n",
    "- Use official syntax and parameter names exactly as documented\n",
    "\n",
    "### 3. Response Structure\n",
    "- **Problem Analysis** (2 to 4 sentences)\n",
    "- **Step-by-Step Solution** with implementation steps\n",
    "- **Code Implementation** with properly formatted examples\n",
    "- **Common Pitfalls & Debugging Tips**\n",
    "- For conceptual questions, compare 2-3 approaches with pros/cons as bullet points\n",
    "- For troubleshooting, identify error scenario, root cause, and provide before/after code\n",
    "\n",
    "### 4. Knowledge Boundaries & Anti-Hallucination\n",
    "- If the answer is not in the provided documentation context, clearly state: \"I cannot answer this question based on the provided context.\"\n",
    "- DO NOT attempt to answer questions beyond what's explicitly in the documentation\n",
    "- NO assumptions or extrapolations beyond the documentation\n",
    "- NO generic advice unless specifically mentioned in Context\n",
    "- If uncertain about any aspect, acknowledge limitations instead of guessing\n",
    "\n",
    "### 5. Documentation Navigation\n",
    "- Guide users to relevant documentation sections\n",
    "- Explain parameter meanings and default values from documentation\n",
    "- When referencing implementation steps, follow TurboML's workflow: Data ingestion → Feature engineering → Model training → Deployment → Monitoring\n",
    "\"\"\"),\n",
    "    (\"human\", \"Question: {question}\\n\\nGenerate comprehensive answer:\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: checkpoints\\current_checkpoint.json\n",
      "Completed 1060 answers so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing context groups:  79%|███████▊  | 70/89 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1070 answers processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing context groups:  80%|███████▉  | 71/89 [07:01<2:06:34, 421.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1080 answers processed)\n",
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1090 answers processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing context groups:  81%|████████  | 72/89 [13:35<1:54:54, 405.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1100 answers processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing context groups:  82%|████████▏ | 73/89 [20:34<1:49:45, 411.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1110 answers processed)\n",
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1120 answers processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing context groups:  83%|████████▎ | 74/89 [27:52<1:45:30, 422.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1130 answers processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing context groups:  84%|████████▍ | 75/89 [35:25<1:41:00, 432.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1140 answers processed)\n",
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1150 answers processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing context groups:  85%|████████▌ | 76/89 [44:32<1:42:15, 471.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1160 answers processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing context groups:  87%|████████▋ | 77/89 [52:11<1:33:30, 467.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1170 answers processed)\n",
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1180 answers processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing context groups:  88%|████████▊ | 78/89 [1:01:07<1:29:43, 489.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1190 answers processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing context groups:  89%|████████▉ | 79/89 [1:09:28<1:22:10, 493.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1200 answers processed)\n",
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1210 answers processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing context groups:  90%|████████▉ | 80/89 [1:16:13<1:09:53, 465.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1220 answers processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing context groups:  91%|█████████ | 81/89 [1:23:06<59:57, 449.69s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1230 answers processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing context groups:  92%|█████████▏| 82/89 [1:31:06<53:32, 459.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1240 answers processed)\n",
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1250 answers processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing context groups:  93%|█████████▎| 83/89 [1:38:59<46:18, 463.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1260 answers processed)\n",
      "\n",
      "Error processing question: Code question missing code block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing context groups:  94%|█████████▍| 84/89 [1:45:56<37:25, 449.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1270 answers processed)\n",
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1280 answers processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing context groups:  96%|█████████▌| 85/89 [1:52:32<28:53, 433.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1290 answers processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing context groups:  97%|█████████▋| 86/89 [2:00:27<22:17, 445.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1300 answers processed)\n",
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1310 answers processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing context groups:  98%|█████████▊| 87/89 [2:07:35<14:40, 440.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1320 answers processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing context groups:  99%|█████████▉| 88/89 [2:15:20<07:27, 447.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1330 answers processed)\n",
      "\n",
      "Checkpoint updated at checkpoints\\current_checkpoint.json (1340 answers processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing context groups: 100%|██████████| 89/89 [2:21:39<00:00, 447.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer Generation Complete!\n",
      "Successfully answered: 1343 questions\n",
      "Failed questions: 10\n",
      "Full results saved to: full_answers.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create checkpoints directory if it doesn't exist\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# Single checkpoint filename that will be overwritten\n",
    "checkpoint_filename = os.path.join(checkpoint_dir, \"current_checkpoint.json\")\n",
    "\n",
    "# Check if there's a previous checkpoint to resume from\n",
    "if os.path.exists(checkpoint_filename):\n",
    "    with open(checkpoint_filename, \"r\") as f:\n",
    "        checkpoint_data = json.load(f)\n",
    "        answers = checkpoint_data[\"answers\"]\n",
    "        failed_questions = checkpoint_data[\"failed_questions\"]\n",
    "        start_group_idx = checkpoint_data.get(\"next_group_idx\", 0)\n",
    "        answer_count = checkpoint_data.get(\"answer_count\", 0)\n",
    "    print(f\"Resuming from checkpoint: {checkpoint_filename}\")\n",
    "    print(f\"Completed {answer_count} answers so far\")\n",
    "else:\n",
    "    answers = []\n",
    "    failed_questions = []\n",
    "    start_group_idx = 0\n",
    "    answer_count = 0\n",
    "\n",
    "# Process all questions\n",
    "total_answer_count = answer_count\n",
    "checkpoint_frequency = 10\n",
    "\n",
    "for ctx_idx, ctx_group in enumerate(tqdm(all_questions[start_group_idx:], desc=\"Processing context groups\", initial=start_group_idx, total=len(all_questions))):\n",
    "    global_ctx_idx = ctx_idx + start_group_idx\n",
    "    ctx_answers = []\n",
    "    \n",
    "    for q_idx, question in enumerate(ctx_group[\"questions\"]):\n",
    "        try:\n",
    "            # Generate answer\n",
    "            response = llm.invoke(\n",
    "                answer_prompt.format(\n",
    "                    full_kb=full_kb,\n",
    "                    question=question\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Extract referenced sections\n",
    "            sections_found = list(set(re.findall(\n",
    "                r\"\\[(.*?)\\]\", \n",
    "                response.content\n",
    "            )))\n",
    "            \n",
    "            # Validate code presence if question requires it\n",
    "            if any(q_word in question.lower() for q_word in [\"code\", \"implement\", \"write\"]):\n",
    "                if \"```python\" not in response.content:\n",
    "                    raise ValueError(\"Code question missing code block\")\n",
    "                \n",
    "            # Store answer\n",
    "            ctx_answers.append({\n",
    "                \"question\": question,\n",
    "                \"answer\": str(response.content).strip(),\n",
    "                \"referenced_sections\": sections_found,\n",
    "                \"token_usage\": response.response_metadata.get(\"token_usage\", {}),\n",
    "                \"safety_ratings\": response.response_metadata.get(\"safety_ratings\", []),\n",
    "                \"generation_time\": time.time_ns()\n",
    "            })\n",
    "            \n",
    "            total_answer_count += 1\n",
    "            \n",
    "            # Create checkpoint at every 10th answer\n",
    "            if total_answer_count % checkpoint_frequency == 0:\n",
    "                # Create current context data\n",
    "                current_ctx_data = {\n",
    "                    \"context_id\": ctx_group[\"context_id\"],\n",
    "                    \"base_sections\": ctx_group[\"base_sections\"],\n",
    "                    \"answers\": ctx_answers.copy()\n",
    "                }\n",
    "                \n",
    "                # Add to answers list\n",
    "                answers.append(current_ctx_data)\n",
    "                \n",
    "                # Create checkpoint file (overwriting previous one)\n",
    "                checkpoint_data = {\n",
    "                    \"answers\": answers,\n",
    "                    \"failed_questions\": failed_questions,\n",
    "                    \"next_group_idx\": global_ctx_idx,\n",
    "                    \"next_question_idx\": q_idx + 1,\n",
    "                    \"answer_count\": total_answer_count,\n",
    "                    \"timestamp\": time.time_ns()\n",
    "                }\n",
    "                \n",
    "                with open(checkpoint_filename, \"w\") as f:\n",
    "                    json.dump(checkpoint_data, f, indent=2)\n",
    "                \n",
    "                print(f\"\\nCheckpoint updated at {checkpoint_filename} ({total_answer_count} answers processed)\")\n",
    "                \n",
    "                # Reset context answers for next batch\n",
    "                ctx_answers = []\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(11)\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_questions.append({\n",
    "                \"question\": question,\n",
    "                \"error\": str(e),\n",
    "                \"context_group\": ctx_group[\"context_id\"]\n",
    "            })\n",
    "            print(f\"\\nError processing question: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # If we have answers in the current context group that weren't checkpointed, add them now\n",
    "    if ctx_answers:\n",
    "        current_ctx_data = {\n",
    "            \"context_id\": ctx_group[\"context_id\"],\n",
    "            \"base_sections\": ctx_group[\"base_sections\"],\n",
    "            \"answers\": ctx_answers\n",
    "        }\n",
    "        answers.append(current_ctx_data)\n",
    "        \n",
    "        # Update checkpoint with these answers too\n",
    "        checkpoint_data = {\n",
    "            \"answers\": answers,\n",
    "            \"failed_questions\": failed_questions,\n",
    "            \"next_group_idx\": global_ctx_idx + 1,  # Move to next group\n",
    "            \"next_question_idx\": 0,\n",
    "            \"answer_count\": total_answer_count,\n",
    "            \"timestamp\": time.time_ns()\n",
    "        }\n",
    "        \n",
    "        with open(checkpoint_filename, \"w\") as f:\n",
    "            json.dump(checkpoint_data, f, indent=2)\n",
    "\n",
    "# Save final results\n",
    "with open(\"full_answers.json\", \"w\") as f:\n",
    "    json.dump(answers, f, indent=2)\n",
    "\n",
    "# Save failed questions separately\n",
    "if failed_questions:\n",
    "    with open(\"failed_questions.json\", \"w\") as f:\n",
    "        json.dump(failed_questions, f, indent=2)\n",
    "\n",
    "print(f\"\"\"\n",
    "Answer Generation Complete!\n",
    "Successfully answered: {total_answer_count} questions\n",
    "Failed questions: {len(failed_questions)}\n",
    "Full results saved to: full_answers.json\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
