{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU faiss-cpu langchain langchain_community langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load Prepared Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 89 document chunks.\n"
     ]
    }
   ],
   "source": [
    "# Load prepared chunks from JSON file\n",
    "with open(\"chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunk_data = json.load(f)\n",
    "\n",
    "# Convert to Document format with metadata\n",
    "documents = [\n",
    "    Document(page_content=item[\"content\"], metadata={\"section\": item[\"section\"]})\n",
    "    for item in chunk_data\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(documents)} document chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load Summary Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary document loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load summary content\n",
    "with open(\"summary.md\", \"r\") as f:\n",
    "    summary_content = f.read()\n",
    "\n",
    "# Create a Document object for the summary\n",
    "summary_doc = Document(\n",
    "    page_content=summary_content,\n",
    "    metadata={\"section\": \"summary\", \"source\": \"summary.md\"}\n",
    ")\n",
    "\n",
    "print(\"Summary document loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Initialize Embeddings & FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_686/1907406924.py:5: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceBgeEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized FAISS index with dimension: 1024\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# Determine the embedding dimension\n",
    "dimension = len(embeddings.embed_query(\"test\"))\n",
    "\n",
    "# Initialize FAISS index\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "print(f\"Initialized FAISS index with dimension: {dimension}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create Vector Store with Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e441ae97-9f6f-4f2d-b41c-319ef2d35716',\n",
       " '6fbe16bc-58ac-4810-90d2-8936e8a4fc0a',\n",
       " '084133a2-8e9e-4119-a3b7-f6a764a6a43a',\n",
       " '81adb2c4-2000-44c5-af45-434b1f64db00',\n",
       " 'bfdb3d83-d16b-4203-9bb4-b2d8cf4507bc',\n",
       " '271fefbb-1c2e-42cb-ba7c-f7734435a669',\n",
       " 'e9bd0197-488a-4ca5-8736-7db47acf92b0',\n",
       " '56b88d95-383c-473b-b29b-d77d3f81d589',\n",
       " '2ae1e3c1-7100-4fc8-9419-9feb2ca07ad3',\n",
       " 'bbed0952-9992-4f76-80e4-c7d1af667922',\n",
       " '6db022b2-ce09-4ed9-ba84-344e4ec59cc9',\n",
       " '0de3c32a-c1d7-4d25-8274-9b1d7f3c8dfa',\n",
       " '40c60786-c9f5-452e-97c3-e62e2fcaad98',\n",
       " '49f27c15-5f69-4e22-b85b-bab3c1a95f3d',\n",
       " '070029cd-63a3-46eb-9925-99bfea59cdab',\n",
       " 'c5016ad5-abee-426b-a6d0-3f935960453e',\n",
       " 'c45e1051-0e5c-4da7-95c8-0073d8851610',\n",
       " '94809f2d-d1ce-4bd7-b480-4254c4452b9d',\n",
       " '7a72489b-119c-44ba-9016-d1cb14218697',\n",
       " 'd13e2dbd-736f-40ca-a7a0-8bfb098762c1',\n",
       " '81fbd1ee-a294-4eac-aeeb-d3c9342199eb',\n",
       " '9b95367d-3163-4dc7-a868-9d4bda86f37d',\n",
       " 'b1c95a3a-5524-4658-a79a-be2303ae4224',\n",
       " 'cfc5434c-5e4b-4188-9418-7c34b0dab8c6',\n",
       " 'd9b9beec-d82a-4691-a3b5-cb3e0a09b9d4',\n",
       " '6e67c9bd-e55d-4a59-9e48-a62d4346067a',\n",
       " '18614981-a868-434b-b0c8-8de8548f17b0',\n",
       " '2f86c8eb-f616-4c25-95f9-5d049fae30f9',\n",
       " 'e7d7c208-d665-454c-a3ec-0becb8203c00',\n",
       " 'c1b5c2c1-0365-4652-a360-019d01ebc20f',\n",
       " '4d855b3a-d879-4ef0-988f-d3376c74e417',\n",
       " '58ec2849-f66a-42b7-a995-ac6b23f65d9c',\n",
       " '0c06aada-6ac6-457f-891a-77bf2f0d9064',\n",
       " '62921dfe-3e3a-47a0-a6a9-9522952020e3',\n",
       " '7bf9b46a-02b5-4086-9f2a-83747f52cfde',\n",
       " '28172d61-7fa9-4d6a-aae9-6eec0441198f',\n",
       " '67d5456c-ea49-4b2a-86d9-26b5d619e14b',\n",
       " 'd6cd1644-2493-4de8-9843-c5e71e1be8d0',\n",
       " '83875d3e-bdaf-4de3-978c-8839531453ee',\n",
       " 'beee1c27-00a0-4d4d-a45f-0ab6dbf01be4',\n",
       " 'beac4414-dd61-4000-b07a-3caad1fcb496',\n",
       " '1626a865-3b51-4955-9bb8-c25295470572',\n",
       " '99be5443-77a5-437c-9d23-2579aed80617',\n",
       " '51b9beae-bba6-4fa3-9353-401a18e5b835',\n",
       " 'cec42d2a-58a3-4a94-b484-5e78e2942cc7',\n",
       " '555fee75-e3ad-4e4b-9483-e9bc0218df9d',\n",
       " '116fc8fb-19b5-410d-8c45-250767006e36',\n",
       " 'fdba2f03-ef22-4d51-a5b9-2769d767fbd7',\n",
       " '644ef490-8526-4ebb-b658-580592c75100',\n",
       " 'a46c8f7d-c775-43ff-9777-b0735ba077b2',\n",
       " '212a64fb-95ae-43e6-9701-5704407c461a',\n",
       " 'fbc807dd-623c-4f49-ac1f-eca3b0dd2c9a',\n",
       " 'db1bd4cb-2f7d-47f1-9718-f9c1e355c5ea',\n",
       " '266cc994-a8f3-45a1-8f1e-1f273b7b0102',\n",
       " '41193ace-ea48-443c-9cd6-9f7f5fa5c25d',\n",
       " '4c88a91d-abd4-454e-81a9-25e256a8e5cb',\n",
       " '22a0e755-7ce9-4309-a1ab-203e251481e6',\n",
       " 'b0440a68-75a1-47cf-a0eb-04177b6f1f59',\n",
       " '20460379-0479-4021-a119-69a2e7c2203b',\n",
       " '6c21e939-1ba1-4927-97b8-d44e8f5f937a',\n",
       " 'ad85ff1a-c177-471a-9fe0-7ac27cef5dfd',\n",
       " '2a6eda39-752c-4f26-9a8b-c9bd02d65180',\n",
       " '3d110656-2644-4a78-97fd-626445a2bc78',\n",
       " 'a9c1c1e3-a780-4219-956b-82cbd68cddc0',\n",
       " '7f5ed23f-fa74-435d-8d14-138edf3c171b',\n",
       " '7fd35487-b519-4c25-a14b-5b89a2fec248',\n",
       " '70d8b6d4-9cd7-4802-82cc-35c41d6aca6b',\n",
       " '3e0c7a95-cef4-4fdc-a83c-2773fb4b29ae',\n",
       " '05375f02-351d-483b-8052-991b3488b5d6',\n",
       " 'a68220a5-e2f2-4f18-8768-153e9cc7426e',\n",
       " '43fa687b-4a61-4dfb-a397-c74a342acdbf',\n",
       " 'f46c0934-e86d-474b-9190-34b27c791a65',\n",
       " '7a253ac1-c3e9-47a0-8f8f-967b95427096',\n",
       " '4d61835f-81cd-417b-bb3b-295ade11f940',\n",
       " 'f8e6e5e6-a19f-4ee5-856d-8a24f83644a0',\n",
       " '329372f4-9aef-446c-82fa-6880119dfcc1',\n",
       " 'a1d17b89-baf3-4f5f-9948-9a1aa0051d51',\n",
       " 'cd1e2b5b-d40c-43b9-8033-0e00fb54dc31',\n",
       " '4aa3d6a9-c9ba-4f0f-b97a-319ff9c1be9d',\n",
       " 'a92c6874-ded0-4add-a2e2-bce9a39c0d7e',\n",
       " 'e359928b-1faa-4f76-b155-04372421ad9d',\n",
       " 'e2315021-76e5-4d58-b83d-54f447f238fb',\n",
       " '6526dd6d-7852-4786-8279-1e30a301c96a',\n",
       " 'ad842bcf-5d4e-47e0-9443-fc4a3a94d8ff',\n",
       " '089b96bd-063d-4621-a696-dfac942d7a82',\n",
       " '51435a74-cd7e-431c-b89d-68f325599ca4',\n",
       " '1d52ff5a-38f8-487a-bc16-685cdd7b6001',\n",
       " '8acfedd6-9e37-4b90-bb50-26dbbe9bfca2',\n",
       " '80567766-324a-4d80-adfb-4c0f48aa02f8']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine summary with all document chunks\n",
    "all_docs = documents\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "vector_store.add_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS vector store created with 89 documents.\n"
     ]
    }
   ],
   "source": [
    "print(f\"FAISS vector store created with {len(all_docs)} documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "5. Generate Merged Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'section': 'What is TurboML?'}, page_content='# What is TurboML?\\n@ TurboML - page_link: https://docs.turboml.com/intro/\\n<page_content>\\nIntroduction  \\nTurboML is a machine learning platform that’s reinvented for real-time. What does that mean? All the steps in the ML lifecycle, from data ingestion, to feature engineering, to ML modelling to post deployment steps like monitoring, are all designed so that in addition to batch data, they can also handle real-time data.  \\n## Data Ingestion [Permalink for this section](https://docs.turboml.com/intro/\\\\#data-ingestion)  \\nThe first step is to bring your data to the TurboML platform. There are two major ways to ingest your data. Pull-based and Push-based.  \\n### Pull-based ingestion [Permalink for this section](https://docs.turboml.com/intro/\\\\#pull-based-ingestion)  \\nWith this approach, you use TurboML’s prebuilt connectors to connect to your data source. The connectors will continuously pull data from your data source, and ingest it into TurboML.  \\n### Push-based ingestion [Permalink for this section](https://docs.turboml.com/intro/\\\\#push-based-ingestion)  \\nSometimes, you might not want to send data via an intermediate data source, but rather directly send the data. Push-based ingestion can be used for this, where data can be send either via REST API calls, or using more performant client SDKs. Here’s an example with a Pandas DataFrame  \\n```transactions = tb.PandasDataset(dataset_name=\"transactions\",dataframe=df, upload=True)\\ntransactions.configure_dataset(key_field=\"index\")\\n```  \\n## Feature Engineering [Permalink for this section](https://docs.turboml.com/intro/\\\\#feature-engineering)  \\nFeature engineering is perhaps the most important step for data scientists. TurboML provides several different interfaces to define features. We’ve designed the feature engineering experience in a way so that after you’ve defined a feature, you can see that feature computed for your local data. This should help debug and iterate faster. Once you’re confident about a feature definition, you can deploy it where it’ll be continuously computed on the real-time data. Once deployed, these features are automatically computed on the streaming data. And we have retrieval APIs to compute it for ad-hoc queries.  \\n### SQL Features [Permalink for this section](https://docs.turboml.com/intro/\\\\#sql-features)  \\nWriting SQL queries is one of the most common way to define ML features. TurboML supports writing arbitrary SQL expressions to enable such features. Here’s an example with a simple SQL feature.  \\n```transactions.feature_engineering.create_sql_features(\\nsql_definition=\\'\"transactionAmount\" + \"localHour\"\\',\\nnew_feature_name=\"my_sql_feat\",\\n)\\n```  \\nNotice that the column names are in quotes.  \\nAnd here’s a more complicated example  \\n```transactions.feature_engineering.create_sql_features(\\nsql_definition=\\'CASE WHEN \"paymentBillingCountryCode\" <> \"ipCountryCode\" THEN 1 ELSE 0 END \\',\\nnew_feature_name=\"country_code_match\",\\n)\\n```  \\n### Aggregate Features [Permalink for this section](https://docs.turboml.com/intro/\\\\#aggregate-features)  \\nA common template for real-time features is aggregating some value over some time window. To define such time-windowed aggregations, you first need to register a timestamp column for your dataset. This can be done as follows,  \\n```transactions.feature_engineering.register_timestamp(column_name=\"timestamp\", format_type=\"epoch_seconds\")\\n```  \\nThe supported formats can be found out using  \\n```tb.get_timestamp_formats()\\n```  \\nOnce the timestamp is registered, we can create a feature using  \\n```transactions.feature_engineering.create_aggregate_features(\\ncolumn_to_operate=\"transactionAmount\",\\ncolumn_to_group=\"accountID\",\\noperation=\"SUM\",\\nnew_feature_name=\"my_sum_feat\",\\ntimestamp_column=\"timestamp\",\\nwindow_duration=24,\\nwindow_unit=\"hours\"\\n)\\n```  \\n### User Defined Features [Permalink for this section](https://docs.turboml.com/intro/\\\\#user-defined-features)  \\nWe understand why data scientists love Python - the simplicity, the ecosystem - is unmatchable. Guess what? You can use native Python, importing any library, [to define features](https://docs.turboml.com/feature_engineering/udf/)!  \\n### IBIS Features [Permalink for this section](https://docs.turboml.com/intro/\\\\#ibis-features)  \\nFor streaming features that are more complex than just windowed aggregations, can be defined using the [ibis interface](https://docs.turboml.com/feature_engineering/advanced/ibis_feature_engineering/). They can then be executed using Apache Flink or RisingWave.  \\n### Feature Retrieval [Permalink for this section](https://docs.turboml.com/intro/\\\\#feature-retrieval)  \\nAs mentioned before, once deployed, the feature computation is automatically added to the real-time streaming pipeline. However, feature values can also be retrieved on ad-hoc data using the retrieval API. Here’s an example  \\n```features = tb.retrieve_features(\"transactions\", query_df)\\n```  \\n## ML Modelling - Basic concepts [Permalink for this section](https://docs.turboml.com/intro/\\\\#ml-modelling---basic-concepts)  \\n### Inputs and Labels [Permalink for this section](https://docs.turboml.com/intro/\\\\#inputs-and-labels)  \\nFor each model, we need to specify the Inputs and the Labels.  \\n### Types of fields [Permalink for this section](https://docs.turboml.com/intro/\\\\#types-of-fields)  \\nDifferent models can accept different types of input fields. The supported types of fields are, numeric, categoric, time series, text, and image.  \\n### TurboML algorithms [Permalink for this section](https://docs.turboml.com/intro/\\\\#turboml-algorithms)  \\nTurboML provides several algorithms out of the box. These algorithms are optimized for online predictions and learning, and have been tested on real-world settings.  \\n```model = tb.HoeffdingTreeClassifier(n_classes=2)\\n```  \\n### Pytorch/TensorFlow/Scikit-learn [Permalink for this section](https://docs.turboml.com/intro/\\\\#pytorchtensorflowscikit-learn)  \\nWe use ONNX to deploy trained models from [Pytorch](https://docs.turboml.com/byo_models/onnx_pytorch/), [TensorFlow](https://docs.turboml.com/byo_models/onnx_tensorflow/), [Scikit-learn](https://docs.turboml.com/byo_models/onnx_sklearn/) or other ONNX compatible frameworks. Example for these three frameworks can be found in the following notebooks.  \\nNote: These models are static, and are not updated automatically.  \\n### Python [Permalink for this section](https://docs.turboml.com/intro/\\\\#python)  \\nTurboML also supports writing arbitrary Python code to define your own algorithms, including any libraries. To add your own algorithms, you need to define a Python class with 2 methods defined with the following signature:  \\n```class Model:\\ndef learn_one(self, features, label):\\npass\\n\\ndef predict_one(self, features, output_data):\\npass\\n```  \\nExamples of using an incremental learning algorithm, as well as a batch-like algorithm, can be found [here](https://docs.turboml.com/wyo_models/native_python_model/) from the river library.  \\n### Combining models [Permalink for this section](https://docs.turboml.com/intro/\\\\#combining-models)  \\nModels can also be combined to create other models, e.g. ensembles. An example of an ensemble model is as follows  \\n```model = tb.LeveragingBaggingClassifier(n_classes=2, base_model = tb.HoeffdingTreeClassifier(n_classes=2))\\n```  \\nPreprocessors can also be chained and applied in a similar manner. E.g.  \\n```model = tb.MinMaxPreProcessor(base_model = model)\\n```  \\n## Model Training [Permalink for this section](https://docs.turboml.com/intro/\\\\#model-training)  \\nOnce we’ve defined a model, it can be trained in different ways.  \\n### Batch way [Permalink for this section](https://docs.turboml.com/intro/\\\\#batch-way)  \\nThe simplest way is to train the model in a batch way. This is similar to sklearn’s fit() method. However, internally the training is performed in an incremental manner. So, you can update an already trained model on some new data too. Here’s an example  \\n```old_trained_model = model.learn(old_features, old_label)\\nnew_trained_model = old_trained_model.learn(new_features, new_label)\\n```  \\nAny trained copy of the model can be then deployed to production.  \\n```deployed_model = new_trained_model.deploy(name = \"deployment_name\", input=features, labels=label, predict_only=True)\\n```  \\nSince this is a trained model, we can also invoke this model in a batch way to get predictions without actually deploying the mode.  \\n```outputs = new_trained_model.predict(query_features)\\n```  \\n### Streaming way [Permalink for this section](https://docs.turboml.com/intro/\\\\#streaming-way)  \\nThis is where the model, after deployment, is continuously trained on new data. The user can choose how to update the model. The choices are online updates (where the model is updated on every new datapoint), or trigger-based updates which can be volume-based, time-based, performance-based or drift-based. The default option is online updates.  \\n```deployed_model = model.deploy(name = \"deployment_name\", input=features, labels=label)\\n```  \\n## Deployment and MLOps [Permalink for this section](https://docs.turboml.com/intro/\\\\#deployment-and-mlops)  \\n### Inference [Permalink for this section](https://docs.turboml.com/intro/\\\\#inference)  \\nOnce you’ve deployed a mode, there are several different ways to perform inference.  \\n#### Async [Permalink for this section](https://docs.turboml.com/intro/\\\\#async)  \\nThe first one is the async method. The data that is streamed from the input source is continuously fed to the model, and the outputs are streamed to another source. This stream can be either be subscribed to directly be the end user application, or sinked to a database or other data sources.  \\n```outputs = deployed_model.get_outputs()\\n```  \\n#### API [Permalink for this section](https://docs.turboml.com/intro/\\\\#api)  \\nA request-response model is used for inference on a single data point synchronously. The `/model_name/predict` endpoint is exposed for each deployed model where a REST API call can be made to get the outputs.  \\n#### Batch [Permalink for this section](https://docs.turboml.com/intro/\\\\#batch)  \\nWhen you have multiple records you’d like to perform inference on, you can use the get\\\\_inference method as follows.  \\n```outputs = deployed_model.get_inference(query_df)\\n```  \\n### Evaluation [Permalink for this section](https://docs.turboml.com/intro/\\\\#evaluation)  \\nTurboML provides standard ML metrics out of the box to perform model evaluation. Multiple metrics can be registered for any deployed model. The metrics pipeline re-uses the labels extracted for model training.  \\n```deployed_model.add_metric(\"WindowedAUC\")\\nmodel_auc_scores = deployed_model.get_evaluation(\"WindowedAUC\")\\n```  \\nLast updated on January 24, 2025  \\n[Quickstart](https://docs.turboml.com/quickstart/ \"Quickstart\")  \\nWhat is TurboML? @ TurboML\\n</page_content>')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 89 merged contexts.\n"
     ]
    }
   ],
   "source": [
    "merged_contexts = []\n",
    "\n",
    "for idx, doc in enumerate(documents):\n",
    "    try:\n",
    "        # Find similar chunks (excluding self and summary)\n",
    "        similar_docs = vector_store.similarity_search(\n",
    "            query=doc.page_content,\n",
    "            k=4,  # Fetch extra in case of self-match\n",
    "        )\n",
    "        \n",
    "        # Remove accidental self-matches\n",
    "        filtered_docs = [d for d in similar_docs if d.metadata[\"section\"] != doc.metadata[\"section\"]]\n",
    "        \n",
    "        # Select top 2 similar documents + current doc + summary\n",
    "        selected_docs = [summary_doc, doc] + filtered_docs[:2]\n",
    "        \n",
    "        # Merge content with section headers\n",
    "        merged_content = \"\\n\\n\".join([\n",
    "            d.page_content\n",
    "            for d in selected_docs\n",
    "        ])\n",
    "        \n",
    "        # Store merged context data\n",
    "        merged_contexts.append({\n",
    "            \"base_chunk\": doc.metadata[\"section\"],\n",
    "            \"context_sections\": [d.metadata[\"section\"] for d in selected_docs],\n",
    "            \"content\": merged_content,\n",
    "            \"embedding\": embeddings.embed_query(merged_content)\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {idx}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Generated {len(merged_contexts)} merged contexts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Save Merged Contexts to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged contexts saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save merged contexts as JSON file\n",
    "with open(\"merged_contexts.json\", \"w\") as f:\n",
    "    json.dump(merged_contexts, f, indent=2)\n",
    "\n",
    "print(\"Merged contexts saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Verify Sample Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Base chunk: What is TurboML?\n",
      "Merged sections: ['summary', 'What is TurboML?', 'TurboML Quickstart', 'Batch APIs']\n",
      "Content length: 58376 characters\n",
      "==================================================\n",
      "Sample 2:\n",
      "Base chunk: TurboML Quickstart\n",
      "Merged sections: ['summary', 'TurboML Quickstart', 'What is TurboML?', 'Batch APIs']\n",
      "Content length: 58376 characters\n",
      "==================================================\n",
      "Sample 3:\n",
      "Base chunk: String Encoding\n",
      "Merged sections: ['summary', 'String Encoding', 'Hyperparameter Tuning', 'Algorithm Tuning']\n",
      "Content length: 36758 characters\n",
      "==================================================\n",
      "Sample 4:\n",
      "Base chunk: AMF Regressor\n",
      "Merged sections: ['summary', 'AMF Regressor', 'AMF Classifier', 'FFM Regressor']\n",
      "Content length: 35606 characters\n",
      "==================================================\n",
      "Sample 5:\n",
      "Base chunk: MultinomialNB\n",
      "Merged sections: ['summary', 'MultinomialNB', 'Gaussian Naive Bayes', 'HeteroAdaBoostClassifier']\n",
      "Content length: 30644 characters\n",
      "==================================================\n",
      "Sample 6:\n",
      "Base chunk: PreProcessors\n",
      "Merged sections: ['summary', 'PreProcessors', 'EmbeddingModel', 'What is TurboML?']\n",
      "Content length: 48675 characters\n",
      "==================================================\n",
      "Sample 7:\n",
      "Base chunk: Python Model: Batch Example\n",
      "Merged sections: ['summary', 'Python Model: Batch Example', 'Python Model: PySAD Example', 'Batch APIs']\n",
      "Content length: 38916 characters\n",
      "==================================================\n",
      "Sample 8:\n",
      "Base chunk: Image Processing (MNIST Example)\n",
      "Merged sections: ['summary', 'Image Processing (MNIST Example)', 'LLM Embeddings', 'What is TurboML?']\n",
      "Content length: 46643 characters\n",
      "==================================================\n",
      "Sample 9:\n",
      "Base chunk: Stream Dataset to Deployed Models\n",
      "Merged sections: ['summary', 'Stream Dataset to Deployed Models', 'TurboML Quickstart', 'datasets.py']\n",
      "Content length: 81624 characters\n",
      "==================================================\n",
      "Sample 10:\n",
      "Base chunk: LeveragingBaggingClassifier\n",
      "Merged sections: ['summary', 'LeveragingBaggingClassifier', 'HeteroLeveragingBaggingClassifier', 'HeteroAdaBoostClassifier']\n",
      "Content length: 32748 characters\n",
      "==================================================\n",
      "Sample 11:\n",
      "Base chunk: TF-IDF embedding example using gRPC Client\n",
      "Merged sections: ['summary', 'TF-IDF embedding example using gRPC Client', 'Resnet example using gRPC Client', 'Image Processing (MNIST Example)']\n",
      "Content length: 38794 characters\n",
      "==================================================\n",
      "Sample 12:\n",
      "Base chunk: EmbeddingModel\n",
      "Merged sections: ['summary', 'EmbeddingModel', 'PreProcessors', 'RandomProjectionEmbedding']\n",
      "Content length: 39651 characters\n",
      "==================================================\n",
      "Sample 13:\n",
      "Base chunk: BanditModelSelection\n",
      "Merged sections: ['summary', 'BanditModelSelection', 'ContextualBanditModelSelection', 'AdaBoostClassifier']\n",
      "Content length: 32286 characters\n",
      "==================================================\n",
      "Sample 14:\n",
      "Base chunk: HeteroLeveragingBaggingClassifier\n",
      "Merged sections: ['summary', 'HeteroLeveragingBaggingClassifier', 'HeteroAdaBoostClassifier', 'LeveragingBaggingClassifier']\n",
      "Content length: 32748 characters\n",
      "==================================================\n",
      "Sample 15:\n",
      "Base chunk: FFM Regressor\n",
      "Merged sections: ['summary', 'FFM Regressor', 'FFM Classifier', 'AMF Regressor']\n",
      "Content length: 34654 characters\n",
      "==================================================\n",
      "Sample 16:\n",
      "Base chunk: Hyperparameter Tuning\n",
      "Merged sections: ['summary', 'Hyperparameter Tuning', 'Algorithm Tuning', 'Performance Improvements']\n",
      "Content length: 39216 characters\n",
      "==================================================\n",
      "Sample 17:\n",
      "Base chunk: Algorithm Tuning\n",
      "Merged sections: ['summary', 'Algorithm Tuning', 'Hyperparameter Tuning', 'Performance Improvements']\n",
      "Content length: 39216 characters\n",
      "==================================================\n",
      "Sample 18:\n",
      "Base chunk: Drift Detection\n",
      "Merged sections: ['summary', 'Drift Detection', 'Performance Improvements', 'TurboML Quickstart']\n",
      "Content length: 50940 characters\n",
      "==================================================\n",
      "Sample 19:\n",
      "Base chunk: MSTREAM\n",
      "Merged sections: ['summary', 'MSTREAM', 'Random Cut Forest', 'Half-Space Trees (HST)']\n",
      "Content length: 31553 characters\n",
      "==================================================\n",
      "Sample 20:\n",
      "Base chunk: RandomProjectionEmbedding\n",
      "Merged sections: ['summary', 'RandomProjectionEmbedding', 'EmbeddingModel', 'Random Sampler']\n",
      "Content length: 31914 characters\n",
      "==================================================\n",
      "Sample 21:\n",
      "Base chunk: Gaussian Naive Bayes\n",
      "Merged sections: ['summary', 'Gaussian Naive Bayes', 'MultinomialNB', 'HeteroLeveragingBaggingClassifier']\n",
      "Content length: 31259 characters\n",
      "==================================================\n",
      "Sample 22:\n",
      "Base chunk: Feature Engineering - Python UDAF\n",
      "Merged sections: ['summary', 'Feature Engineering - Python UDAF', 'Feature Engineering - Python UDFs', 'udf.py']\n",
      "Content length: 37357 characters\n",
      "==================================================\n",
      "Sample 23:\n",
      "Base chunk: TurboML LLM Tutorial\n",
      "Merged sections: ['summary', 'TurboML LLM Tutorial', 'LLM Embeddings', 'llm.py']\n",
      "Content length: 36809 characters\n",
      "==================================================\n",
      "Sample 24:\n",
      "Base chunk: AMF Classifier\n",
      "Merged sections: ['summary', 'AMF Classifier', 'AMF Regressor', 'Hoeffding Tree Classifier']\n",
      "Content length: 36300 characters\n",
      "==================================================\n",
      "Sample 25:\n",
      "Base chunk: Image Processing (MNIST Example)\n",
      "Merged sections: ['summary', 'Image Processing (MNIST Example)', 'Algorithm Tuning', 'TurboML Quickstart']\n",
      "Content length: 51680 characters\n",
      "==================================================\n",
      "Sample 26:\n",
      "Base chunk: Feature Engineering - Python UDFs\n",
      "Merged sections: ['summary', 'Feature Engineering - Python UDFs', 'Feature Engineering - Python UDAF', 'Feature Engineering - Complex Stream Processing']\n",
      "Content length: 46037 characters\n",
      "==================================================\n",
      "Sample 27:\n",
      "Base chunk: Native Python Models\n",
      "Merged sections: ['summary', 'Native Python Models', 'Ensembling Custom Python Models in TurboML', 'Python Model: Batch Example']\n",
      "Content length: 46209 characters\n",
      "==================================================\n",
      "Sample 28:\n",
      "Base chunk: ONNX tutorial with Scikit-Learn\n",
      "Merged sections: ['summary', 'ONNX tutorial with Scikit-Learn', 'ONNX tutorial with TensorFlow', 'ONNX tutorial with PyTorch']\n",
      "Content length: 40051 characters\n",
      "==================================================\n",
      "Sample 29:\n",
      "Base chunk: Adaptive LightGBM\n",
      "Merged sections: ['summary', 'Adaptive LightGBM', 'Adaptive XGBoost', 'SGT Regressor']\n",
      "Content length: 34505 characters\n",
      "==================================================\n",
      "Sample 30:\n",
      "Base chunk: HoeffdingTreeRegressor\n",
      "Merged sections: ['summary', 'HoeffdingTreeRegressor', 'Hoeffding Tree Classifier', 'SGT Regressor']\n",
      "Content length: 35423 characters\n",
      "==================================================\n",
      "Sample 31:\n",
      "Base chunk: Python Model: PySAD Example\n",
      "Merged sections: ['summary', 'Python Model: PySAD Example', 'Python Model: Batch Example', 'Native Python Models']\n",
      "Content length: 43452 characters\n",
      "==================================================\n",
      "Sample 32:\n",
      "Base chunk: Batch APIs\n",
      "Merged sections: ['summary', 'Batch APIs', 'What is TurboML?', 'TurboML Quickstart']\n",
      "Content length: 58376 characters\n",
      "==================================================\n",
      "Sample 33:\n",
      "Base chunk: ContextualBanditModelSelection\n",
      "Merged sections: ['summary', 'ContextualBanditModelSelection', 'BanditModelSelection', 'LeveragingBaggingClassifier']\n",
      "Content length: 32737 characters\n",
      "==================================================\n",
      "Sample 34:\n",
      "Base chunk: ONNX tutorial with TensorFlow\n",
      "Merged sections: ['summary', 'ONNX tutorial with TensorFlow', 'ONNX tutorial with Scikit-Learn', 'ONNX tutorial with PyTorch']\n",
      "Content length: 40051 characters\n",
      "==================================================\n",
      "Sample 35:\n",
      "Base chunk: Local Model\n",
      "Merged sections: ['summary', 'Local Model', 'TurboML Quickstart', 'ONNX tutorial with PyTorch']\n",
      "Content length: 61564 characters\n",
      "==================================================\n",
      "Sample 36:\n",
      "Base chunk: Neural Network\n",
      "Merged sections: ['summary', 'Neural Network', 'Online Neural Network', 'ONNX']\n",
      "Content length: 32352 characters\n",
      "==================================================\n",
      "Sample 37:\n",
      "Base chunk: OCR example using RestAPI Client\n",
      "Merged sections: ['summary', 'OCR example using RestAPI Client', 'Resnet example using gRPC Client', 'TF-IDF embedding example using gRPC Client']\n",
      "Content length: 38789 characters\n",
      "==================================================\n",
      "Sample 38:\n",
      "Base chunk: HeteroAdaBoostClassifier\n",
      "Merged sections: ['summary', 'HeteroAdaBoostClassifier', 'HeteroLeveragingBaggingClassifier', 'AdaBoostClassifier']\n",
      "Content length: 32297 characters\n",
      "==================================================\n",
      "Sample 39:\n",
      "Base chunk: ONNX\n",
      "Merged sections: ['summary', 'ONNX', 'ONNX tutorial with TensorFlow', 'ONNX tutorial with Scikit-Learn']\n",
      "Content length: 35721 characters\n",
      "==================================================\n",
      "Sample 40:\n",
      "Base chunk: Resnet example using gRPC Client\n",
      "Merged sections: ['summary', 'Resnet example using gRPC Client', 'TF-IDF embedding example using gRPC Client', 'OCR example using RestAPI Client']\n",
      "Content length: 38789 characters\n",
      "==================================================\n",
      "Sample 41:\n",
      "Base chunk: OVR (OnevsRestClassifier)\n",
      "Merged sections: ['summary', 'OVR (OnevsRestClassifier)', 'Random Sampler', 'PreProcessors']\n",
      "Content length: 39166 characters\n",
      "==================================================\n",
      "Sample 42:\n",
      "Base chunk: Hoeffding Tree Classifier\n",
      "Merged sections: ['summary', 'Hoeffding Tree Classifier', 'HoeffdingTreeRegressor', 'AMF Classifier']\n",
      "Content length: 35839 characters\n",
      "==================================================\n",
      "Sample 43:\n",
      "Base chunk: AdaBoostClassifier\n",
      "Merged sections: ['summary', 'AdaBoostClassifier', 'HeteroAdaBoostClassifier', 'BanditModelSelection']\n",
      "Content length: 32035 characters\n",
      "==================================================\n",
      "Sample 44:\n",
      "Base chunk: Random Sampler\n",
      "Merged sections: ['summary', 'Random Sampler', 'EmbeddingModel', 'RandomProjectionEmbedding']\n",
      "Content length: 31914 characters\n",
      "==================================================\n",
      "Sample 45:\n",
      "Base chunk: FFM Classifier\n",
      "Merged sections: ['summary', 'FFM Classifier', 'FFM Regressor', 'AMF Classifier']\n",
      "Content length: 35871 characters\n",
      "==================================================\n",
      "Sample 46:\n",
      "Base chunk: SNARIMAX\n",
      "Merged sections: ['summary', 'SNARIMAX', 'What is TurboML?', 'ONNX tutorial with Scikit-Learn']\n",
      "Content length: 44763 characters\n",
      "==================================================\n",
      "Sample 47:\n",
      "Base chunk: LLM Embeddings\n",
      "Merged sections: ['summary', 'LLM Embeddings', 'Image Processing (MNIST Example)', 'TurboML LLM Tutorial']\n",
      "Content length: 38800 characters\n",
      "==================================================\n",
      "Sample 48:\n",
      "Base chunk: LLAMA Embedding\n",
      "Merged sections: ['summary', 'LLAMA Embedding', 'LLM Embeddings', 'HeteroAdaBoostClassifier']\n",
      "Content length: 33928 characters\n",
      "==================================================\n",
      "Sample 49:\n",
      "Base chunk: Model Explanations using iXAI\n",
      "Merged sections: ['summary', 'Model Explanations using iXAI', 'What is TurboML?', 'TurboML Quickstart']\n",
      "Content length: 58804 characters\n",
      "==================================================\n",
      "Sample 50:\n",
      "Base chunk: ONNX tutorial with PyTorch\n",
      "Merged sections: ['summary', 'ONNX tutorial with PyTorch', 'ONNX tutorial with Scikit-Learn', 'ONNX tutorial with TensorFlow']\n",
      "Content length: 40051 characters\n",
      "==================================================\n",
      "Sample 51:\n",
      "Base chunk: Half-Space Trees (HST)\n",
      "Merged sections: ['summary', 'Half-Space Trees (HST)', 'MSTREAM', 'Random Cut Forest']\n",
      "Content length: 31553 characters\n",
      "==================================================\n",
      "Sample 52:\n",
      "Base chunk: SGT Regressor\n",
      "Merged sections: ['summary', 'SGT Regressor', 'HoeffdingTreeRegressor', 'AMF Regressor']\n",
      "Content length: 34411 characters\n",
      "==================================================\n",
      "Sample 53:\n",
      "Base chunk: Ensembling Custom Python Models in TurboML\n",
      "Merged sections: ['summary', 'Ensembling Custom Python Models in TurboML', 'Native Python Models', 'Python Model: Batch Example']\n",
      "Content length: 46209 characters\n",
      "==================================================\n",
      "Sample 54:\n",
      "Base chunk: Custom Evaluation Metric\n",
      "Merged sections: ['summary', 'Custom Evaluation Metric', 'Ensembling Custom Python Models in TurboML', 'Feature Engineering - Python UDAF']\n",
      "Content length: 43188 characters\n",
      "==================================================\n",
      "Sample 55:\n",
      "Base chunk: Adaptive XGBoost\n",
      "Merged sections: ['summary', 'Adaptive XGBoost', 'Adaptive LightGBM', 'Model Explanations using iXAI']\n",
      "Content length: 36440 characters\n",
      "==================================================\n",
      "Sample 56:\n",
      "Base chunk: Random Cut Forest\n",
      "Merged sections: ['summary', 'Random Cut Forest', 'MSTREAM', 'AMF Regressor']\n",
      "Content length: 32540 characters\n",
      "==================================================\n",
      "Sample 57:\n",
      "Base chunk: Feature Engineering - Complex Stream Processing\n",
      "Merged sections: ['summary', 'Feature Engineering - Complex Stream Processing', 'TurboML Ibis Quickstart', 'datasets.py']\n",
      "Content length: 63926 characters\n",
      "==================================================\n",
      "Sample 58:\n",
      "Base chunk: TurboML Ibis Quickstart\n",
      "Merged sections: ['summary', 'TurboML Ibis Quickstart', 'Feature Engineering - Complex Stream Processing', 'TurboML Quickstart']\n",
      "Content length: 56883 characters\n",
      "==================================================\n",
      "Sample 59:\n",
      "Base chunk: Performance Improvements\n",
      "Merged sections: ['summary', 'Performance Improvements', 'Algorithm Tuning', 'Hyperparameter Tuning']\n",
      "Content length: 39216 characters\n",
      "==================================================\n",
      "Sample 60:\n",
      "Base chunk: Online Neural Network\n",
      "Merged sections: ['summary', 'Online Neural Network', 'ONNX', 'Neural Network']\n",
      "Content length: 32352 characters\n",
      "==================================================\n",
      "Sample 61:\n",
      "Base chunk: __init__.py\n",
      "Merged sections: ['summary', '__init__.py', 'models.py', 'ml_algs.py']\n",
      "Content length: 123458 characters\n",
      "==================================================\n",
      "Sample 62:\n",
      "Base chunk: api.py\n",
      "Merged sections: ['summary', 'api.py', 'namespaces.py', 'dataloader.py']\n",
      "Content length: 46486 characters\n",
      "==================================================\n",
      "Sample 63:\n",
      "Base chunk: concurrent.py\n",
      "Merged sections: ['summary', 'concurrent.py', 'internal.py', 'types.py']\n",
      "Content length: 35171 characters\n",
      "==================================================\n",
      "Sample 64:\n",
      "Base chunk: dataloader.py\n",
      "Merged sections: ['summary', 'dataloader.py', 'models.py', 'ml_algs.py']\n",
      "Content length: 120384 characters\n",
      "==================================================\n",
      "Sample 65:\n",
      "Base chunk: datasets.py\n",
      "Merged sections: ['summary', 'datasets.py', 'Stream Dataset to Deployed Models', 'feature_engineering.py']\n",
      "Content length: 110144 characters\n",
      "==================================================\n",
      "Sample 66:\n",
      "Base chunk: default_model_configs.py\n",
      "Merged sections: ['summary', 'default_model_configs.py', 'config_pb2.py', 'models.py']\n",
      "Content length: 116174 characters\n",
      "==================================================\n",
      "Sample 67:\n",
      "Base chunk: env.py\n",
      "Merged sections: ['summary', 'env.py', 'pymodel.py', 'pytypes.py']\n",
      "Content length: 28113 characters\n",
      "==================================================\n",
      "Sample 68:\n",
      "Base chunk: feature_engineering.py\n",
      "Merged sections: ['summary', 'feature_engineering.py', 'models.py', 'datasets.py']\n",
      "Content length: 120181 characters\n",
      "==================================================\n",
      "Sample 69:\n",
      "Base chunk: internal.py\n",
      "Merged sections: ['summary', 'internal.py', 'ml_algs.py', 'dataloader.py']\n",
      "Content length: 101589 characters\n",
      "==================================================\n",
      "Sample 70:\n",
      "Base chunk: llm.py\n",
      "Merged sections: ['summary', 'llm.py', 'TurboML LLM Tutorial', 'LLM Embeddings']\n",
      "Content length: 36809 characters\n",
      "==================================================\n",
      "Sample 71:\n",
      "Base chunk: ml_algs.py\n",
      "Merged sections: ['summary', 'ml_algs.py', 'models.py', '__init__.py']\n",
      "Content length: 123458 characters\n",
      "==================================================\n",
      "Sample 72:\n",
      "Base chunk: models.py\n",
      "Merged sections: ['summary', 'models.py', 'ml_algs.py', '__init__.py']\n",
      "Content length: 123458 characters\n",
      "==================================================\n",
      "Sample 73:\n",
      "Base chunk: model_comparison.py\n",
      "Merged sections: ['summary', 'model_comparison.py', '__init__.py', 'Python Model: Batch Example']\n",
      "Content length: 47587 characters\n",
      "==================================================\n",
      "Sample 74:\n",
      "Base chunk: namespaces.py\n",
      "Merged sections: ['summary', 'namespaces.py', 'api.py', 'env.py']\n",
      "Content length: 34890 characters\n",
      "==================================================\n",
      "Sample 75:\n",
      "Base chunk: pymodel.py\n",
      "Merged sections: ['summary', 'pymodel.py', 'pytypes.py', 'types.py']\n",
      "Content length: 27832 characters\n",
      "==================================================\n",
      "Sample 76:\n",
      "Base chunk: pytypes.py\n",
      "Merged sections: ['summary', 'pytypes.py', 'pymodel.py', 'types.py']\n",
      "Content length: 27832 characters\n",
      "==================================================\n",
      "Sample 77:\n",
      "Base chunk: types.py\n",
      "Merged sections: ['summary', 'types.py', 'pytypes.py', 'pymodel.py']\n",
      "Content length: 27832 characters\n",
      "==================================================\n",
      "Sample 78:\n",
      "Base chunk: udf.py\n",
      "Merged sections: ['summary', 'udf.py', 'Feature Engineering - Python UDAF', 'Custom Evaluation Metric']\n",
      "Content length: 38508 characters\n",
      "==================================================\n",
      "Sample 79:\n",
      "Base chunk: util.py\n",
      "Merged sections: ['summary', 'util.py', 'internal.py', 'api.py']\n",
      "Content length: 44631 characters\n",
      "==================================================\n",
      "Sample 80:\n",
      "Base chunk: config_pb2.py\n",
      "Merged sections: ['summary', 'config_pb2.py', 'output_pb2.py', 'default_model_configs.py']\n",
      "Content length: 93025 characters\n",
      "==================================================\n",
      "Sample 81:\n",
      "Base chunk: flink_pb2.py\n",
      "Merged sections: ['summary', 'flink_pb2.py', 'sources_pb2.py', 'models.py']\n",
      "Content length: 79630 characters\n",
      "==================================================\n",
      "Sample 82:\n",
      "Base chunk: flink_pb2_grpc.py\n",
      "Merged sections: ['summary', 'flink_pb2_grpc.py', 'ml_service_pb2_grpc.py', 'flink_pb2.py']\n",
      "Content length: 50545 characters\n",
      "==================================================\n",
      "Sample 83:\n",
      "Base chunk: input_pb2.py\n",
      "Merged sections: ['summary', 'input_pb2.py', 'output_pb2.py', 'metrics_pb2.py']\n",
      "Content length: 33800 characters\n",
      "==================================================\n",
      "Sample 84:\n",
      "Base chunk: metrics_pb2.py\n",
      "Merged sections: ['summary', 'metrics_pb2.py', 'input_pb2.py', 'output_pb2.py']\n",
      "Content length: 33800 characters\n",
      "==================================================\n",
      "Sample 85:\n",
      "Base chunk: ml_service_pb2.py\n",
      "Merged sections: ['summary', 'ml_service_pb2.py', 'metrics_pb2.py', 'input_pb2.py']\n",
      "Content length: 32880 characters\n",
      "==================================================\n",
      "Sample 86:\n",
      "Base chunk: ml_service_pb2_grpc.py\n",
      "Merged sections: ['summary', 'ml_service_pb2_grpc.py', 'flink_pb2_grpc.py', 'ml_service_pb2.py']\n",
      "Content length: 36781 characters\n",
      "==================================================\n",
      "Sample 87:\n",
      "Base chunk: output_pb2.py\n",
      "Merged sections: ['summary', 'output_pb2.py', 'input_pb2.py', 'metrics_pb2.py']\n",
      "Content length: 33800 characters\n",
      "==================================================\n",
      "Sample 88:\n",
      "Base chunk: sources_pb2.py\n",
      "Merged sections: ['summary', 'sources_pb2.py', 'sources_p2p.py', 'flink_pb2.py']\n",
      "Content length: 56896 characters\n",
      "==================================================\n",
      "Sample 89:\n",
      "Base chunk: sources_p2p.py\n",
      "Merged sections: ['summary', 'sources_p2p.py', 'sources_pb2.py', 'input_pb2.py']\n",
      "Content length: 43902 characters\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(merged_contexts):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"Base chunk: {sample['base_chunk']}\")\n",
    "    print(f\"Merged sections: {sample['context_sections']}\")\n",
    "    print(f\"Content length: {len(sample['content'])} characters\")\n",
    "    print(\"=\" * 50)  # Separator for readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# **TurboML: A Real-Time Machine Learning Platform - Detailed Summary**\n",
      "\n",
      "TurboML is a platform designed for building, deploying, and managing real-time machine learning applications. It emphasizes streaming data and provides tools for the entire ML lifecycle, from data ingestion to model monitoring.\n",
      "\n",
      "**1. Data Ingestion and Management:**\n",
      "\n",
      "*   **Core Principle:** TurboML treats data as continuous streams, enabling real-time processing and updates.\n",
      "*   **Ingestion Methods:**\n",
      "    *   **Pull-based:**\n",
      "        *   Uses pre-built connectors to continuously pull data from various sources.\n",
      "        *   Supported sources include cloud storage (e.g., S3) and databases (e.g., Postgres). *While Kafka is used internally, the documentation doesn't explicitly present it as a direct pull-based source for end-users in the introductory sections.*\n",
      "        *   Connectors are configured to handle data formats and connection details.\n",
      "    *   **Push-based:**\n",
      "        *   Allows direct data injection into the TurboML platform.\n",
      "        *   Methods:\n",
      "            *   **REST API:** Send data via HTTP requests using the `dataset/{dataset_id}/upload` endpoint.\n",
      "            *   **Client SDKs:** More performant options for high-volume data. The Python SDK provides convenient methods for working with Pandas DataFrames.\n",
      "            *   **gRPC API:** Upload data using Arrow Flight gRPC, providing the most performant option.\n",
      "        *   Example (Pandas DataFrame):\n",
      "            ```python\n",
      "            transactions = tb.OnlineDataset.from_pd(\n",
      "                id=\"qs_transactions\",\n",
      "                df=transactions_df,\n",
      "                key_field=\"transactionID\",\n",
      "                load_if_exists=True,\n",
      "            )\n",
      "            ```\n",
      "*   **Dataset Classes:**\n",
      "    *   **`OnlineDataset`:**\n",
      "        *   Represents a dataset managed by the TurboML platform.\n",
      "        *   Supports continuous data ingestion (pull or push).\n",
      "        *   Provides methods for feature engineering, model deployment, and monitoring.\n",
      "        *   Can be created from Pandas DataFrames (`from_pd`), or loaded if it already exists (`load`).\n",
      "        *   `add_pd()` method allows adding new data to an existing `OnlineDataset` (using Arrow Flight Protocol over gRPC). There are also `dataset/dataset_id/upload` REST API endpoint and direct gRPC API options for data upload.\n",
      "        *   `sync_features()` method synchronizes materialized streaming features to the `OnlineDataset` object.  This is important after uploading new data or materializing features.\n",
      "    *   **`LocalDataset`:**\n",
      "        *   Represents an in-memory dataset, primarily for local experimentation and development.\n",
      "        *   Can be created from Pandas DataFrames (`from_pd`).\n",
      "        *   Useful for testing feature engineering logic before deploying to an `OnlineDataset`.\n",
      "        *   Can be converted to an `OnlineDataset` using `to_online()`.\n",
      "    * **`PandasDataset`:** *This class is present in the `intro` documentation, but its role and usage are less clearly defined compared to `OnlineDataset` and `LocalDataset`. It appears to be a less preferred way of interacting with data.*\n",
      "* **Data Schema:**\n",
      "    *   Datasets have a defined schema, specifying field names and data types.\n",
      "    *   Schemas are automatically inferred from Pandas DataFrames.\n",
      "    *   Schemas are managed by the platform and used for data validation and consistency.\n",
      "    *   Supported data types include: INT32, INT64, FLOAT, DOUBLE, STRING, BOOL, BYTES.\n",
      "* **Key Field:**\n",
      "    *   Each dataset must have a primary key field (`key_field`) to uniquely identify records.\n",
      "    *   Used for merging data, performing lookups, and ensuring data integrity.\n",
      "\n",
      "**2. Feature Engineering:**\n",
      "\n",
      "*   **Core Philosophy:** Define features once, test them locally, and then deploy them for continuous, real-time computation.\n",
      "*   **Feature Definition Interfaces:**\n",
      "    *   **SQL Features:**\n",
      "        *   Define features using standard SQL expressions.\n",
      "        *   Column names are enclosed in double quotes.\n",
      "        *   Example:\n",
      "            ```python\n",
      "            transactions.feature_engineering.create_sql_features(\n",
      "                sql_definition='\"transactionAmount\" + \"localHour\"',\n",
      "                new_feature_name=\"my_sql_feat\",\n",
      "            )\n",
      "            ```\n",
      "    *   **Aggregate Features:**\n",
      "        *   Define time-windowed aggregations (SUM, COUNT, AVG, MIN, MAX, etc.).\n",
      "        *   Require a registered timestamp column.\n",
      "        *   Specify:\n",
      "            *   `column_to_operate`: The column to aggregate.\n",
      "            *   `column_to_group`: The column(s) to group by.\n",
      "            *   `operation`: The aggregation function (SUM, COUNT, etc.).\n",
      "            *   `new_feature_name`: The name of the new feature column.\n",
      "            *   `timestamp_column`: The column containing timestamps.\n",
      "            *   `window_duration`: The length of the time window.\n",
      "            *   `window_unit`: The unit of the window duration (seconds, minutes, hours, etc.).\n",
      "        *   Example:\n",
      "            ```python\n",
      "            transactions.feature_engineering.register_timestamp(column_name=\"timestamp\", format_type=\"epoch_seconds\")\n",
      "\n",
      "            transactions.feature_engineering.create_aggregate_features(\n",
      "                column_to_operate=\"transactionAmount\",\n",
      "                column_to_group=\"accountID\",\n",
      "                operation=\"SUM\",\n",
      "                new_feature_name=\"my_sum_feat\",\n",
      "                timestamp_column=\"timestamp\",\n",
      "                window_duration=24,\n",
      "                window_unit=\"hours\",\n",
      "            )\n",
      "            ```\n",
      "    *   **User-Defined Functions (UDFs):**\n",
      "        *   Define features using custom Python code.\n",
      "        *   **Simple UDFs:** Functions that take column values as input and return a single value.\n",
      "            ```python\n",
      "            myfunction_contents = \"\"\"\n",
      "            import numpy as np\n",
      "\n",
      "            def myfunction(x):\n",
      "                return np.sin(x)\n",
      "            \"\"\"\n",
      "            transactions.feature_engineering.create_udf_features(\n",
      "                new_feature_name=\"sine_of_amount\",\n",
      "                argument_names=[\"transactionAmount\"],\n",
      "                function_name=\"myfunction\",\n",
      "                function_file_contents=myfunction_contents,\n",
      "                libraries=[\"numpy\"],\n",
      "            )\n",
      "\n",
      "            ```\n",
      "        *   **Rich UDFs:** Class-based UDFs that can maintain state and perform more complex operations (e.g., database lookups). Require an `__init__` method and a `func` method. The `dev_initializer_arguments` and `prod_initializer_arguments` are used for development and production environments, respectively.\n",
      "        *   **User-Defined Aggregate Functions (UDAFs):** Custom aggregations defined in Python. Require implementing `create_state`, `accumulate`, `retract` (optional), `merge_states`, and `finish` methods.\n",
      "            ```python\n",
      "            function_file_contents = \"\"\"\n",
      "            def create_state():\n",
      "                return 0, 0\n",
      "\n",
      "            def accumulate(state, value, weight):\n",
      "                if value is None or weight is None:\n",
      "                    return state\n",
      "                (s, w) = state\n",
      "                s += value * weight\n",
      "                w += weight\n",
      "                return s, w\n",
      "\n",
      "            def retract(state, value, weight):\n",
      "                if value is None or weight is None:\n",
      "                    return state\n",
      "                (s, w) = state\n",
      "                s -= value * weight\n",
      "                w -= weight\n",
      "                return s, w\n",
      "\n",
      "            def merge_states(state_a, state_b):\n",
      "                (s_a, w_a) = state_a\n",
      "                (s_b, w_b) = state_b\n",
      "                return s_a + s_b, w_a + w_b\n",
      "\n",
      "            def finish(state):\n",
      "                (sum, weight) = state\n",
      "                if weight == 0:\n",
      "                    return None\n",
      "                else:\n",
      "                    return sum / weight\n",
      "            \"\"\"\n",
      "            transactions.feature_engineering.create_udaf_features(\n",
      "                new_feature_name=\"weighted_avg\",\n",
      "                column_to_operate=[\"transactionAmount\", \"transactionTime\"],\n",
      "                function_name=\"weighted_avg\",\n",
      "                return_type=\"DOUBLE\",\n",
      "                function_file_contents=function_file_contents,\n",
      "                column_to_group=[\"accountID\"],\n",
      "                timestamp_column=\"timestamp\",\n",
      "                window_duration=1,\n",
      "                window_unit=\"hours\",\n",
      "            )\n",
      "            ```\n",
      "    *   **Ibis Features:**\n",
      "        *   Define complex streaming features using the Ibis DataFrame API.\n",
      "        *   Supports Apache Flink and RisingWave backends for execution.\n",
      "        *   Allows for more sophisticated feature engineering than simple SQL or aggregations.\n",
      "        *   Example:\n",
      "            ```python\n",
      "            fe = tb.IbisFeatureEngineering()\n",
      "            transactions = fe.get_ibis_table(\"transactions_stream\")\n",
      "            # ... define features using Ibis expressions ...\n",
      "            fe.materialize_features(\n",
      "                transactions_with_frequency_score,\n",
      "                \"transactions_with_frequency_score\",\n",
      "                \"transactionID\",\n",
      "                BackEnd.Flink, # Or BackEnd.Risingwave\n",
      "                \"transactions_stream\",\n",
      "            )\n",
      "            ```\n",
      "\n",
      "*   **Feature Materialization:**\n",
      "    *   `materialize_features()`: Submits feature definitions to the platform for continuous computation. Features are computed in real-time as new data arrives.\n",
      "    *   `materialize_ibis_features()`: Submits Ibis feature definitions.\n",
      "*   **Feature Retrieval:**\n",
      "    *   `get_features()`: Retrieves a *snapshot* of the raw data stream (for experimentation). *Note:* The returned data is not guaranteed to be in the same order or size on each call.\n",
      "    *   `get_local_features()`: Returns a DataFrame with the locally computed features (for debugging and experimentation).\n",
      "    *   `get_materialized_features()`: Retrieves the *continuously computed* features from the platform.\n",
      "    *   `retrieve_features()`: Computes feature values on ad-hoc data (not part of the stream).\n",
      "*   **Timestamp Handling:**\n",
      "    *   `register_timestamp()`: Registers a column as the timestamp for the dataset. Required for time-windowed aggregations.\n",
      "    *   `get_timestamp_formats()`: Returns a list of supported timestamp format strings.\n",
      "    *   `convert_timestamp()`: *This function is an internal utility, not a directly exposed user-facing API. It's used within the feature engineering logic.*\n",
      "* **Classes/Functions:**\n",
      "    *   `FeatureEngineering`: Class for defining SQL and aggregation features on `OnlineDataset`.\n",
      "    *   `LocalFeatureEngineering`: Class for defining features on `LocalDataset`.\n",
      "    *   `IbisFeatureEngineering`: Class for defining features using the Ibis interface.\n",
      "    *   `tb.register_source()`: Registers a data source configuration.\n",
      "    *   `DataSource`: Defines where and how raw data is accessed.\n",
      "        *    `FileSource`: Specifies a file-based data source (e.g., CSV, Parquet).\n",
      "        *   `PostgresSource`: Specifies a PostgreSQL data source.\n",
      "        *   `KafkaSource`: *Mentioned in the context, but not as a direct source for `DataSource` in the `intro`.*\n",
      "        *   `FeatureGroupSource`: Specifies a feature group as a data source.\n",
      "    *   `TimestampFormatConfig`: Configures timestamp format.\n",
      "    *   `Watermark`: Defines watermark settings for streaming data.\n",
      "    *   `TurboMLScalarFunction`: Base class for defining rich UDFs.\n",
      "\n",
      "**3. ML Modeling:**\n",
      "\n",
      "*   **Model Types:**\n",
      "    *   **Supervised:** Models that learn from labeled data (e.g., classification, regression).\n",
      "    *   **Unsupervised:** Models that learn from unlabeled data (e.g., anomaly detection).\n",
      "*   **Input Specification:**\n",
      "    *   Models require specifying the input features using:\n",
      "        *   `numerical_fields`: List of numeric column names.\n",
      "        *   `categorical_fields`: List of categorical column names.\n",
      "        *   `textual_fields`: List of text column names.\n",
      "        *   `imaginal_fields`: List of image column names (binary data).\n",
      "        *   `time_field`: Name of the timestamp column (optional).\n",
      "    *   Example:\n",
      "        ```python\n",
      "        numerical_fields = [\"transactionAmount\", \"localHour\"]\n",
      "        categorical_fields = [\"digitalItemCount\", \"physicalItemCount\", \"isProxyIP\"]\n",
      "        features = transactions.get_model_inputs(\n",
      "            numerical_fields=numerical_fields, categorical_fields=categorical_fields\n",
      "        )\n",
      "        label = labels.get_model_labels(label_field=\"is_fraud\")\n",
      "        ```\n",
      "* **Supported Algorithms (Examples):**\n",
      "    *   **Classification:**\n",
      "        *   `HoeffdingTreeClassifier`: Incremental decision tree for classification.\n",
      "        *   `AMFClassifier`: Aggregated Mondrian Forest classifier.\n",
      "        *   `FFMClassifier`: Field-aware Factorization Machine classifier.\n",
      "        *   `SGTClassifier`: Stochastic Gradient Tree classifier.\n",
      "        *   `MultinomialNB`: Multinomial Naive Bayes.\n",
      "        *   `GaussianNB`: Gaussian Naive Bayes.\n",
      "    *   **Regression:**\n",
      "        *   `HoeffdingTreeRegressor`: Incremental decision tree for regression.\n",
      "        *   `AMFRegressor`: Aggregated Mondrian Forest regressor.\n",
      "        *   `FFMRegressor`: Field-aware Factorization Machine regressor.\n",
      "        *   `SGTRegressor`: Stochastic Gradient Tree regressor.\n",
      "        *   `SNARIMAX`: Time series forecasting model.\n",
      "    *   **Anomaly Detection:**\n",
      "        *   `RCF`: Random Cut Forest.\n",
      "        *   `HST`: Half-Space Trees.\n",
      "        *   `MStream`: Multi-aspect stream anomaly detection.\n",
      "    *   **Ensemble Methods:**\n",
      "        *   `LeveragingBaggingClassifier`: Bagging with ADWIN for concept drift.\n",
      "        *   `HeteroLeveragingBaggingClassifier`: Bagging with different base models.\n",
      "        *   `AdaBoostClassifier`: AdaBoost.\n",
      "        *   `HeteroAdaBoostClassifier`: AdaBoost with different base models.\n",
      "        *   `BanditModelSelection`: Model selection using bandit algorithms.\n",
      "        *   `ContextualBanditModelSelection`: Model selection using contextual bandits.\n",
      "        *   `RandomSampler`: Random sampling for imbalanced datasets.\n",
      "    *   **General Purpose:**\n",
      "        *   `NeuralNetwork`: Configurable neural network.\n",
      "        *   `ONN`: Online Neural Network.\n",
      "        *   `AdaptiveXGBoost`: XGBoost with concept drift handling.\n",
      "        *   `AdaptiveLGBM`: LightGBM with concept drift handling.\n",
      "        *   `ONNX`: Deploy models from other frameworks (PyTorch, TensorFlow, Scikit-learn) using ONNX format (static models).\n",
      "        *   `Python`: Define custom models using Python classes.\n",
      "        *   `PythonEnsembleModel`: Define custom ensemble models using Python classes.\n",
      "        *   `RestAPIClient`: Use custom models via REST API.\n",
      "        *   `GRPCClient`: Use custom models via gRPC API.\n",
      "*   **Bring Your Own Models (BYOM):**\n",
      "    *   **ONNX:** Deploy models trained in other frameworks.\n",
      "    *   **Python:** Define custom models with `learn_one` and `predict_one` methods. Requires defining a class with `init_imports`, `learn_one`, and `predict_one` methods. A virtual environment (`venv`) can be set up to manage dependencies.\n",
      "    *   **gRPC:** Integrate models via gRPC.\n",
      "    *   **REST API:** Integrate models via REST API.\n",
      "*   **Model Composition:**\n",
      "    *   Combine models using ensemble methods (e.g., `LeveragingBaggingClassifier`).\n",
      "    *   Chain preprocessors with models (e.g., `MinMaxPreProcessor` + `HoeffdingTreeClassifier`).\n",
      "*   **Preprocessors:**\n",
      "    *   Transform input data before it's passed to the model.\n",
      "    *   Examples:\n",
      "        *   `MinMaxPreProcessor`: Scales numerical features to [0, 1].\n",
      "        *   `NormalPreProcessor`: Standardizes numerical features (zero mean, unit variance).\n",
      "        *   `RobustPreProcessor`: Scales features using robust statistics (median, IQR).\n",
      "        *   `LabelPreProcessor`: Converts strings to ordinal integers (textual fields).\n",
      "        *   `OneHotPreProcessor`: Creates one-hot encoding for categorical features.\n",
      "        *   `BinaryPreProcessor`: Creates binary encoding for categorical features.\n",
      "        *   `FrequencyPreProcessor`: Encodes strings based on their frequency.\n",
      "        *   `TargetPreProcessor`: Encodes strings based on the average target value.\n",
      "        *   `LlamaCppPreProcessor`: Generates text embeddings using Llama.cpp models (GGUF format).\n",
      "        *   `ClipEmbeddingPreprocessor`: Generates image embeddings using CLIP models (GGUF format).\n",
      "        *   `ImageToNumericPreProcessor`: Converts binary image data to numerical data.\n",
      "        *   `RandomProjectionEmbedding`: Dimensionality reduction using random projection.\n",
      "        *   `LLAMAEmbedding`: Text embeddings using GGUF models.\n",
      "        *   `ClipEmbedding`: Image embeddings using GGUF models.\n",
      "        *   `EmbeddingModel`: Combines an embedding model with a base model.\n",
      "        *   `OVR`: One-vs-the-rest multiclass strategy.\n",
      "    *   Preprocessors are typically combined with a `base_model`.\n",
      "* **Model Training:**\n",
      "    *   **Batch Training:**\n",
      "        *   Use the `learn()` method on a `Model` instance.\n",
      "        *   Can be used to incrementally train a model on multiple batches of data.\n",
      "        *   Example:\n",
      "            ```python\n",
      "            model = tb.HoeffdingTreeClassifier(n_classes=2)\n",
      "            trained_model = model.learn(features, label)  # Train on initial data\n",
      "            new_trained_model = trained_model.learn(new_features, new_label)  # Update with new data\n",
      "            ```\n",
      "    *   **Streaming Training:**\n",
      "        *   Models are continuously updated as new data arrives.\n",
      "        *   Enabled by default when deploying a model with `deploy()`.\n",
      "        *   Can be configured with different update strategies:\n",
      "            *   **Online:** Update on every data point.\n",
      "            *   **Trigger-based:** Update based on volume, time, performance, or drift (*mentioned in `intro`, but specific configuration details are limited in the provided documentation*).\n",
      "* **Model Deployment:**\n",
      "    *   `model.deploy(name, input, labels, predict_only=False)`: Deploys a model to the TurboML platform.\n",
      "        *   `name`: A unique name for the deployed model.\n",
      "        *   `input`: An `OnlineInputs` object defining the input features.\n",
      "        *   `labels`: An `OnlineLabels` object defining the target labels.\n",
      "        *   `predict_only`: If `True`, the model will not be updated with new data (useful for batch-trained models).\n",
      "    *   Returns a `DeployedModel` instance.\n",
      "* **Model Retrieval:**\n",
      "    *    `tb.retrieve_model(model_name: str)`: Fetches a reference to an already deployed model, allowing interaction in a new workspace/environment without redeployment.\n",
      "\n",
      "**4. Model Evaluation and Monitoring (MLOps):**\n",
      "\n",
      "*   **Evaluation Metrics:**\n",
      "    *   **Built-in Metrics:**\n",
      "        *   `WindowedAUC`: Area Under the ROC Curve (for classification).\n",
      "        *   `WindowedAccuracy`: Accuracy (for classification).\n",
      "        *   `WindowedMAE`: Mean Absolute Error (for regression).\n",
      "        *   `WindowedMSE`: Mean Squared Error (for regression).\n",
      "        *   `WindowedRMSE`: Root Mean Squared Error (for regression).\n",
      "    *   **Custom Metrics:**\n",
      "        *   Define custom aggregate metrics using Python classes that inherit from `ModelMetricAggregateFunction`.\n",
      "        *   Implement `create_state`, `accumulate`, `retract` (optional), `merge_states`, and `finish` methods.\n",
      "    *   **Continuous Evaluation:** Metrics are calculated continuously as new data arrives.\n",
      "    *   **Functions:**\n",
      "        *   `deployed_model.add_metric(metric_name)`: Registers a metric for a deployed model.\n",
      "        *   `deployed_model.get_evaluation(metric_name, filter_expression=\"\", window_size=1000, limit=100000)`: Retrieves evaluation results for a specific metric. The `filter_expression` allows filtering data based on SQL expressions.\n",
      "        *   `tb.evaluation_metrics()`: Lists available built-in metrics.\n",
      "        *   `tb.register_custom_metric(metric_name, metric_class)`: Registers a custom metric.\n",
      "        *   `tb.compare_model_metrics(models, metric)`: Compares multiple models on a given metric (generates a Plotly plot).\n",
      "*   **Drift Detection:**\n",
      "    *   **Univariate Drift:** Detects changes in the distribution of individual features.\n",
      "        *   Uses the Adaptive Windowing (ADWIN) method by default.\n",
      "        *   Register with `dataset.register_univariate_drift(numerical_field, label=None)`.\n",
      "        *   Retrieve with `dataset.get_univariate_drift(label=None, numerical_field=None)`.\n",
      "    *   **Multivariate Drift:** Detects changes in the joint distribution of multiple features.\n",
      "        *   Uses PCA-based reconstruction by default.\n",
      "        *   Register with `dataset.register_multivariate_drift(label, numerical_fields)`.\n",
      "        *   Retrieve with `dataset.get_multivariate_drift(label)`.\n",
      "    *   **Model Drift (Target Drift):** Detects changes in the relationship between input features and the target variable.\n",
      "        *   Register with `deployed_model.add_drift()`.\n",
      "        *   Retrieve with `deployed_model.get_drifts()`.\n",
      "*   **Model Explanations:**\n",
      "    *   Integration with the `iXAI` library for incremental model explanations.\n",
      "    *   Provides insights into feature importance and model behavior.  The example shows using `IncrementalPFI` from `iXAI`.\n",
      "*   **Model Management:**\n",
      "    *   `deployed_model.pause()`: Pauses a running model.\n",
      "    *   `deployed_model.resume()`: Resumes a paused model.\n",
      "    *   `deployed_model.delete(delete_output_topic=True)`: Deletes a model and optionally its associated output data.\n",
      "    *   `deployed_model.get_endpoints()`: Retrieves the API endpoints for a deployed model. These endpoints can be used for synchronous inference.\n",
      "    *   `deployed_model.get_logs()`: Retrieves logs for a deployed model.\n",
      "* **Inference:**\n",
      "    *   **Async:**\n",
      "        *   The data streamed from the input source is continuously fed to the model, and the outputs are streamed to another source.\n",
      "        *   ```python\n",
      "            outputs = deployed_model.get_outputs()\n",
      "            ```\n",
      "    *   **API:**\n",
      "        *   A request-response model is used for inference on a single data point synchronously.\n",
      "        *   The `/model_name/predict` endpoint is exposed for each deployed model where a REST API call can be made to get the outputs.\n",
      "            ```python\n",
      "            import requests\n",
      "            resp = requests.post(model_endpoints[0], json=model_query_datapoint, headers=tb.common.api.headers)\n",
      "            ```\n",
      "    *   **Batch:**\n",
      "        *   When you have multiple records you’d like to perform inference on, you can use the get_inference method as follows.\n",
      "            ```python\n",
      "            outputs = deployed_model.get_inference(query_df)\n",
      "            ```\n",
      "\n",
      "**5. Advanced Features:**\n",
      "\n",
      "*   **Hyperparameter Tuning:**\n",
      "    *   `tb.hyperparameter_tuning()`: Performs grid search to find the best combination of hyperparameters for a given model and dataset.\n",
      "*   **Algorithm Tuning:**\n",
      "    *   `tb.algorithm_tuning()`: Compares different models on a given dataset to identify the best-performing algorithm.\n",
      "* **Local Model:**\n",
      "    * The `LocalModel` class provides direct access to TurboML's machine learning models in Python, allowing for local training and prediction without deploying to the platform.\n",
      "    * Useful for offline experimentation and development.\n",
      "\n",
      "**Code Structure and Key Modules:**\n",
      "\n",
      "*   **`turboml`:** The main package, providing access to all core functionalities.\n",
      "*   **`common`:**\n",
      "    *   `api.py`: Handles communication with the TurboML backend API. Includes `ApiException`, `NotFoundException`, and retry logic.\n",
      "    *   `dataloader.py`: Functions for data loading, serialization, and interaction with the Arrow server. Includes `get_proto_msgs`, `create_protobuf_from_row_tuple`, `upload_df`, and more.\n",
      "    *   `datasets.py`: Defines dataset classes (`LocalDataset`, `OnlineDataset`, standard datasets like `FraudDetectionDatasetFeatures`).\n",
      "    *   `feature_engineering.py`: Classes and functions for defining and managing features (`FeatureEngineering`, `LocalFeatureEngineering`, `IbisFeatureEngineering`).\n",
      "    *   `internal.py`: Internal utilities (e.g., `TbPyArrow` for PyArrow helpers, `TbPandas` for Pandas helpers).\n",
      "    *   `llm.py`: Functions for working with Large Language Models (LLMs), including acquiring models from Hugging Face and spawning LLM servers.\n",
      "    *   `ml_algs.py`: Core ML algorithms, model deployment logic, and model classes (`Model`, `DeployedModel`, `LocalModel`). Includes functions like `ml_modelling`, `model_learn`, `model_predict`, `get_score_for_model`, and model classes for various algorithms.\n",
      "    *   `models.py`: Pydantic models for data structures and API requests/responses. Defines core data structures like `DatasetSchema`, `InputSpec`, `ModelConfigStorageRequest`, etc.\n",
      "    *   `namespaces.py`: Functions for managing namespaces.\n",
      "    *   `pymodel.py`: Python bindings for the underlying C++ model implementation (using `turboml_bindings`).\n",
      "    *   `pytypes.py`: Python type definitions (using `turboml_bindings`).\n",
      "    *   `udf.py`: Base class for defining custom metric aggregation functions (`ModelMetricAggregateFunction`).\n",
      "    *   `util.py`: General utility functions (e.g., type conversions, timestamp handling, Ibis utilities).\n",
      "    *   `default_model_configs.py`: Default configurations for ML algorithms.\n",
      "    *   `model_comparison.py`: Functions for comparing model performance (e.g., `compare_model_metrics`).\n",
      "    *   `env.py`: Environment configuration (e.g., server addresses).\n",
      "*   **`protos`:**\n",
      "    *   Contains Protobuf definitions (`.proto` files) and generated Python code (`_pb2.py`, `_pb2.pyi`) for data structures used in communication with the backend. Key files include `config.proto`, `input.proto`, `output.proto`, `metrics.proto`.\n",
      "*   **`wyo_models`:**\n",
      "    *   Contains examples of how to implement custom models using Python.\n",
      "*   **Subpackages for Algorithms:**\n",
      "    *   `regression`, `classification`, `anomaly_detection`, `ensembles`, `general_purpose`, `forecasting`: Contain specific ML algorithm implementations.\n",
      "*   **`pre_deployment_ml`:**\n",
      "    *   Contains modules for pre-deployment tasks like hyperparameter tuning and algorithm tuning.\n",
      "*   **`post_deployment_ml`:**\n",
      "    *   Contains modules for post-deployment tasks like drift detection, model explanations, and custom metrics.\n",
      "*   **`general_examples`:**\n",
      "    *   Contains examples of using TurboML features.\n",
      "*   **`non_numeric_inputs`:**\n",
      "    *   Contains examples of using non-numeric inputs like images and text.\n",
      "*   **`byo_models`:**\n",
      "    *   Contains examples of bringing your own models.\n",
      "*   **`llms`:**\n",
      "    *   Contains modules and examples for using LLMs.\n",
      "\n",
      "**Conclusion:**\n",
      "TurboML provides a robust and flexible platform for building and deploying real-time machine learning applications. Its strengths lie in its streaming-first architecture, support for diverse data sources and ML algorithms, comprehensive feature engineering capabilities (including SQL, aggregations, UDFs, and Ibis integration), and built-in MLOps features like model monitoring, drift detection, and evaluation. The platform's Python SDK and support for BYOM (Bring Your Own Model) via ONNX, gRPC, REST API, and custom Python code make it adaptable to a wide range of use cases and existing workflows. The combination of ease of use, performance, and real-time capabilities makes TurboML a powerful tool for developing and managing data-driven applications.\n",
      "\n",
      "# AMF Regressor\n",
      "@ TurboML - page_link: https://docs.turboml.com/regression/amfregressor/\n",
      "<page_content>\n",
      "Regression  \n",
      "AMF Regressor  \n",
      "**Aggregated Mondrian Forest** regressor for online learning.  \n",
      "This algorithm is truly online, in the sense that a single pass is performed, and that predictions can be produced anytime.  \n",
      "Each node in a tree predicts according to the average of the labels it contains. The prediction for a sample is computed as the aggregated predictions of all the subtrees along the path leading to the leaf node containing the sample. The aggregation weights are exponential weights with learning rate `step` using a squared loss when `use_aggregation` is `True`.  \n",
      "This computation is performed exactly thanks to a context tree weighting algorithm. More details can be found in the original paper[1](https://docs.turboml.com/regression/amfregressor/#user-content-fn-1).  \n",
      "The final predictions are the average of the predictions of each of the `n_estimators` trees in the forest.  \n",
      "## Parameters [Permalink for this section](https://docs.turboml.com/regression/amfregressor/\\#parameters)  \n",
      "- **n\\_estimators**( `int`, Default: `10`) → The number of trees in the forest.  \n",
      "- **step** ( `float`, Default: `1.0`) → Step-size for the aggregation weights.  \n",
      "- **use\\_aggregation**( `bool`, Default: `True`) → Controls if aggregation is used in the trees. It is highly recommended to leave it as `True`.  \n",
      "- **seed**( `int` \\| `None`, Default: `None`) → Random seed for reproducibility.  \n",
      "## Example Usage [Permalink for this section](https://docs.turboml.com/regression/amfregressor/\\#example-usage)  \n",
      "We can create an instance of the AMF Regressor model like this.  \n",
      "```import turboml as tb\n",
      "amf_model = tb.AMFRegressor()\n",
      "```  \n",
      "## Footnotes [Permalink for this section](https://docs.turboml.com/regression/amfregressor/\\#footnote-label)  \n",
      "1. Mourtada, J., Gaïffas, S., & Scornet, E. (2021). AMF: Aggregated Mondrian forests for online learning. Journal of the Royal Statistical Society Series B: Statistical Methodology, 83(3), 505-533. [↩](https://docs.turboml.com/regression/amfregressor/#user-content-fnref-1)  \n",
      "Last updated on January 24, 2025  \n",
      "[Random Cut Forest](https://docs.turboml.com/anomaly_detection/rcf/ \"Random Cut Forest\") [FFM Regressor](https://docs.turboml.com/regression/ffmregressor/ \"FFM Regressor\")\n",
      "</page_content>\n",
      "\n",
      "# AMF Classifier\n",
      "@ TurboML - page_link: https://docs.turboml.com/classification/amfclassifier/\n",
      "<page_content>\n",
      "Classification  \n",
      "AMF Classifier  \n",
      "**Aggregated Mondrian Forest** classifier for online learning.\n",
      "This implementation is truly online, in the sense that a single pass is performed, and that predictions can be produced anytime.  \n",
      "Each node in a _tree_ predicts according to the distribution of the labels it contains. This distribution is regularized using a **Jeffreys** prior with parameter `dirichlet`. For each class with count labels in the node and n\\_samples samples in it, the prediction of a node is given by  \n",
      "The prediction for a sample is computed as the aggregated predictions of all the subtrees along the path leading to the leaf node containing the sample. The aggregation weights are exponential weights with learning rate step and log-loss when use\\_aggregation is True.  \n",
      "This computation is performed exactly thanks to a context tree weighting algorithm. More details can be found in the paper cited in the reference[1](https://docs.turboml.com/classification/amfclassifier/#user-content-fn-1) below.  \n",
      "The final predictions are the average class probabilities predicted by each of the n\\_estimators trees in the forest.  \n",
      "## Parameters [Permalink for this section](https://docs.turboml.com/classification/amfclassifier/\\#parameters)  \n",
      "- **n\\_classes**( `int`) → The number of classes for classification.  \n",
      "- **n\\_estimators**( `int`, Default: `10`) → The number of trees in the forest.  \n",
      "- **step** ( `float`, Default: `1.0`) → Step-size for the aggregation weights. Default is 1 for classification with the log-loss, which is usually the best choice.  \n",
      "- **use\\_aggregation**( `bool`, Default: `True`) → Controls if aggregation is used in the trees. It is highly recommended to leave it as True.  \n",
      "- **dirichlet** ( `float`, Default: `0.5`) → Regularization level of the class frequencies used for predictions in each node. A rule of thumb is to set this to 1 / n\\_classes, where n\\_classes is the expected number of classes which might appear. Default is dirichlet = 0.5, which works well for binary classification problems.  \n",
      "- **split\\_pure**( `bool`, Default: `False`) → Controls if nodes that contains only sample of the same class should be split (\"pure\" nodes). Default is False, namely pure nodes are not split, but True can be sometimes better.  \n",
      "- **seed**( `int` \\| `None`, Default: `None`) → Random seed for reproducibility.  \n",
      "## Example Usage [Permalink for this section](https://docs.turboml.com/classification/amfclassifier/\\#example-usage)  \n",
      "We can simply use the below syntax to invoke the list of algorithms preconfigured in TurboML, here `have_labels=True` means supervised models.  \n",
      "```import turboml as tb\n",
      "amf_model = tb.AMFClassifier(n_classes=2)\n",
      "```  \n",
      "ℹ  \n",
      "Only log\\_loss used for the computation of the aggregation weights is supported for now, namely the log-loss for multi-class classification.  \n",
      "## Footnotes [Permalink for this section](https://docs.turboml.com/classification/amfclassifier/\\#footnote-label)  \n",
      "1. Mourtada, J., Gaïffas, S., & Scornet, E. (2021). AMF: Aggregated Mondrian forests for online learning. Journal of the Royal Statistical Society Series B: Statistical Methodology, 83(3), 505-533. [↩](https://docs.turboml.com/classification/amfclassifier/#user-content-fnref-1)  \n",
      "Last updated on January 24, 2025  \n",
      "[SGT Regressor](https://docs.turboml.com/regression/sgtregressor/ \"SGT Regressor\") [FFM Classifier](https://docs.turboml.com/classification/ffmclassifier/ \"FFM Classifier\")\n",
      "</page_content>\n",
      "\n",
      "# FFM Regressor\n",
      "@ TurboML - page_link: https://docs.turboml.com/regression/ffmregressor/\n",
      "<page_content>\n",
      "Regression  \n",
      "FFM Regressor  \n",
      "**Field-aware Factorization Machine** [1](https://docs.turboml.com/regression/ffmregressor/#user-content-fn-1) for regression.  \n",
      "The model equation is defined by:\n",
      "Where is the latent vector corresponding to feature for field, and is the latent vector corresponding to feature for field.\n",
      "`$$ \\sum_{f1=1}^{F} \\sum_{f2=f1+1}^{F} \\mathbf{w_{i1}} \\cdot \\mathbf{w_{i2}}, \\text{where } i1 = \\Phi(v_{f1}, f1, f2), \\quad i2 = \\Phi(v_{f2}, f2, f1) $$`\n",
      "Our implementation automatically applies MinMax scaling to the inputs, use normal distribution for latent initialization and squared loss for optimization.  \n",
      "## Parameters [Permalink for this section](https://docs.turboml.com/regression/ffmregressor/\\#parameters)  \n",
      "- **n\\_factors**( `int`, Default: `10`) → Dimensionality of the factorization or number of latent factors.  \n",
      "- **l1\\_weight**( `int`, Default: `0.0`) → Amount of L1 regularization used to push weights towards 0.  \n",
      "- **l2\\_weight**( `int`, Default: `0.0`) → Amount of L2 regularization used to push weights towards 0.  \n",
      "- **l1\\_latent**( `int`, Default: `0.0`) → Amount of L1 regularization used to push latent weights towards 0.  \n",
      "- **l2\\_latent**( `int`, Default: `0.0`) → Amount of L2 regularization used to push latent weights towards 0.  \n",
      "- **intercept**( `int`, Default: `0.0`) → Initial intercept value.  \n",
      "- **intercept\\_lr**( `float`, Default: `0.01`) → Learning rate scheduler used for updating the intercept. No intercept will be used if this is set to 0.  \n",
      "- **clip\\_gradient**(Default: `1000000000000.0`) → Clips the absolute value of each gradient value.  \n",
      "- **seed**( `int` \\| `None`, Default: `None`) → Randomization seed used for reproducibility.  \n",
      "## Example Usage [Permalink for this section](https://docs.turboml.com/regression/ffmregressor/\\#example-usage)  \n",
      "We can create an instance of the FFM model like this.  \n",
      "```import turboml as tb\n",
      "ffm_model = tb.FFMRegressor()\n",
      "```  \n",
      "## Footnotes [Permalink for this section](https://docs.turboml.com/regression/ffmregressor/\\#footnote-label)  \n",
      "1. Juan, Y., Zhuang, Y., Chin, W.S. and Lin, C.J., 2016, September. Field-aware factorization machines for CTR prediction. In Proceedings of the 10th ACM Conference on Recommender Systems (pp. 43-50). [↩](https://docs.turboml.com/regression/ffmregressor/#user-content-fnref-1)  \n",
      "Last updated on January 24, 2025  \n",
      "[AMF Regressor](https://docs.turboml.com/regression/amfregressor/ \"AMF Regressor\") [Hoeffding Tree Regressor](https://docs.turboml.com/regression/hoeffdingtreeregressor/ \"Hoeffding Tree Regressor\")\n",
      "</page_content>\n"
     ]
    }
   ],
   "source": [
    "# Check first merged context\n",
    "sample = merged_contexts[3]\n",
    "\n",
    "print(sample[\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
