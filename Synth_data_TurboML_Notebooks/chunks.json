[
    {
        "section": "What is TurboML?",
        "content": "# What is TurboML?\n@ TurboML - page_link: https://docs.turboml.com/intro/\n<page_content>\nIntroduction  \nTurboML is a machine learning platform that’s reinvented for real-time. What does that mean? All the steps in the ML lifecycle, from data ingestion, to feature engineering, to ML modelling to post deployment steps like monitoring, are all designed so that in addition to batch data, they can also handle real-time data.  \n## Data Ingestion [Permalink for this section](https://docs.turboml.com/intro/\\#data-ingestion)  \nThe first step is to bring your data to the TurboML platform. There are two major ways to ingest your data. Pull-based and Push-based.  \n### Pull-based ingestion [Permalink for this section](https://docs.turboml.com/intro/\\#pull-based-ingestion)  \nWith this approach, you use TurboML’s prebuilt connectors to connect to your data source. The connectors will continuously pull data from your data source, and ingest it into TurboML.  \n### Push-based ingestion [Permalink for this section](https://docs.turboml.com/intro/\\#push-based-ingestion)  \nSometimes, you might not want to send data via an intermediate data source, but rather directly send the data. Push-based ingestion can be used for this, where data can be send either via REST API calls, or using more performant client SDKs. Here’s an example with a Pandas DataFrame  \n```transactions = tb.PandasDataset(dataset_name=\"transactions\",dataframe=df, upload=True)\ntransactions.configure_dataset(key_field=\"index\")\n```  \n## Feature Engineering [Permalink for this section](https://docs.turboml.com/intro/\\#feature-engineering)  \nFeature engineering is perhaps the most important step for data scientists. TurboML provides several different interfaces to define features. We’ve designed the feature engineering experience in a way so that after you’ve defined a feature, you can see that feature computed for your local data. This should help debug and iterate faster. Once you’re confident about a feature definition, you can deploy it where it’ll be continuously computed on the real-time data. Once deployed, these features are automatically computed on the streaming data. And we have retrieval APIs to compute it for ad-hoc queries.  \n### SQL Features [Permalink for this section](https://docs.turboml.com/intro/\\#sql-features)  \nWriting SQL queries is one of the most common way to define ML features. TurboML supports writing arbitrary SQL expressions to enable such features. Here’s an example with a simple SQL feature.  \n```transactions.feature_engineering.create_sql_features(\nsql_definition='\"transactionAmount\" + \"localHour\"',\nnew_feature_name=\"my_sql_feat\",\n)\n```  \nNotice that the column names are in quotes.  \nAnd here’s a more complicated example  \n```transactions.feature_engineering.create_sql_features(\nsql_definition='CASE WHEN \"paymentBillingCountryCode\" <> \"ipCountryCode\" THEN 1 ELSE 0 END ',\nnew_feature_name=\"country_code_match\",\n)\n```  \n### Aggregate Features [Permalink for this section](https://docs.turboml.com/intro/\\#aggregate-features)  \nA common template for real-time features is aggregating some value over some time window. To define such time-windowed aggregations, you first need to register a timestamp column for your dataset. This can be done as follows,  \n```transactions.feature_engineering.register_timestamp(column_name=\"timestamp\", format_type=\"epoch_seconds\")\n```  \nThe supported formats can be found out using  \n```tb.get_timestamp_formats()\n```  \nOnce the timestamp is registered, we can create a feature using  \n```transactions.feature_engineering.create_aggregate_features(\ncolumn_to_operate=\"transactionAmount\",\ncolumn_to_group=\"accountID\",\noperation=\"SUM\",\nnew_feature_name=\"my_sum_feat\",\ntimestamp_column=\"timestamp\",\nwindow_duration=24,\nwindow_unit=\"hours\"\n)\n```  \n### User Defined Features [Permalink for this section](https://docs.turboml.com/intro/\\#user-defined-features)  \nWe understand why data scientists love Python - the simplicity, the ecosystem - is unmatchable. Guess what? You can use native Python, importing any library, [to define features](https://docs.turboml.com/feature_engineering/udf/)!  \n### IBIS Features [Permalink for this section](https://docs.turboml.com/intro/\\#ibis-features)  \nFor streaming features that are more complex than just windowed aggregations, can be defined using the [ibis interface](https://docs.turboml.com/feature_engineering/advanced/ibis_feature_engineering/). They can then be executed using Apache Flink or RisingWave.  \n### Feature Retrieval [Permalink for this section](https://docs.turboml.com/intro/\\#feature-retrieval)  \nAs mentioned before, once deployed, the feature computation is automatically added to the real-time streaming pipeline. However, feature values can also be retrieved on ad-hoc data using the retrieval API. Here’s an example  \n```features = tb.retrieve_features(\"transactions\", query_df)\n```  \n## ML Modelling - Basic concepts [Permalink for this section](https://docs.turboml.com/intro/\\#ml-modelling---basic-concepts)  \n### Inputs and Labels [Permalink for this section](https://docs.turboml.com/intro/\\#inputs-and-labels)  \nFor each model, we need to specify the Inputs and the Labels.  \n### Types of fields [Permalink for this section](https://docs.turboml.com/intro/\\#types-of-fields)  \nDifferent models can accept different types of input fields. The supported types of fields are, numeric, categoric, time series, text, and image.  \n### TurboML algorithms [Permalink for this section](https://docs.turboml.com/intro/\\#turboml-algorithms)  \nTurboML provides several algorithms out of the box. These algorithms are optimized for online predictions and learning, and have been tested on real-world settings.  \n```model = tb.HoeffdingTreeClassifier(n_classes=2)\n```  \n### Pytorch/TensorFlow/Scikit-learn [Permalink for this section](https://docs.turboml.com/intro/\\#pytorchtensorflowscikit-learn)  \nWe use ONNX to deploy trained models from [Pytorch](https://docs.turboml.com/byo_models/onnx_pytorch/), [TensorFlow](https://docs.turboml.com/byo_models/onnx_tensorflow/), [Scikit-learn](https://docs.turboml.com/byo_models/onnx_sklearn/) or other ONNX compatible frameworks. Example for these three frameworks can be found in the following notebooks.  \nNote: These models are static, and are not updated automatically.  \n### Python [Permalink for this section](https://docs.turboml.com/intro/\\#python)  \nTurboML also supports writing arbitrary Python code to define your own algorithms, including any libraries. To add your own algorithms, you need to define a Python class with 2 methods defined with the following signature:  \n```class Model:\ndef learn_one(self, features, label):\npass\n\ndef predict_one(self, features, output_data):\npass\n```  \nExamples of using an incremental learning algorithm, as well as a batch-like algorithm, can be found [here](https://docs.turboml.com/wyo_models/native_python_model/) from the river library.  \n### Combining models [Permalink for this section](https://docs.turboml.com/intro/\\#combining-models)  \nModels can also be combined to create other models, e.g. ensembles. An example of an ensemble model is as follows  \n```model = tb.LeveragingBaggingClassifier(n_classes=2, base_model = tb.HoeffdingTreeClassifier(n_classes=2))\n```  \nPreprocessors can also be chained and applied in a similar manner. E.g.  \n```model = tb.MinMaxPreProcessor(base_model = model)\n```  \n## Model Training [Permalink for this section](https://docs.turboml.com/intro/\\#model-training)  \nOnce we’ve defined a model, it can be trained in different ways.  \n### Batch way [Permalink for this section](https://docs.turboml.com/intro/\\#batch-way)  \nThe simplest way is to train the model in a batch way. This is similar to sklearn’s fit() method. However, internally the training is performed in an incremental manner. So, you can update an already trained model on some new data too. Here’s an example  \n```old_trained_model = model.learn(old_features, old_label)\nnew_trained_model = old_trained_model.learn(new_features, new_label)\n```  \nAny trained copy of the model can be then deployed to production.  \n```deployed_model = new_trained_model.deploy(name = \"deployment_name\", input=features, labels=label, predict_only=True)\n```  \nSince this is a trained model, we can also invoke this model in a batch way to get predictions without actually deploying the mode.  \n```outputs = new_trained_model.predict(query_features)\n```  \n### Streaming way [Permalink for this section](https://docs.turboml.com/intro/\\#streaming-way)  \nThis is where the model, after deployment, is continuously trained on new data. The user can choose how to update the model. The choices are online updates (where the model is updated on every new datapoint), or trigger-based updates which can be volume-based, time-based, performance-based or drift-based. The default option is online updates.  \n```deployed_model = model.deploy(name = \"deployment_name\", input=features, labels=label)\n```  \n## Deployment and MLOps [Permalink for this section](https://docs.turboml.com/intro/\\#deployment-and-mlops)  \n### Inference [Permalink for this section](https://docs.turboml.com/intro/\\#inference)  \nOnce you’ve deployed a mode, there are several different ways to perform inference.  \n#### Async [Permalink for this section](https://docs.turboml.com/intro/\\#async)  \nThe first one is the async method. The data that is streamed from the input source is continuously fed to the model, and the outputs are streamed to another source. This stream can be either be subscribed to directly be the end user application, or sinked to a database or other data sources.  \n```outputs = deployed_model.get_outputs()\n```  \n#### API [Permalink for this section](https://docs.turboml.com/intro/\\#api)  \nA request-response model is used for inference on a single data point synchronously. The `/model_name/predict` endpoint is exposed for each deployed model where a REST API call can be made to get the outputs.  \n#### Batch [Permalink for this section](https://docs.turboml.com/intro/\\#batch)  \nWhen you have multiple records you’d like to perform inference on, you can use the get\\_inference method as follows.  \n```outputs = deployed_model.get_inference(query_df)\n```  \n### Evaluation [Permalink for this section](https://docs.turboml.com/intro/\\#evaluation)  \nTurboML provides standard ML metrics out of the box to perform model evaluation. Multiple metrics can be registered for any deployed model. The metrics pipeline re-uses the labels extracted for model training.  \n```deployed_model.add_metric(\"WindowedAUC\")\nmodel_auc_scores = deployed_model.get_evaluation(\"WindowedAUC\")\n```  \nLast updated on January 24, 2025  \n[Quickstart](https://docs.turboml.com/quickstart/ \"Quickstart\")  \nWhat is TurboML? @ TurboML\n</page_content>"
    },
    {
        "section": "TurboML Quickstart",
        "content": "# TurboML Quickstart\n@ TurboML - page_link: https://docs.turboml.com/quickstart/\n<page_content>\nQuickstart  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/quickstart.ipynb)  \n```import turboml as tb\n```  \n## Inspecting Data [Permalink for this section](https://docs.turboml.com/quickstart/\\#inspecting-data)  \nTurboML is built for real-time machine learning, and as such, deals with streams of data. This can be achieved by using connectors to continuously pull data from your data source (like S3 or postgres), or use push-based approaches using REST API or Client SDKs.  \nFor the purpose of this tutorial, we can use simulate real-time data generation, with a batch-like setting using pandas dataframes. Let's first load some pandas dataframes. In this example, we're using a credit card fraud detection dataset.  \n```transactions_df = tb.datasets.FraudDetectionDatasetFeatures().df\nlabels_df = tb.datasets.FraudDetectionDatasetLabels().df\n```  \n```transactions_df\n```  \n```labels_df\n```  \nOur dataset has 201406 datapoints, each with a corresponding label. Since we don't have a natural primary key in the dataset that can uniquely identify each row, we'll use the inbuilt index that pandas provides.  \n```transactions_df.head()\n```  \n```labels_df.head()\n```  \n## Data Ingestion [Permalink for this section](https://docs.turboml.com/quickstart/\\#data-ingestion)  \nWe can now upload these dataframes to the TurboML platform, the **OnlineDataset** class can be used here. It takes in the dataframe, the primary key, and the name of the dataset that is to be created for the given dataframe as input.  \n```# Attempt to create and upload dataset\ntransactions = tb.OnlineDataset.from_pd(\nid=\"qs_transactions\",\ndf=transactions_df,\nkey_field=\"transactionID\",\nload_if_exists=True,\n)\nlabels = tb.OnlineDataset.from_pd(\nid=\"qs_transaction_labels\",\ndf=labels_df,\nkey_field=\"transactionID\",\nload_if_exists=True,\n)\n```  \n## Feature Engineering [Permalink for this section](https://docs.turboml.com/quickstart/\\#feature-engineering)  \nTurboML platform facilitates transformations on raw data to produce new features. You can use the jupyter notebook as a \"playground\" to explore different features. This involves 3 steps.  \n- **fetch data**: Experimentation is easier on static data. Since TurboML works with continuous data streams, to enable experimentation we fetch a snapshot or a subset of data in the jupyter notebook.\n- **add feature definitions**: Now that we have a static dataset, we can define multiple different features, and see their values on this dataset. Since we can observe their values, we can perform simple experiments and validations like correlations, plots and other exploratory analysis.\n- **submit feature definitions**: Once we're confident about the features we've defined, we can now submit the ones we want TurboML to compute continuously for the actual data stream.  \n### Fetch data [Permalink for this section](https://docs.turboml.com/quickstart/\\#fetch-data)  \nWe can use the **get\\_features** function to get a snapshot or subset of the data stream.  \n**Note**: This size of the dataset returned by this function can change on each invocation. Also, the dataset is not guaranteed to be in the same order.  \n### Add feature definitions [Permalink for this section](https://docs.turboml.com/quickstart/\\#add-feature-definitions)  \nTo add feature definitions, we have a class from turboml package called **FeatureEngineering**. This allows us to define SQL-based and dynamic aggregation-based features.  \nThe following cell shows how to define an SQL-based feature. The sql\\_definition parameter in the **create\\_sql\\_features** function takes in the SQL expression to be used to prepare the feature. It returns a dataframe with all the original columns, and another column which, on a high-level is defined as `SELECT sql_definition AS new_feature_name FROM dataframe`.  \n```transactions.feature_engineering.create_sql_features(\nsql_definition='\"transactionAmount\" + \"localHour\"',\nnew_feature_name=\"my_sql_feat\",\n)\n```  \n```transactions.feature_engineering.get_local_features()\n```  \n```tb.get_timestamp_formats()\n```  \n```transactions.feature_engineering.register_timestamp(\ncolumn_name=\"timestamp\", format_type=\"epoch_seconds\"\n)\n```  \nThe following cell shows how to define an aggregation-based feature using the **create\\_aggregate\\_features** function. It returns a dataframe with all the original columns, and another column which, on a high-level is defined as `SELECT operation(column_to_operate) OVER (PARTITION BY column_to_group ORDER BY time_column RANGE BETWEEN INTERVAL window_duration PRECEDING AND CURRENT ROW) as new_feature_name from dataframe`.  \n```transactions.feature_engineering.create_aggregate_features(\ncolumn_to_operate=\"transactionAmount\",\ncolumn_to_group=\"accountID\",\noperation=\"SUM\",\nnew_feature_name=\"my_sum_feat\",\ntimestamp_column=\"timestamp\",\nwindow_duration=24,\nwindow_unit=\"hours\",\n)\n```  \n```transactions.feature_engineering.get_local_features()\n```  \n### Submit feature definitions [Permalink for this section](https://docs.turboml.com/quickstart/\\#submit-feature-definitions)  \nNow that we've seen the newly created features, and everything looks good, we can submit these feature definitions to the TurboML platform so that this can be computed continously for the input data stream.  \nWe need to tell the platform to start computations for all pending features for the given dataset. This can be done by calling the **materialize\\_features** function.  \n```transactions.feature_engineering.materialize_features([\"my_sql_feat\", \"my_sum_feat\"])\n```  \n```df_transactions = transactions.feature_engineering.get_materialized_features()\ndf_transactions\n```  \n## Machine Learning Modelling [Permalink for this section](https://docs.turboml.com/quickstart/\\#machine-learning-modelling)  \nTurboML provides out of the box algorithms, optimized for real-time ML, and supports bringing your own models and algorithms as well. In this tutorial, we'll use the algorithms provided by TurboML.  \n### Check the available algorithms [Permalink for this section](https://docs.turboml.com/quickstart/\\#check-the-available-algorithms)  \nYou can check what are the available ML algorithms based on `tb.ml_algorithms(have_labels=True/False)` depending on supervised or unsupervised learning.  \n```tb.ml_algorithms(have_labels=False)\n```  \nLet's use the RandomCutForest (RCF) algorithm.  \n### Create model [Permalink for this section](https://docs.turboml.com/quickstart/\\#create-model)  \nNow that we've chosen an algorithm, we need to create a model.  \n```model = tb.RCF(number_of_trees=50)\n```  \n### Run Streaming ML jobs [Permalink for this section](https://docs.turboml.com/quickstart/\\#run-streaming-ml-jobs)  \nNow that we've instantiated the model, we can deploy it using the **deploy** function.\nFor an unsupervised ML job, we need to provide a dataset from which the model can consume inputs. For each record in this dataset, the model will make a prediction, produce the prediction to an output dataset, and then perform unsupervised updates using this record.  \nThere are four types of fields that can be used by any ML algorithm:  \n- numerical\\_fields: This represents fields that we want our algorithm to treat as real-valued fields.\n- categorical\\_fields: This represents fields that we want our algorithm to treat as categorical fields.\n- time\\_field: This is used for time-series applications to capture the timestamp field.\n- textual\\_fields: This represents fields that we want our algorithm to treat as text fields.  \nThe input values from any of these fields are suitably converted to the desired type. String values are converted using the hashing trick.  \nLet's construct a model config using the following numerical fields, no categorical or time fields.  \n```numerical_fields = [\\\n\"transactionAmount\",\\\n\"localHour\",\\\n\"my_sum_feat\",\\\n\"my_sql_feat\",\\\n]\nfeatures = transactions.get_model_inputs(numerical_fields=numerical_fields)\nlabel = labels.get_model_labels(label_field=\"is_fraud\")\n```  \n```deployed_model_rcf = model.deploy(name=\"demo_model_rcf\", input=features, labels=label)\n```  \n### Inspect model outputs [Permalink for this section](https://docs.turboml.com/quickstart/\\#inspect-model-outputs)  \nWe can now fetch the outputs that the model produced by calling the **get\\_outputs** function.  \n**Note**: This size of the outputs returned by this function can change on each invocation, since the model is continuosly producing outputs.  \n```outputs = deployed_model_rcf.get_outputs()\n```  \n```len(outputs)\n```  \n```sample_output = outputs[-1]\nsample_output\n```  \nThe above output corresponds to an input with the key, or index, sample\\_output.key. Along with the anomaly score, the output also contains attributions to different features. We can see that the first numerical feature, i.e. 'transactionAmount' is around sample\\_output.feature\\_score\\[0\\]\\*100% responsible for the anomaly score  \n```import matplotlib.pyplot as plt\n\nplt.plot([output[\"record\"].score for output in outputs])\n```  \n### Model Endpoints [Permalink for this section](https://docs.turboml.com/quickstart/\\#model-endpoints)  \nThe above method of interacting with the model was asynchronous. We were adding our datapoints to an input dataset, and getting the corresponding model outputs in an output dataset. In some scenarios, we need a synchronous method to query the model. This is where we can use the model endpoints that TurboML exposes.  \n```model_endpoints = deployed_model_rcf.get_endpoints()\nmodel_endpoints\n```  \nNow that we know what endpoint to send the request to, we now need to figure out the right format. Let's try to make a prediction on the last row from our input dataset.  \n```model_query_datapoint = transactions_df.iloc[-1].to_dict()\nmodel_query_datapoint\n```  \n```import requests\n\nresp = requests.post(\nmodel_endpoints[0], json=model_query_datapoint, headers=tb.common.api.headers\n)\n```  \n```resp.json()\n```  \n### Batch Inference on Models [Permalink for this section](https://docs.turboml.com/quickstart/\\#batch-inference-on-models)  \nWhile the above method is more suited for individual requests, we can also perform batch inference on the models. We use the **get\\_inference** function for this purpose.  \n```outputs = deployed_model_rcf.get_inference(transactions_df)\noutputs\n```  \n## Model Evaluation [Permalink for this section](https://docs.turboml.com/quickstart/\\#model-evaluation)  \nSimilar to ML models, TurboML provides in-built metrics, and supports defining your own metrics. Let's see the available metrics.  \n```tb.evaluation_metrics()\n```  \nWe can select the AreaUnderCurve (AUC) metric to evaluate our anomaly detection model. The windowed prefix means we're evaluating these metrics over a rolling window. By default, the window size is `1000`.  \n```deployed_model_rcf.add_metric(\"WindowedAUC\")\n```  \nSimilar to steps like feature engineering and ML modelling, model evaluation is also a continuosly running job. We can look at the snapshot of the model metrics at any given instance by using the **get\\_evaluation** function.  \n**Note**: This size of the outputs returned by this function can change on each invocation, since we're continuously evaluating the model.  \n```model_auc_scores = deployed_model_rcf.get_evaluation(\"WindowedAUC\")\nmodel_auc_scores[-1]\n```  \n```import matplotlib.pyplot as plt\n\nplt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])\n```  \n### Model Evaluation with filter and custom window size [Permalink for this section](https://docs.turboml.com/quickstart/\\#model-evaluation-with-filter-and-custom-window-size)  \nWe support running evaluation on filtered model data using valid SQL expression along with custom window size.  \n```model_auc_scores = deployed_model_rcf.get_evaluation(\n\"WindowedAUC\",\nfilter_expression=\"input_data.transactionCurrencyCode != 'USD' AND output_data.score > 0.6\",\nwindow_size=200,\n)\nmodel_auc_scores[-1]\n```  \n```import matplotlib.pyplot as plt\n\nplt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])\n```  \n## Supervised Learning [Permalink for this section](https://docs.turboml.com/quickstart/\\#supervised-learning)  \nLet's now take an example with a supervised learning algorithm. First, let's see what algorithms are supported out of the box.  \n```tb.ml_algorithms(have_labels=True)\n```  \nWe can use HoeffdingTreeClassifier to try to classify fraudulent and normal activity on the same dataset. First, we need to instantiate a model.  \n```htc_model = tb.HoeffdingTreeClassifier(n_classes=2)\n```  \nWe can use the same numerical fields in this model as well. However, let's add some categorical fields as well.  \n```categorical_fields = [\\\n\"digitalItemCount\",\\\n\"physicalItemCount\",\\\n\"isProxyIP\",\\\n]\nfeatures = transactions.get_model_inputs(\nnumerical_fields=numerical_fields, categorical_fields=categorical_fields\n)\nlabel = labels.get_model_labels(label_field=\"is_fraud\")\n```  \n### Run Supervised ML jobs [Permalink for this section](https://docs.turboml.com/quickstart/\\#run-supervised-ml-jobs)  \nSame as before, we can deploy this model with the **deploy** function.  \n```deployed_model_htc = htc_model.deploy(\"demo_classifier\", input=features, labels=label)\n```  \nWe can now inspect the outputs.  \n```outputs = deployed_model_htc.get_outputs()\n```  \n```len(outputs)\n```  \n```sample_output = outputs[-1]\nsample_output\n```  \nWe notice that since this is a classification model, we have some new attributes in the output, specifically `class_probabilities` and `predicted_class`. We also have the `score` attribute which, for classification, just shows us the probability for the last class.  \n### Supervised Model Endpoints [Permalink for this section](https://docs.turboml.com/quickstart/\\#supervised-model-endpoints)  \nPredict API for supervised models is exactly the same as unsupervised models.  \n```model_endpoints = deployed_model_htc.get_endpoints()\nmodel_endpoints\n```  \n```resp = requests.post(\nmodel_endpoints[0], json=model_query_datapoint, headers=tb.common.api.headers\n)\nresp.json()\n```  \n### Supervised Model Evaluation [Permalink for this section](https://docs.turboml.com/quickstart/\\#supervised-model-evaluation)  \nLet's now evaluate our supervised ML model. The process is exactly the same as for unsupervised model evaluation.  \n```deployed_model_htc.add_metric(\"WindowedAUC\")\n```  \nWe can use the same **get\\_evaluation** function to fetch the metrics for this model as well. Remember, this function retrieves the metric values present at that moment of time. So, if the number of records recieved seem low, just re-run this function.  \n```model_auc_scores = deployed_model_htc.get_evaluation(\"WindowedAUC\")\nmodel_auc_scores[-1]\n```  \n```plt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])\n```  \n## Model Comparison [Permalink for this section](https://docs.turboml.com/quickstart/\\#model-comparison)  \nNow that we have 2 models deployed, and we've registered metrics for both of them, we can compare them on real-time data. On each invocation, the following function will fetch the latest evaluations of the models and plot them.  \n```tb.compare_model_metrics(\nmodels=[deployed_model_rcf, deployed_model_htc], metric=\"WindowedAUC\"\n)\n```  \n## Model Deletion [Permalink for this section](https://docs.turboml.com/quickstart/\\#model-deletion)  \nWe can delete the models like this, by default the generated output is deleted. If you want to retain the output generated by model, use `delete_output_topic=False`.  \n```deployed_model_rcf.delete()\n```  \nLast updated on January 24, 2025  \n[Introduction](https://docs.turboml.com/intro/ \"Introduction\") [Batch API](https://docs.turboml.com/general_examples/batch_api/ \"Batch API\")\n</page_content>"
    },
    {
        "section": "String Encoding",
        "content": "# String Encoding\n@ TurboML - page_link: https://docs.turboml.com/non_numeric_inputs/string_encoding/\n<page_content>\nNon-Numeric Inputs  \nString Encoding  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/string_encoding.ipynb)  \nTextual data needs to be converted into numerical data to be used by ML models. For larger textual data like sentences and paragraphs, we saw in llm\\_embedding notebook how embeddings from pre-trained languages models can be used. But what about smaller strings, like country name? How do we use such strings as features in our ML models? This notebook covers different encoding methods that TurboML provides for textual features.  \n```import turboml as tb\n```  \n```transactions = tb.datasets.FraudDetectionDatasetFeatures().to_online(\nid=\"transactions\", load_if_exists=True\n)\nlabels = tb.datasets.FraudDetectionDatasetLabels().to_online(\nid=\"transaction_labels\", load_if_exists=True\n)\n```  \n```numerical_fields = [\\\n\"transactionAmount\",\\\n]\ntextual_fields = [\"transactionCurrencyCode\"]\nfeatures = transactions.get_model_inputs(\nnumerical_fields=numerical_fields, textual_fields=textual_fields\n)\nlabel = labels.get_model_labels(label_field=\"is_fraud\")\n```  \nNotice that now we're extracting a textual feature called transactionCurrencyCode from our dataset. To make sure that the model finally works with numerical data, we can define preprocessors that transform the textual data to numerical data via some encoding methods. By default, TurboML uses the hashing trick ( [https://en.wikipedia.org/wiki/Feature\\_hashing (opens in a new tab)](https://en.wikipedia.org/wiki/Feature_hashing)) to automatically hash and convert string data to numeric data. However, TurboML also supports popular encoding methods to handle strings including  \n- LabelPreProcessor\n- OneHotPreProcessor\n- TargetPreProcessor\n- FrequencyPreProcessor\n- BinaryPreProcessor  \nWe'll try an example using FrequencyPreProcessor. For these pre-processors, we need to specify in advance the cardinality of our data, which can be computed as follows.  \n```htc_model = tb.HoeffdingTreeClassifier(n_classes=2)\n```  \n```import pandas as pd\n\ndemo_classifier = tb.FrequencyPreProcessor(\ntext_categories=[\\\nlen(pd.unique(transactions.preview_df[col])) for col in textual_fields\\\n],\nbase_model=htc_model,\n)\n```  \n```deployed_model = demo_classifier.deploy(\n\"demo_classifier_htc\", input=features, labels=label\n)\n```  \n```outputs = deployed_model.get_outputs()\n```  \n```sample_output = outputs[-1]\nsample_output\n```  \n```\n```  \nLast updated on January 24, 2025  \n[LLM Tutorial](https://docs.turboml.com/llms/turboml_llm_tutorial/ \"LLM Tutorial\") [Image Input](https://docs.turboml.com/non_numeric_inputs/image_input/ \"Image Input\")\n</page_content>"
    },
    {
        "section": "AMF Regressor",
        "content": "# AMF Regressor\n@ TurboML - page_link: https://docs.turboml.com/regression/amfregressor/\n<page_content>\nRegression  \nAMF Regressor  \n**Aggregated Mondrian Forest** regressor for online learning.  \nThis algorithm is truly online, in the sense that a single pass is performed, and that predictions can be produced anytime.  \nEach node in a tree predicts according to the average of the labels it contains. The prediction for a sample is computed as the aggregated predictions of all the subtrees along the path leading to the leaf node containing the sample. The aggregation weights are exponential weights with learning rate `step` using a squared loss when `use_aggregation` is `True`.  \nThis computation is performed exactly thanks to a context tree weighting algorithm. More details can be found in the original paper[1](https://docs.turboml.com/regression/amfregressor/#user-content-fn-1).  \nThe final predictions are the average of the predictions of each of the `n_estimators` trees in the forest.  \n## Parameters [Permalink for this section](https://docs.turboml.com/regression/amfregressor/\\#parameters)  \n- **n\\_estimators**( `int`, Default: `10`) → The number of trees in the forest.  \n- **step** ( `float`, Default: `1.0`) → Step-size for the aggregation weights.  \n- **use\\_aggregation**( `bool`, Default: `True`) → Controls if aggregation is used in the trees. It is highly recommended to leave it as `True`.  \n- **seed**( `int` \\| `None`, Default: `None`) → Random seed for reproducibility.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/regression/amfregressor/\\#example-usage)  \nWe can create an instance of the AMF Regressor model like this.  \n```import turboml as tb\namf_model = tb.AMFRegressor()\n```  \n## Footnotes [Permalink for this section](https://docs.turboml.com/regression/amfregressor/\\#footnote-label)  \n1. Mourtada, J., Gaïffas, S., & Scornet, E. (2021). AMF: Aggregated Mondrian forests for online learning. Journal of the Royal Statistical Society Series B: Statistical Methodology, 83(3), 505-533. [↩](https://docs.turboml.com/regression/amfregressor/#user-content-fnref-1)  \nLast updated on January 24, 2025  \n[Random Cut Forest](https://docs.turboml.com/anomaly_detection/rcf/ \"Random Cut Forest\") [FFM Regressor](https://docs.turboml.com/regression/ffmregressor/ \"FFM Regressor\")\n</page_content>"
    },
    {
        "section": "MultinomialNB",
        "content": "# MultinomialNB\n@ TurboML - page_link: https://docs.turboml.com/classification/multinomialnb/\n<page_content>\nClassification  \nMultinomial Naive Bayes  \nNaive Bayes classifier for multinomial models.  \nMultinomial Naive Bayes model learns from occurrences between features such as word counts and discrete classes. The input vector must contain positive values, such as counts or TF-IDF values.  \n## Parameters [Permalink for this section](https://docs.turboml.com/classification/multinomialnb/\\#parameters)  \n- **n\\_classes**( `int`) → The number of classes for the classifier.  \n- **alpha**(Default: `1.0`) → Additive (Laplace/Lidstone) smoothing parameter (use 0 for no smoothing).  \n## Example Usage [Permalink for this section](https://docs.turboml.com/classification/multinomialnb/\\#example-usage)  \nWe can create an instance and deploy Multinomial NB model like this.  \n```import turboml as tb\nmodel = tb.MultinomialNB(n_classes=2)\n```  \nLast updated on January 24, 2025  \n[Gaussian Naive Bayes](https://docs.turboml.com/classification/gaussiannb/ \"Gaussian Naive Bayes\") [Hoeffding Tree Classifier](https://docs.turboml.com/classification/hoeffdingtreeclassifier/ \"Hoeffding Tree Classifier\")\n</page_content>"
    },
    {
        "section": "PreProcessors",
        "content": "# PreProcessors\n@ TurboML - page_link: https://docs.turboml.com/pipeline_components/preprocessors/\n<page_content>\nPipeline Components  \nPreProcessors  \nSince our preprocessors must also work with streaming data, we define preprocessors by combining them with a base model. Under the hood, we apply the transformation by the preprocessor, and pass the transformed inputs to the base model. This concept is similar to `Pipelines` in Scikit-Learn.  \n## MinMaxPreProcessor [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#minmaxpreprocessor)  \nWorks on numerical fields of the input. Scales them between 0 and 1, by maintaining running min and max for all numerical features.  \n### Parameters [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#parameters)  \n- **base\\_model**( `Model`) → The model to call after transforming the input.  \n### Example Usage [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#example-usage)  \nWe can create an instance of the MinMaxPreProcessor model like this.  \n```import turboml as tb\nembedding = tb.MinMaxPreProcessor(base_model=tb.HoeffdingTreeClassifier(n_classes=2))\n```  \n## NormalPreProcessor [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#normalpreprocessor)  \nWorks on numerical fields of the input. Scales the data so that it has zero mean and unit variance, by maintaining running mean and variance for all numerical features.  \n### Parameters [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#parameters-1)  \n- **base\\_model**( `Model`) → The model to call after transforming the input.  \n### Example Usage [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#example-usage-1)  \nWe can create an instance of the NormalPreProcessor model like this.  \n```import turboml as tb\nembedding = tb.NormalPreProcessor(base_model=tb.HoeffdingTreeClassifier(n_classes=2))\n```  \n## RobustPreProcessor [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#robustpreprocessor)  \nWorks on numerical fields of the input. Scales the data using statistics that are robust to outliers, by removing the running median and scaling by running interquantile range.  \n### Parameters [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#parameters-2)  \n- **base\\_model**( `Model`) → The model to call after transforming the input.  \n### Example Usage [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#example-usage-2)  \nWe can create an instance of the RobustPreProcessor model like this.  \n```import turboml as tb\nembedding = tb.RobustPreProcessor(base_model=tb.HoeffdingTreeClassifier(n_classes=2))\n```  \n## LabelPreProcessor [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#labelpreprocessor)  \nWorks on textual fields of the input. For each textual feature, we need to know in advance the cardinality of that feature. Converts the strings into ordinal integers. The resulting numbers are appended to the numerical features.  \n### Parameters [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#parameters-3)  \n- **base\\_model**( `Model`) → The model to call after transforming the input.  \n- **text\\_categories**( `List[int]`) → List of cardinalities for each textual feature.  \n### Example Usage [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#example-usage-3)  \nWe can create an instance of the LabelPreProcessor model like this.  \n```import turboml as tb\nembedding = tb.LabelPreProcessor(text_categories=[5, 10], base_model=tb.HoeffdingTreeClassifier(n_classes=2))\n```  \n## OneHotPreProcessor [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#onehotpreprocessor)  \nWorks on textual fields of the input. For each textual feature, we need to know in advance the cardinality of that feature. Converts the strings into one-hot encoding. The resulting numbers are appended to the numerical features.  \n### Parameters [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#parameters-4)  \n- **base\\_model**( `Model`) → The model to call after transforming the input.  \n- **text\\_categories**( `List[int]`) → List of cardinalities for each textual feature.  \n### Example Usage [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#example-usage-4)  \nWe can create an instance of the OneHotPreProcessor model like this.  \n```import turboml as tb\nembedding = tb.OneHotPreProcessor(text_categories=[5, 10], base_model=tb.HoeffdingTreeClassifier(n_classes=2))\n```  \n## BinaryPreProcessor [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#binarypreprocessor)  \nWorks on textual fields of the input. For each textual feature, we need to know in advance the cardinality of that feature. Converts the strings into binary encoding. The resulting numbers are appended to the numerical features.  \n### Parameters [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#parameters-5)  \n- **base\\_model**( `Model`) → The model to call after transforming the input.  \n- **text\\_categories**( `List[int]`) → List of cardinalities for each textual feature.  \n### Example Usage [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#example-usage-5)  \nWe can create an instance of the BinaryPreProcessor model like this.  \n```import turboml as tb\nembedding = tb.BinaryPreProcessor(text_categories=[5, 10], base_model=tb.HoeffdingTreeClassifier(n_classes=2))\n```  \n## FrequencyPreProcessor [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#frequencypreprocessor)  \nWorks on textual fields of the input. For each textual feature, we need to know in advance the cardinality of that feature. Converts the strings into their frequency based on the values seen so far. The resulting numbers are appended to the numerical features.  \n### Parameters [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#parameters-6)  \n- **base\\_model**( `Model`) → The model to call after transforming the input.  \n- **text\\_categories**( `List[int]`) → List of cardinalities for each textual feature.  \n### Example Usage [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#example-usage-6)  \nWe can create an instance of the FrequencyPreProcessor model like this.  \n```import turboml as tb\nembedding = tb.FrequencyPreProcessor(text_categories=[5, 10], base_model=tb.HoeffdingTreeClassifier(n_classes=2))\n```  \n## TargetPreProcessor [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#targetpreprocessor)  \nWorks on textual fields of the input. For each textual feature, we need to know in advance the cardinality of that feature. Converts the strings into average target value seen for them so far. The resulting numbers are appended to the numerical features.  \n### Parameters [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#parameters-7)  \n- **base\\_model**( `Model`) → The model to call after transforming the input.  \n- **text\\_categories**( `List[int]`) → List of cardinalities for each textual feature.  \n### Example Usage [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#example-usage-7)  \nWe can create an instance of the TargetPreProcessor model like this.  \n```import turboml as tb\nembedding = tb.TargetPreProcessor(text_categories=[5, 10], base_model=tb.HoeffdingTreeClassifier(n_classes=2))\n```  \n## LlamaCppPreProcessor [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#llamacpppreprocessor)  \nWorks on textual fields of the input. Converts the text features into their embeddings obtained from a pre-trained language model. The resulting embeddings are appended to the numerical features.  \n### Parameters [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#parameters-8)  \n- **base\\_model**( `Model`) → The model to call after transforming the input.  \n- **gguf\\_model\\_id**( `List[int]`) → A model id issued by `tb.acquire_hf_model_as_gguf`.  \n- **max\\_tokens\\_per\\_input**( `int`) → The maximum number of tokens to consider in the input text. Tokens beyond this limit will be truncated. Default is 512.  \n### Example Usage [Permalink for this section](https://docs.turboml.com/pipeline_components/preprocessors/\\#example-usage-8)  \nWe can create an instance of the LlamaCppPreProcessor model like this.  \n```import turboml as tb\nembedding = tb.LlamaCppPreProcessor(gguf_model_id=tb.acquire_hf_model_as_gguf(\"BAAI/bge-small-en-v1.5\", \"f16\"), max_tokens_per_input=512, base_model=tb.HoeffdingTreeClassifier(n_classes=2))\n```  \nLast updated on January 24, 2025  \n[One-Vs-Rest](https://docs.turboml.com/pipeline_components/ovr/ \"One-Vs-Rest\") [Embedding Model](https://docs.turboml.com/pipeline_components/embeddingmodel/ \"Embedding Model\")\n</page_content>"
    },
    {
        "section": "Python Model: Batch Example",
        "content": "# Python Model: Batch Example\n@ TurboML - page_link: https://docs.turboml.com/wyo_models/batch_python_model/\n<page_content>\nWrite Your Own Models  \nBatch Python Model  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/batch_python_model.ipynb)  \nIn this example we emulate batch training of custom models defined using TurboML's `Python` model.  \n```import turboml as tb\n```  \n```import pandas as pd\nimport numpy as np\n```  \n## Model Definition [Permalink for this section](https://docs.turboml.com/wyo_models/batch_python_model/\\#model-definition)  \nHere we define `MyBatchModel` with buffers to store the input features and labels until we exceed our buffer limit. Then, the model can be brained all at once on the buffered samples.  \nWe use `Scikit-Learn`'s `Perceptron` for this task.  \n```from sklearn.linear_model import Perceptron\nimport turboml.common.pytypes as types\n\n\nclass MyBatchModel:\ndef __init__(self):\nself.model = Perceptron()\nself.X_buffer = []\nself.y_buffer = []\nself.batch_size = 64\nself.trained = False\n\ndef init_imports(self):\nfrom sklearn.linear_model import Perceptron\nimport numpy as np\n\ndef learn_one(self, input: types.InputData):\nself.X_buffer.append(input.numeric)\nself.y_buffer.append(input.label)\n\nif len(self.X_buffer) >= self.batch_size:\nself.model = self.model.partial_fit(\nnp.array(self.X_buffer), np.array(self.y_buffer), classes=[0, 1]\n)\n\nself.X_buffer = []\nself.y_buffer = []\n\nself.trained = True\n\ndef predict_one(self, input: types.InputData, output: types.OutputData):\nif self.trained:\nprediction = self.model.predict(np.array(input.numeric).reshape(1, -1))[0]\n\noutput.set_predicted_class(prediction)\nelse:\noutput.set_score(0.0)\n```  \nNow, we define a custom virtual environment with the correct list of dependencies which the model will be using, and link our model to this `venv`.  \n```venv = tb.setup_venv(\"my_batch_python_venv\", [\"scikit-learn\", \"numpy<2\"])\nvenv.add_python_class(MyBatchModel)\n```  \n## Model Deployment [Permalink for this section](https://docs.turboml.com/wyo_models/batch_python_model/\\#model-deployment)  \nOnce the virtual environment is ready, we prepare the dataset to be used in this task and deploy the model with its features and labels.  \n```batch_model = tb.Python(class_name=MyBatchModel.__name__, venv_name=venv.name)\n```  \n```transactions = tb.datasets.FraudDetectionDatasetFeatures().to_online(\n\"transactions\", load_if_exists=True\n)\nlabels = tb.datasets.FraudDetectionDatasetLabels().to_online(\n\"transaction_labels\", load_if_exists=True\n)\n```  \n```numerical_fields = [\\\n\"transactionAmount\",\\\n\"localHour\",\\\n\"isProxyIP\",\\\n\"digitalItemCount\",\\\n\"physicalItemCount\",\\\n]\nfeatures = transactions.get_model_inputs(numerical_fields=numerical_fields)\nlabel = labels.get_model_labels(label_field=\"is_fraud\")\n```  \n```deployed_batch_model = batch_model.deploy(\"batch_model\", input=features, labels=label)\n```  \n## Evaluation [Permalink for this section](https://docs.turboml.com/wyo_models/batch_python_model/\\#evaluation)  \n```import matplotlib.pyplot as plt\n\ndeployed_batch_model.add_metric(\"WindowedRMSE\")\nmodel_auc_scores = deployed_batch_model.get_evaluation(\"WindowedRMSE\")\nplt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])\n```  \nLast updated on January 24, 2025  \n[Ensemble Python Model](https://docs.turboml.com/wyo_models/ensemble_python_model/ \"Ensemble Python Model\") [PySAD Example](https://docs.turboml.com/wyo_models/pysad_example/ \"PySAD Example\")\n</page_content>"
    },
    {
        "section": "Image Processing (MNIST Example)",
        "content": "# Image Processing (MNIST Example)\n@ TurboML - page_link: https://docs.turboml.com/llms/image_embeddings/\n<page_content>\nLLMs  \nImage Embeddings  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/image_embeddings.ipynb)  \n```import turboml as tb\n```  \n```import pandas as pd\nfrom torchvision import datasets, transforms\nimport io\nfrom PIL import Image\n```  \n```class PILToBytes:\ndef __init__(self, format=\"JPEG\"):\nself.format = format\n\ndef __call__(self, img):\nif not isinstance(img, Image.Image):\nraise TypeError(f\"Input should be a PIL Image, but got {type(img)}.\")\nbuffer = io.BytesIO()\nimg.save(buffer, format=self.format)\nreturn buffer.getvalue()\n\n\ntransform = transforms.Compose(\n[\\\ntransforms.Resize((28, 28)),\\\nPILToBytes(format=\"PNG\"),\\\n]\n)\n```  \n## Data Inspection [Permalink for this section](https://docs.turboml.com/llms/image_embeddings/\\#data-inspection)  \nDownloading the MNIST dataset to be used in ML modelling.  \n```mnist_dataset_train = datasets.MNIST(\nroot=\"./data\", train=True, download=True, transform=transform\n)\nmnist_dataset_test = datasets.MNIST(\nroot=\"./data\", train=False, download=True, transform=transform\n)\n```  \n```images_train = []\nimages_test = []\nlabels_train = []\nlabels_test = []\n\nfor image, label in mnist_dataset_train:\nimages_train.append(image)\nlabels_train.append(label)\n\nfor image, label in mnist_dataset_test:\nimages_test.append(image)\nlabels_test.append(label)\n```  \nTransforming the lists into Pandas DataFrames.  \n```image_dict_train = {\"images\": images_train}\nlabel_dict_train = {\"labels\": labels_train}\nimage_df_train = pd.DataFrame(image_dict_train)\nlabel_df_train = pd.DataFrame(label_dict_train)\n\nimage_dict_test = {\"images\": images_test}\nlabel_dict_test = {\"labels\": labels_test}\nimage_df_test = pd.DataFrame(image_dict_test)\nlabel_df_test = pd.DataFrame(label_dict_test)\n```  \nAdding index columns to the DataFrames to act as primary keys for the datasets.  \n```image_df_train.reset_index(inplace=True)\nlabel_df_train.reset_index(inplace=True)\n\nimage_df_test.reset_index(inplace=True)\nlabel_df_test.reset_index(inplace=True)\n```  \n```image_df_train.head()\n```  \n```image_df_test.head()\n```  \n```image_df_test = image_df_test[:5].reset_index(drop=True)\nlabel_df_test = label_df_test[:5].reset_index(drop=True)\n```  \nUsing `LocalDataset` class for compatibility with the TurboML platform.  \n```images_train = tb.LocalDataset.from_pd(df=image_df_train, key_field=\"index\")\nlabels_train = tb.LocalDataset.from_pd(df=label_df_train, key_field=\"index\")\n\nimages_test = tb.LocalDataset.from_pd(df=image_df_test, key_field=\"index\")\nlabels_test = tb.LocalDataset.from_pd(df=label_df_test, key_field=\"index\")\n```  \nExtracting the features and the targets from the TurboML-compatible datasets.  \n```imaginal_fields = [\"images\"]\n\nfeatures_train = images_train.get_model_inputs(imaginal_fields=imaginal_fields)\ntargets_train = labels_train.get_model_labels(label_field=\"labels\")\n\nfeatures_test = images_test.get_model_inputs(imaginal_fields=imaginal_fields)\ntargets_test = labels_test.get_model_labels(label_field=\"labels\")\n```  \n## Clip Model Initialization [Permalink for this section](https://docs.turboml.com/llms/image_embeddings/\\#clip-model-initialization)  \nWe Simply create a ClipEmbedding model with gguf\\_model. The CLIP model is pulled from the Huggingface repository. As it is already quantized, we can directly pass the model file name in 'select\\_model\\_file' parameter.  \n```gguf_model = tb.llm.acquire_hf_model_as_gguf(\n\"xtuner/llava-llama-3-8b-v1_1-gguf\", \"auto\", \"llava-llama-3-8b-v1_1-mmproj-f16.gguf\"\n)\ngguf_model\n```  \n```model = tb.ClipEmbedding(gguf_model_id=gguf_model)\n```  \n## Model Training [Permalink for this section](https://docs.turboml.com/llms/image_embeddings/\\#model-training)  \nSetting the model combined with the `ImageToNumeric PreProcessor` to learn on the training data.  \n```model = model.learn(features_train, targets_train)\n```  \n## Model Inference [Permalink for this section](https://docs.turboml.com/llms/image_embeddings/\\#model-inference)  \nPerforming inference on the trained model using the test data.  \n```outputs_test = model.predict(features_test)\n```  \n```outputs_test\n```  \nLast updated on January 24, 2025  \n[LLM Embeddings](https://docs.turboml.com/llms/llm_embedding/ \"LLM Embeddings\") [LLM Tutorial](https://docs.turboml.com/llms/turboml_llm_tutorial/ \"LLM Tutorial\")\n</page_content>"
    },
    {
        "section": "Stream Dataset to Deployed Models",
        "content": "# Stream Dataset to Deployed Models\n@ TurboML - page_link: https://docs.turboml.com/general_examples/stream_dataset_online/\n<page_content>\nGeneral  \nStream Dataset Online  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/stream_dataset_online.ipynb)  \nThis notebook demonstrates how to upload data to an already registered dataset with a deployed model.  \n```import turboml as tb\n```  \n```import pandas as pd\nimport requests\nimport time\n\nfrom tqdm.notebook import tqdm\n```  \n```# Add helper functions\ndef do_retry(\noperation,\nreturn_on: lambda result: True,\nretry_count=3,\nsleep_seconds=3,\n):\nattempt = 1\nwhile attempt <= retry_count:\nprint(f\"## Attempt {attempt} of {retry_count}.\")\nresult = operation()\nif return_on(result):\nprint(f\"## Finished in {attempt} attempt.\")\nreturn result\nelse:\ntime.sleep(sleep_seconds)\nattempt += 1\ncontinue\nprint(f\"## Exiting after {attempt} attempts.\")\n\n\ndef simulate_realtime_stream(df: pd.DataFrame, chunk_size: int, delay: float):\n# Number of chunks to yield\nnum_chunks = len(df) // chunk_size + (1 if len(df) % chunk_size != 0 else 0)\n\nfor i in range(num_chunks):\n# Yield the chunk of DataFrame\nchunk = df.iloc[i * chunk_size : (i + 1) * chunk_size]\nyield chunk\n\n# Simulate real-time delay\ntime.sleep(delay)\n```  \n## Inspecting Data [Permalink for this section](https://docs.turboml.com/general_examples/stream_dataset_online/\\#inspecting-data)  \n```transactions_df = tb.datasets.FraudDetectionDatasetFeatures().df\nlabels_df = tb.datasets.FraudDetectionDatasetLabels().df\n```  \nWe will only use a subset of the dataset for initial model deployment.  \n```sub_transactions_df = transactions_df.iloc[0:20000]\nsub_transactions_df = sub_transactions_df\nsub_transactions_df\n```  \n```sub_labels_df = labels_df.iloc[0:20000]\nsub_labels_df = sub_labels_df\nsub_labels_df\n```  \n```sub_transactions_df.head()\n```  \n```sub_labels_df.head()\n```  \n## Data Ingestion [Permalink for this section](https://docs.turboml.com/general_examples/stream_dataset_online/\\#data-ingestion)  \n```input_dataset_id = \"transactions_stream_online\"\n\ntransactions = tb.OnlineDataset.from_pd(\ndf=sub_transactions_df,\nid=input_dataset_id,\nkey_field=\"transactionID\",\nload_if_exists=True,\n)\n```  \n```label_dataset_id = \"transaction_stream_labels\"\nlabels = tb.OnlineDataset.from_pd(\ndf=sub_labels_df,\nid=label_dataset_id,\nkey_field=\"transactionID\",\nload_if_exists=True,\n)\n```  \n## Feature Engineering [Permalink for this section](https://docs.turboml.com/general_examples/stream_dataset_online/\\#feature-engineering)  \n### Fetch data [Permalink for this section](https://docs.turboml.com/general_examples/stream_dataset_online/\\#fetch-data)  \n```tb.get_features(dataset_id=input_dataset_id)\n```  \n### Add feature definitions [Permalink for this section](https://docs.turboml.com/general_examples/stream_dataset_online/\\#add-feature-definitions)  \n```transactions.feature_engineering.create_sql_features(\nsql_definition='\"transactionAmount\" + \"localHour\"',\nnew_feature_name=\"my_sql_feat\",\n)\n```  \n```transactions.feature_engineering.get_local_features()\n```  \n```tb.get_timestamp_formats()\n```  \n```transactions.feature_engineering.register_timestamp(\ncolumn_name=\"timestamp\", format_type=\"epoch_seconds\"\n)\n```  \n```transactions.feature_engineering.create_aggregate_features(\ncolumn_to_operate=\"transactionAmount\",\ncolumn_to_group=\"accountID\",\noperation=\"SUM\",\nnew_feature_name=\"my_sum_feat\",\ntimestamp_column=\"timestamp\",\nwindow_duration=24,\nwindow_unit=\"hours\",\n)\n```  \n```transactions.feature_engineering.get_local_features()\n```  \n### Submit feature definitions [Permalink for this section](https://docs.turboml.com/general_examples/stream_dataset_online/\\#submit-feature-definitions)  \n```transactions.feature_engineering.materialize_features([\"my_sql_feat\", \"my_sum_feat\"])\n```  \n```materialized_features = transactions.feature_engineering.get_materialized_features()\nmaterialized_features\n```  \n## Supervised Learning [Permalink for this section](https://docs.turboml.com/general_examples/stream_dataset_online/\\#supervised-learning)  \n```htc_model = tb.HoeffdingTreeClassifier(n_classes=2)\n```  \n```numerical_fields = [\\\n\"transactionAmount\",\\\n\"localHour\",\\\n\"my_sum_feat\",\\\n\"my_sql_feat\",\\\n]\ncategorical_fields = [\\\n\"digitalItemCount\",\\\n\"physicalItemCount\",\\\n\"isProxyIP\",\\\n]\nfeatures = transactions.get_model_inputs(\nnumerical_fields=numerical_fields, categorical_fields=categorical_fields\n)\nlabel = labels.get_model_labels(label_field=\"is_fraud\")\n```  \n### Run Supervised ML jobs [Permalink for this section](https://docs.turboml.com/general_examples/stream_dataset_online/\\#run-supervised-ml-jobs)  \nWe will deploy a HoeffdingTreeClassifier Model trained on a subset of our dataset.  \n```deployed_model_htc = htc_model.deploy(\n\"demo_classifier_htc_stream_model\", input=features, labels=label\n)\n```  \n```outputs = do_retry(\ndeployed_model_htc.get_outputs, return_on=(lambda result: len(result) > 0)\n)\n```  \n```outputs[-1]\n```  \n### Supervised Model Endpoints [Permalink for this section](https://docs.turboml.com/general_examples/stream_dataset_online/\\#supervised-model-endpoints)  \n```model_endpoints = deployed_model_htc.get_endpoints()\nmodel_endpoints\n```  \n```model_query_datapoint = transactions_df.iloc[765].to_dict()\nmodel_query_datapoint\n```  \n```resp = requests.post(\nmodel_endpoints[0], json=model_query_datapoint, headers=tb.common.api.headers\n)\nresp.json()\n```  \n### Supervised Model Evaluation [Permalink for this section](https://docs.turboml.com/general_examples/stream_dataset_online/\\#supervised-model-evaluation)  \n```deployed_model_htc.add_metric(\"WindowedAUC\")\n```  \n```model_auc_scores = do_retry(\nlambda: deployed_model_htc.get_evaluation(\"WindowedAUC\"),\nreturn_on=(lambda result: len(result) > 0),\n)\n```  \n```import matplotlib.pyplot as plt\n\nplt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])\n```  \n## Upload to dataset with online model [Permalink for this section](https://docs.turboml.com/general_examples/stream_dataset_online/\\#upload-to-dataset-with-online-model)  \nWe will upload data to the registered dataset, which will be used for training and inference by the respective deployed model in realtime.  \nWe use a helper function `simulate_realtime_stream` to simulate realtime streaming data from dataframe.  \n### Upload using SDK [Permalink for this section](https://docs.turboml.com/general_examples/stream_dataset_online/\\#upload-using-sdk)  \nHere we use the **upload\\_df** method provided by the **OnlineDataset** class to upload data to a registered dataset. This method internally uploads the data using the **Arrow Flight Protocol** over gRPC.  \n```sub_transactions_df = transactions_df.iloc[20000:100000]\nsub_transactions_df = sub_transactions_df\nsub_transactions_df\n```  \n```sub_labels_df = labels_df.iloc[20000:100000]\nsub_labels_df = sub_labels_df\nsub_labels_df\n```  \nSet the chunk size and delay for the `simulate_realtime_stream` helper function  \n```chunk_size = 10 * 1024\ndelay = 0.1\n```  \nHere we zip the two stream generators to get a batch of dataframe for input and label datasets and we upload them.  \n```sub_labels_df\n```  \n```# lets normalize these dfs to replace any nan's with 0-values for the corresponding type\nsub_transactions_df = tb.datasets.PandasHelpers.normalize_df(sub_transactions_df)\nsub_labels_df = tb.datasets.PandasHelpers.normalize_df(sub_labels_df)\n\nrealtime_input_stream = simulate_realtime_stream(sub_transactions_df, chunk_size, delay)\nrealtime_label_stream = simulate_realtime_stream(sub_labels_df, chunk_size, delay)\n\nwith tqdm(\ntotal=len(sub_transactions_df), desc=\"Progress\", unit=\"rows\", unit_scale=True\n) as pbar:\nfor input_stream, label_stream in zip(\nrealtime_input_stream, realtime_label_stream, strict=True\n):\nstart = time.perf_counter()\ntransactions.add_pd(input_stream)\nlabels.add_pd(label_stream)\nend = time.perf_counter()\n\npbar.update(len(input_stream))\nprint(\nf\"# Uploaded {len(input_stream)} input, label rows for processing in {end - start:.6f} seconds.\"\n)\n```  \n#### Check Updated Dataset and Model [Permalink for this section](https://docs.turboml.com/general_examples/stream_dataset_online/\\#check-updated-dataset-and-model)  \n```tb.get_features(dataset_id=input_dataset_id)\n```  \nWe can use the **sync\\_features** method to sync the materialized streaming features to the **OnlineDataset** object.  \n```time.sleep(1)\ntransactions.sync_features()\n```  \nCalling **get\\_materialized\\_features** method will show that newly uploaded data is properly materialized.  \n```materialized_features = transactions.feature_engineering.get_materialized_features()\nmaterialized_features\n```  \nThe **get\\_ouputs** method will return the latest processed ouput.  \n```outputs = do_retry(\ndeployed_model_htc.get_outputs, return_on=(lambda result: len(result) > 0)\n)\noutputs[-1]\n```  \n```model_auc_scores = deployed_model_htc.get_evaluation(\"WindowedAUC\")\nprint(len(model_auc_scores))\nplt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])\n```  \n```resp = requests.post(\nmodel_endpoints[0], json=model_query_datapoint, headers=tb.common.api.headers\n)\nresp.json()\n```  \n### Upload using REST API [Permalink for this section](https://docs.turboml.com/general_examples/stream_dataset_online/\\#upload-using-rest-api)  \nHere we use the **dataset/dataset\\_id/upload** REST API endpoint to upload data to a registered dataset. This endpoint will directly upload the data to the registered **dataset kafka topic**.  \n```sub_transactions_df = transactions_df.iloc[100000:170000]\nsub_transactions_df = sub_transactions_df\nsub_transactions_df\n```  \n```sub_labels_df = labels_df.iloc[100000:170000]\nsub_labels_df = sub_labels_df\nsub_labels_df\n```  \n```from turboml.common.api import api\nimport json\n```  \nWe use the turboml api module to initiate the HTTP call, since auth is already configured for it.  \n```def rest_upload_df(dataset_id: str, df: pd.DataFrame):\nrow_list = json.loads(df.to_json(orient=\"records\"))\napi.post(f\"dataset/{dataset_id}/upload\", json=row_list)\n```  \n```realtime_input_stream = simulate_realtime_stream(sub_transactions_df, chunk_size, delay)\nrealtime_label_stream = simulate_realtime_stream(sub_labels_df, chunk_size, delay)\n\nwith tqdm(\ntotal=len(sub_transactions_df), desc=\"Progress\", unit=\"rows\", unit_scale=True\n) as pbar:\nfor input_stream, label_stream in zip(\nrealtime_input_stream, realtime_label_stream, strict=True\n):\nstart = time.perf_counter()\nrest_upload_df(input_dataset_id, input_stream)\nrest_upload_df(label_dataset_id, label_stream)\nend = time.perf_counter()\n\npbar.update(len(input_stream))\nprint(\nf\"# Uploaded {len(input_stream)} input, label rows for processing in {end - start:.6f} seconds.\"\n)\n```  \n#### Check Updated Dataset and Model [Permalink for this section](https://docs.turboml.com/general_examples/stream_dataset_online/\\#check-updated-dataset-and-model-1)  \n```time.sleep(1)\ntransactions.sync_features()\n```  \n```materialized_features = transactions.feature_engineering.get_materialized_features()\nmaterialized_features\n```  \n```outputs = do_retry(\ndeployed_model_htc.get_outputs, return_on=(lambda result: len(result) > 0)\n)\n```  \n```outputs[-1]\n```  \n```model_auc_scores = deployed_model_htc.get_evaluation(\"WindowedAUC\")\nprint(len(model_auc_scores))\nplt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])\n```  \n```deployed_model_htc.get_inference(transactions_df)\n```  \n### Upload using gRPC API [Permalink for this section](https://docs.turboml.com/general_examples/stream_dataset_online/\\#upload-using-grpc-api)  \nThis example shows how to directly upload data to the registered dataset using Arrow Flight gRPC.  \n```sub_transactions_df = transactions_df.iloc[170000:]\nsub_transactions_df = sub_transactions_df\nsub_transactions_df\n```  \n```sub_labels_df = labels_df.iloc[170000:]\nsub_labels_df = sub_labels_df\nsub_labels_df\n```  \n```import pyarrow\nimport struct\nimport itertools\nfrom functools import partial\nfrom pyarrow.flight import FlightDescriptor\n\nfrom turboml.common.env import CONFIG as tb_config\nfrom turboml.common import get_protobuf_class, create_protobuf_from_row_tuple\n```  \nHere we have defined a helper function `write_batch` to write pyarrow record batch given a pyarrow flight client instance.  \n```def write_batch(writer, df, proto_gen_partial_func):\nrow_iter = df.itertuples(index=False, name=None)\nbatch_size = 1024\nwhile True:\nbatch = list(\nmap(\nproto_gen_partial_func,\nitertools.islice(row_iter, batch_size),\n)\n)\n\nif not batch:\nbreak\n\nbatch = pyarrow.RecordBatch.from_arrays([batch], [\"value\"])\nwriter.write(batch)\n```  \nWe initiate connection for the pyarrow flight client to the TurboML arrow server with the required configs.  \n```arrow_server_grpc_endpoint = tb_config.ARROW_SERVER_ADDRESS\n\n# Note: SchemaId prefix is required for proper kafka protobuf serialization.\ninput_proto_gen_func = partial(\ncreate_protobuf_from_row_tuple,\nfields=sub_transactions_df.columns.tolist(),\nproto_cls=transactions.protobuf_cls,\nprefix=struct.pack(\"!xIx\", transactions.registered_schema.id),\n)\n\nlabel_proto_gen_func = partial(\ncreate_protobuf_from_row_tuple,\nfields=sub_labels_df.columns.tolist(),\nproto_cls=labels.protobuf_cls,\nprefix=struct.pack(\"!xIx\", labels.registered_schema.id),\n)\n\nclient = pyarrow.flight.connect(arrow_server_grpc_endpoint)\n# Note: Expected arrow schema is a column named 'value' with serialized protobuf binary message.\npa_schema = pyarrow.schema([(\"value\", pyarrow.binary())])\n\ninput_stream_writer, _ = client.do_put(\nFlightDescriptor.for_command(f\"produce:{input_dataset_id}\"),\npa_schema,\noptions=pyarrow.flight.FlightCallOptions(headers=api.arrow_headers),\n)\n\nlabel_stream_writer, _ = client.do_put(\nFlightDescriptor.for_command(f\"produce:{label_dataset_id}\"),\npa_schema,\noptions=pyarrow.flight.FlightCallOptions(headers=api.arrow_headers),\n)\n```  \nNow, we use the stream generator and pass the data to the `write_batch` function along with **pyarrow client write handler** for for both input and label data writes respectively.  \n```realtime_input_stream = simulate_realtime_stream(sub_transactions_df, chunk_size, delay)\nrealtime_label_stream = simulate_realtime_stream(sub_labels_df, chunk_size, delay)\n\nwith tqdm(\ntotal=len(sub_transactions_df), desc=\"Progress\", unit=\"rows\", unit_scale=True\n) as pbar:\nfor input_stream, label_stream in zip(\nrealtime_input_stream, realtime_label_stream, strict=True\n):\nstart = time.perf_counter()\nwrite_batch(input_stream_writer, input_stream, input_proto_gen_func)\nwrite_batch(label_stream_writer, label_stream, label_proto_gen_func)\nend = time.perf_counter()\n\npbar.update(len(input_stream))\nprint(\nf\"# Uploaded {len(input_stream)} input, label rows for processing in {end - start:.6f} seconds.\"\n)\n```  \nClose the pyarrow client write handlers.  \n```input_stream_writer.close()\nlabel_stream_writer.close()\n```  \n#### Check Updated Dataset and Model [Permalink for this section](https://docs.turboml.com/general_examples/stream_dataset_online/\\#check-updated-dataset-and-model-2)  \n```time.sleep(1)\ntransactions.sync_features()\n```  \n```materialized_features = transactions.feature_engineering.get_materialized_features()\nmaterialized_features\n```  \n```outputs = do_retry(\ndeployed_model_htc.get_outputs, return_on=(lambda result: len(result) > 0)\n)\n```  \n```outputs[-1]\n```  \n```model_auc_scores = deployed_model_htc.get_evaluation(\"WindowedAUC\")\nprint(len(model_auc_scores))\nplt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])\n```  \nLast updated on January 24, 2025  \n[Local Model](https://docs.turboml.com/general_examples/local_model/ \"Local Model\") [ONNX - Pytorch](https://docs.turboml.com/byo_models/onnx_pytorch/ \"ONNX - Pytorch\")\n</page_content>"
    },
    {
        "section": "LeveragingBaggingClassifier",
        "content": "# LeveragingBaggingClassifier\n@ TurboML - page_link: https://docs.turboml.com/ensembles/leveragingbaggingclassifier/\n<page_content>\nEnsembles  \nLeveraging Bagging Classifier  \nLeveraging Bagging is an improvement over the `Oza Bagging algorithm`. The bagging performance is leveraged by increasing the re-sampling. It uses a poisson distribution to simulate the re-sampling process. To increase re-sampling it uses a higher w value of the Poisson distribution (agerage number of events), 6 by default, increasing the input space diversity, by attributing a different range of weights to the data samples.  \nTo deal with concept drift, Leveraging Bagging uses the `ADWIN` algorithm to monitor the performance of each member of the enemble If concept drift is detected, the worst member of the ensemble (based on the error estimation by ADWIN) is replaced by a new (empty) classifier.  \n## Parameters [Permalink for this section](https://docs.turboml.com/ensembles/leveragingbaggingclassifier/\\#parameters)  \n- **model**( `Model`) → The classifier to bag.  \n- **n\\_models**( `int`, Default: `10`) → The number of models in the ensemble.  \n- **w**( `float`, Default: `6`) → Indicates the average number of events. This is the lambda parameter of the Poisson distribution used to compute the re-sampling weight.  \n- **bagging\\_method**( `str`, Default: `bag`) → The bagging method to use. Can be one of the following:\n- `bag` \\- Leveraging Bagging using ADWIN.\n- `me` \\- Assigns if sample is misclassified, otherwise.\n- `half` \\- Use resampling without replacement for half of the instances.\n- `wt` \\- Resample without taking out all instances.\n- `subag` \\- Resampling without replacement.\n- **seed**( `int` \\| `None`, Default: `None`) → Random number generator seed for reproducibility.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/ensembles/leveragingbaggingclassifier/\\#example-usage)  \nWe can create an instance and deploy LBC model like this.  \n```import turboml as tb\nhtc_model = tb.HoeffdingTreeClassifier(n_classes=2)\nlbc_model = tb.LeveragingBaggingClassifier(n_classes=2, base_model = htc_model)\n```  \nLast updated on January 24, 2025  \n[Contextual Bandit Model Selection](https://docs.turboml.com/ensembles/contextualbanditmodelselection/ \"Contextual Bandit Model Selection\") [Heterogeneous Leveraging Bagging Classifier](https://docs.turboml.com/ensembles/heteroleveragingbaggingclassifier/ \"Heterogeneous Leveraging Bagging Classifier\")\n</page_content>"
    },
    {
        "section": "TF-IDF embedding example using gRPC Client",
        "content": "# TF-IDF embedding example using gRPC Client\n@ TurboML - page_link: https://docs.turboml.com/byo_models/tfidf_example/\n<page_content>\nBring Your Own Models  \nTF-IDF Example  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/tfidf_example.ipynb)  \nThis example demonstrates using our gRPC API client to generate TF-IDF embedding.  \n```import turboml as tb\n```  \n```!pip install nltk grpcio\n```  \n### Start gRPC server for tfdif embedding from jupyter-notebook [Permalink for this section](https://docs.turboml.com/byo_models/tfidf_example/\\#start-grpc-server-for-tfdif-embedding-from-jupyter-notebook)  \n```import pandas as pd\nfrom utils.tfidf_grpc_server import serve\nimport threading\n\n\ndef run_server_in_background(url):\nserve(url)  # This will start the gRPC server\n\n\n# Start the server in a separate thread\nurl = \"0.0.0.0:50047\"\nserver_thread = threading.Thread(\ntarget=run_server_in_background, args=(url,), daemon=True\n)\nserver_thread.start()\n\nprint(\"gRPC server is running in the background...\")\n```  \n### Load text dataset [Permalink for this section](https://docs.turboml.com/byo_models/tfidf_example/\\#load-text-dataset)  \n```import re\nimport urllib.request\n\nwith urllib.request.urlopen(\n\"https://raw.githubusercontent.com/TurboML-Inc/colab-notebooks/refs/heads/main/data/tfidf_test_data.txt\"\n) as file:\ntext = file.read().decode()\n\nsentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", text)\n\nsentences = [sentence.strip() for sentence in sentences if sentence.strip()]\nlabels = [0] * len(sentences)\n```  \n```text_dict_test = {\"text\": sentences}\nlabel_dict_test = {\"labels\": labels}\ntext_df_test = pd.DataFrame(text_dict_test)\nlabel_df_test = pd.DataFrame(label_dict_test)\ntext_df_test.reset_index(inplace=True)\nlabel_df_test.reset_index(inplace=True)\n```  \n```text_df_test = text_df_test.reset_index(drop=True)\nlabel_df_test = label_df_test.reset_index(drop=True)\n```  \n```text_train = tb.LocalDataset.from_pd(df=text_df_test, key_field=\"index\")\nlabels_train = tb.LocalDataset.from_pd(df=label_df_test, key_field=\"index\")\n\ntext_test = tb.LocalDataset.from_pd(df=text_df_test, key_field=\"index\")\nlabels_test = tb.LocalDataset.from_pd(df=label_df_test, key_field=\"index\")\n```  \n```textual_fields = [\"text\"]\nfeatures_train = text_train.get_model_inputs(textual_fields=textual_fields)\ntargets_train = labels_train.get_model_labels(label_field=\"labels\")\n\nfeatures_test = text_test.get_model_inputs(textual_fields=textual_fields)\ntargets_test = labels_test.get_model_labels(label_field=\"labels\")\n```  \n### Using TurboML Client to request gRPC server [Permalink for this section](https://docs.turboml.com/byo_models/tfidf_example/\\#using-turboml-client-to-request-grpc-server)  \n```grpc_model = tb.GRPCClient(\nserver_url=\"0.0.0.0:50047\",\nconnection_timeout=10000,\nmax_request_time=10000,\nmax_retries=1,\n)\n```  \n```model_trained = grpc_model.learn(features_train, targets_train)\n```  \n```outputs_test = model_trained.predict(features_test)\n```  \n```outputs_test\n```  \nLast updated on January 24, 2025  \n[ONNX - Tensorflow](https://docs.turboml.com/byo_models/onnx_tensorflow/ \"ONNX - Tensorflow\") [ResNet Example](https://docs.turboml.com/byo_models/resnet_example/ \"ResNet Example\")\n</page_content>"
    },
    {
        "section": "EmbeddingModel",
        "content": "# EmbeddingModel\n@ TurboML - page_link: https://docs.turboml.com/pipeline_components/embeddingmodel/\n<page_content>\nPipeline Components  \nEmbedding Model  \nAn embedding is a numerical representation of a piece of information, for example, text, documents, images, audio, etc. The representation captures the semantic meaning of what is being embedded. This is a meta-model which takes in a `embedding_model` and a `base_model`. The embeddings are computed by the embedding\\_model, and passed as numerical input to the base\\_model for training/prediction.  \n## Parameters [Permalink for this section](https://docs.turboml.com/pipeline_components/embeddingmodel/\\#parameters)  \n- **embedding\\_model**( `embedding_model`) → The embedding model to used.\n- **base\\_model**( `base_model`) → The base classifier or regressor model.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/pipeline_components/embeddingmodel/\\#example-usage)  \nWe can use and deploy the EmbeddingModel as such.  \n```import turboml as tb\nembedding = tb.RandomProjectionEmbedding(n_embeddings = 4)\nhtc_model = tb.HoeffdingTreeClassifier(n_classes=2)\nembedding_model = tb.EmbeddingModel(embedding_model = embedding, base_model = htc_model)\n```  \nLast updated on January 24, 2025  \n[PreProcessors](https://docs.turboml.com/pipeline_components/preprocessors/ \"PreProcessors\") [Random Projection Embedding](https://docs.turboml.com/pipeline_components/randomprojectionembedding/ \"Random Projection Embedding\")\n</page_content>"
    },
    {
        "section": "BanditModelSelection",
        "content": "# BanditModelSelection\n@ TurboML - page_link: https://docs.turboml.com/ensembles/banditmodelselection/\n<page_content>\nEnsembles  \nBandit Model Selection  \nBandit-based model selection.  \nEach model is associated with an arm, in a multi-arm bandit scenario. The bandit algorithm is used to decide which models to update on new data. The reward of the bandit is the performance of the model on that sample. Fo prediction, we always use the current best model based on the bandit.  \n## Parameters [Permalink for this section](https://docs.turboml.com/ensembles/banditmodelselection/\\#parameters)  \n- **bandit**(Default: `EpsGreedy`) → The underlying bandit algorithm. Options are: EpsGreedy, UCB, and GaussianTS.  \n- **metric\\_name**(Default: `WindowedMAE`) → The metric to use to evaluate models. Options are: WindowedAUC, WindowedAccuracy, WindowedMAE, WindowedMSE, and WindowedRMSE.  \n- **base\\_models**( `list[Model]`) → The list of models over which to perform model selection.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/ensembles/banditmodelselection/\\#example-usage)  \nWe can create an instance and deploy BanditModel like this.  \n```import turboml as tb\nhtc_model = tb.HoeffdingTreeRegressor()\namf_model = tb.AMFRegressor()\nffm_model = tb.FFMRegressor()\nbandit_model = tb.BanditModelSelection(base_models = [htc_model, amf_model, ffm_model])\n```  \nLast updated on January 24, 2025  \n[AdaBoost Classifier](https://docs.turboml.com/ensembles/adaboostclassifer/ \"AdaBoost Classifier\") [Contextual Bandit Model Selection](https://docs.turboml.com/ensembles/contextualbanditmodelselection/ \"Contextual Bandit Model Selection\")\n</page_content>"
    },
    {
        "section": "HeteroLeveragingBaggingClassifier",
        "content": "# HeteroLeveragingBaggingClassifier\n@ TurboML - page_link: https://docs.turboml.com/ensembles/heteroleveragingbaggingclassifier/\n<page_content>\nEnsembles  \nHeterogeneous Leveraging Bagging Classifier  \nSimilar to LeveragingBaggingClassifier, but instead of multiple copies of the same model, it can work with different base models.  \n## Parameters [Permalink for this section](https://docs.turboml.com/ensembles/heteroleveragingbaggingclassifier/\\#parameters)  \n- **base\\_models**( `list[Model]`) → The list of classifier models.  \n- **n\\_classes**( `int`) → The number of classes for the classifier.  \n- **w**( `float`, Default: `6`) → Indicates the average number of events. This is the lambda parameter of the Poisson distribution used to compute the re-sampling weight.  \n- **bagging\\_method**( `str`, Default: `bag`) → The bagging method to use. Can be one of the following:\n- `bag` \\- Leveraging Bagging using ADWIN.\n- `me` \\- Assigns if sample is misclassified, otherwise.\n- `half` \\- Use resampling without replacement for half of the instances.\n- `wt` \\- Resample without taking out all instances.\n- `subag` \\- Resampling without replacement.\n- **seed**( `int` \\| `None`, Default: `None`) → Random number generator seed for reproducibility.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/ensembles/heteroleveragingbaggingclassifier/\\#example-usage)  \nWe can create an instance and deploy LBC model like this.  \n```import turboml as tb\nmodel = tb.HeteroLeveragingBaggingClassifier(n_classes=2, base_models = [tb.HoeffdingTreeClassifier(n_classes=2), tb.AMFClassifier(n_classes=2)])\n```  \nLast updated on January 24, 2025  \n[Leveraging Bagging Classifier](https://docs.turboml.com/ensembles/leveragingbaggingclassifier/ \"Leveraging Bagging Classifier\") [Heterogeneous AdaBoost Classifier](https://docs.turboml.com/ensembles/heteroadaboostclassifer/ \"Heterogeneous AdaBoost Classifier\")\n</page_content>"
    },
    {
        "section": "FFM Regressor",
        "content": "# FFM Regressor\n@ TurboML - page_link: https://docs.turboml.com/regression/ffmregressor/\n<page_content>\nRegression  \nFFM Regressor  \n**Field-aware Factorization Machine** [1](https://docs.turboml.com/regression/ffmregressor/#user-content-fn-1) for regression.  \nThe model equation is defined by:\nWhere is the latent vector corresponding to feature for field, and is the latent vector corresponding to feature for field.\n`$$ \\sum_{f1=1}^{F} \\sum_{f2=f1+1}^{F} \\mathbf{w_{i1}} \\cdot \\mathbf{w_{i2}}, \\text{where } i1 = \\Phi(v_{f1}, f1, f2), \\quad i2 = \\Phi(v_{f2}, f2, f1) $$`\nOur implementation automatically applies MinMax scaling to the inputs, use normal distribution for latent initialization and squared loss for optimization.  \n## Parameters [Permalink for this section](https://docs.turboml.com/regression/ffmregressor/\\#parameters)  \n- **n\\_factors**( `int`, Default: `10`) → Dimensionality of the factorization or number of latent factors.  \n- **l1\\_weight**( `int`, Default: `0.0`) → Amount of L1 regularization used to push weights towards 0.  \n- **l2\\_weight**( `int`, Default: `0.0`) → Amount of L2 regularization used to push weights towards 0.  \n- **l1\\_latent**( `int`, Default: `0.0`) → Amount of L1 regularization used to push latent weights towards 0.  \n- **l2\\_latent**( `int`, Default: `0.0`) → Amount of L2 regularization used to push latent weights towards 0.  \n- **intercept**( `int`, Default: `0.0`) → Initial intercept value.  \n- **intercept\\_lr**( `float`, Default: `0.01`) → Learning rate scheduler used for updating the intercept. No intercept will be used if this is set to 0.  \n- **clip\\_gradient**(Default: `1000000000000.0`) → Clips the absolute value of each gradient value.  \n- **seed**( `int` \\| `None`, Default: `None`) → Randomization seed used for reproducibility.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/regression/ffmregressor/\\#example-usage)  \nWe can create an instance of the FFM model like this.  \n```import turboml as tb\nffm_model = tb.FFMRegressor()\n```  \n## Footnotes [Permalink for this section](https://docs.turboml.com/regression/ffmregressor/\\#footnote-label)  \n1. Juan, Y., Zhuang, Y., Chin, W.S. and Lin, C.J., 2016, September. Field-aware factorization machines for CTR prediction. In Proceedings of the 10th ACM Conference on Recommender Systems (pp. 43-50). [↩](https://docs.turboml.com/regression/ffmregressor/#user-content-fnref-1)  \nLast updated on January 24, 2025  \n[AMF Regressor](https://docs.turboml.com/regression/amfregressor/ \"AMF Regressor\") [Hoeffding Tree Regressor](https://docs.turboml.com/regression/hoeffdingtreeregressor/ \"Hoeffding Tree Regressor\")\n</page_content>"
    },
    {
        "section": "Hyperparameter Tuning",
        "content": "# Hyperparameter Tuning\n@ TurboML - page_link: https://docs.turboml.com/pre_deployment_ml/hyperparameter_tuning/\n<page_content>\nPre-Deployment ML  \nHyperparameter Tuning  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/hyperparameter_tuning.ipynb)  \nHyperparameter Tuning uses grid search to scan through a given hyperparameter space for a model and find out the best combination of hyperparameters with respect to a given performance metric.  \n```import turboml as tb\n```  \n```from sklearn import metrics\n```  \n## Dataset [Permalink for this section](https://docs.turboml.com/pre_deployment_ml/hyperparameter_tuning/\\#dataset)  \nWe use our standard `FraudDetection` dataset for this example, exposed through the `LocalDataset` interface that can be used for tuning, and also configure the dataset to indicate the column with the primary key.  \nFor this example, we use the first 100k rows.  \n```transactions = tb.datasets.FraudDetectionDatasetFeatures()\nlabels = tb.datasets.FraudDetectionDatasetLabels()\n\ntransactions_100k = transactions[:100000]\nlabels_100k = labels[:100000]\n```  \n```numerical_fields = [\"transactionAmount\", \"localHour\"]\ncategorical_fields = [\"digitalItemCount\", \"physicalItemCount\", \"isProxyIP\"]\ninputs = transactions_100k.get_model_inputs(\nnumerical_fields=numerical_fields, categorical_fields=categorical_fields\n)\nlabel = labels_100k.get_model_labels(label_field=\"is_fraud\")\n```  \n## Training/Tuning [Permalink for this section](https://docs.turboml.com/pre_deployment_ml/hyperparameter_tuning/\\#trainingtuning)  \nWe will be using the `AdaBoost Classifier` with `Hoeffding Tree Classifier` being the base model as an example.  \n```model_to_tune = tb.AdaBoostClassifier(\nn_classes=2, base_model=tb.HoeffdingTreeClassifier(n_classes=2)\n)\n```  \nSince a particular model object can include other base models and PreProcessors as well, the `hyperparameter_tuning` function accepts a list of hyperparameter spaces for all such models as part of the `model` parameter, and tests all possible combinations across the different spaces.  \nIn this example, the first dictionary in the list corresponds to the hyperparameters of `AdaBoostClassifier` while the second dictionary is the hyperparameter space for the `HoeffdingTreeClassifier`.  \nIt is not necessary to include all possible hyperparameters in the space; default values are taken for those not specified  \n```model_score_list = tb.hyperparameter_tuning(\nmetric_to_optimize=\"accuracy\",\nmodel=model_to_tune,\nhyperparameter_space=[\\\n{\"n_models\": [2, 3]},\\\n{\\\n\"delta\": [1e-7, 1e-5, 1e-3],\\\n\"tau\": [0.05, 0.01, 0.1],\\\n\"grace_period\": [200, 100, 500],\\\n\"n_classes\": [2],\\\n\"leaf_pred_method\": [\"mc\"],\\\n\"split_method\": [\"gini\", \"info_gain\", \"hellinger\"],\\\n},\\\n],\ninput=inputs,\nlabels=label,\n)\nbest_model, best_score = model_score_list[0]\nbest_model\n```  \n```features = transactions.get_model_inputs(\nnumerical_fields=numerical_fields, categorical_fields=categorical_fields\n)\n\noutputs = best_model.predict(features)\n```  \n```labels_df = labels.df\nprint(\n\"Accuracy: \",\nmetrics.accuracy_score(labels_df[\"is_fraud\"], outputs[\"predicted_class\"]),\n)\nprint(\"F1: \", metrics.f1_score(labels_df[\"is_fraud\"], outputs[\"predicted_class\"]))\n```  \nLast updated on January 24, 2025  \n[Algorithm Tuning](https://docs.turboml.com/pre_deployment_ml/algorithm_tuning/ \"Algorithm Tuning\") [Performance Improvements](https://docs.turboml.com/pre_deployment_ml/performance_improvements/ \"Performance Improvements\")\n</page_content>"
    },
    {
        "section": "Algorithm Tuning",
        "content": "# Algorithm Tuning\n@ TurboML - page_link: https://docs.turboml.com/pre_deployment_ml/algorithm_tuning/\n<page_content>\nPre-Deployment ML  \nAlgorithm Tuning  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/algorithm_tuning.ipynb)  \nAlgorithm Tuning allows us to test different models on a given dataset, and helps to figure out which particular model gives the highest value of a user-defined performance metric on that particular dataset.  \n```import turboml as tb\n```  \n```import pandas as pd\nfrom sklearn import metrics\n```  \n## Dataset [Permalink for this section](https://docs.turboml.com/pre_deployment_ml/algorithm_tuning/\\#dataset)  \nWe use our standard `FraudDetection` dataset for this example, exposed through the `LocalDataset` interface that can be used for tuning, and also configure the dataset to indicate the column with the primary key.  \nFor this example, we use the first 100k rows.  \n```transactions = tb.datasets.FraudDetectionDatasetFeatures()\nlabels = tb.datasets.FraudDetectionDatasetLabels()\n```  \n```transactions_100k = transactions[:100000]\nlabels_100k = labels[:100000]\n\nnumerical_fields = [\\\n\"transactionAmount\",\\\n]\ncategorical_fields = [\"digitalItemCount\", \"physicalItemCount\", \"isProxyIP\"]\ninputs = transactions_100k.get_model_inputs(\nnumerical_fields=numerical_fields, categorical_fields=categorical_fields\n)\nlabel = labels_100k.get_model_labels(label_field=\"is_fraud\")\n```  \n## Training/Tuning [Permalink for this section](https://docs.turboml.com/pre_deployment_ml/algorithm_tuning/\\#trainingtuning)  \nWe will be comparing the `Neural Network` and `Hoeffding Tree Classifier`, and the metric we will be optimizing is `accuracy`.  \nConfiguring the NN according to the dataset.  \n```new_layer = tb.NNLayer(output_size=2)\n\nnn = tb.NeuralNetwork()\nnn.layers.append(new_layer)\n```  \nThe `algorithm_tuning` function takes in the models being tested as a list along with the metric to test against, and returns an object for the model which had the highest score for the given metric.  \n```model_score_list = tb.algorithm_tuning(\nmodels_to_test=[\\\ntb.HoeffdingTreeClassifier(n_classes=2),\\\nnn,\\\n],\nmetric_to_optimize=\"accuracy\",\ninput=inputs,\nlabels=label,\n)\nbest_model, best_score = model_score_list[0]\nbest_model\n```  \n## Testing  \nAfter finding out the best performing model, we can use it normally for inference on the entire dataset and testing on more performance metrics.  \n```transactions_test = transactions[100000:]\nlabels_test = labels[100000:]\n```  \n```features = transactions_test.get_model_inputs(\nnumerical_fields=numerical_fields, categorical_fields=categorical_fields\n)\n\noutputs = best_model.predict(features)\n```  \n```print(\n\"Accuracy: \",\nmetrics.accuracy_score(labels_test.df[\"is_fraud\"], outputs[\"predicted_class\"]),\n)\nprint(\"F1: \", metrics.f1_score(labels_test.df[\"is_fraud\"], outputs[\"predicted_class\"]))\n```  \nLast updated on January 24, 2025  \n[Ibis Feature Engineering](https://docs.turboml.com/feature_engineering/advanced/ibis_feature_engineering/ \"Ibis Feature Engineering\") [Hyperparameter Tuning](https://docs.turboml.com/pre_deployment_ml/hyperparameter_tuning/ \"Hyperparameter Tuning\")\n</page_content>"
    },
    {
        "section": "Drift Detection",
        "content": "# Drift Detection\n@ TurboML - page_link: https://docs.turboml.com/post_deployment_ml/drift/\n<page_content>\nPost-Deployment ML  \nDrift  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/drift.ipynb)  \nDrift detection is a crucial part of ML observability. As is the case with other components, drift detection in TurboML is a continuous streaming process. In this notebook, we'll see how to compute data drift (univariate and multivariate) and model drift.  \nFor univariate drift detection, by default we're using Adaptive Windowing method, and for multivariate drift detection, by default we're using PCA based reconstruction method.  \n```import turboml as tb\n```  \n```transactions = tb.datasets.FraudDetectionDatasetFeatures().to_online(\n\"transactions\", load_if_exists=True\n)\nlabels = tb.datasets.FraudDetectionDatasetLabels().to_online(\n\"transaction_labels\", load_if_exists=True\n)\n```  \n```model = tb.RCF(number_of_trees=50)\n```  \n```numerical_fields = [\\\n\"transactionAmount\",\\\n\"localHour\",\\\n\"isProxyIP\",\\\n\"physicalItemCount\",\\\n\"digitalItemCount\",\\\n]\nfeatures = transactions.get_model_inputs(numerical_fields=numerical_fields)\nlabel = labels.get_model_labels(label_field=\"is_fraud\")\n```  \n```deployed_model = model.deploy(name=\"drift_demo\", input=features, labels=label)\n```  \nWe can register univariate drift by using `numerical_field` and optionally a `label`. By default, label is same as `numerical_field`.  \n```transactions.register_univariate_drift(numerical_field=\"transactionAmount\")\n```  \n```transactions.register_univariate_drift(\nlabel=\"demo_uv_drift\", numerical_field=\"physicalItemCount\"\n)\n```  \nFor multivariate drift, providing `label` is required.  \n```transactions.register_multivariate_drift(\nlabel=\"demo_mv_drift\", numerical_fields=numerical_fields\n)\n```  \n```deployed_model.add_drift()\n```  \n```import matplotlib.pyplot as plt\n\n\ndef plot_drift(drifts):\nplt.plot([drift[\"record\"].score for drift in drifts])\n```  \nWe can use either `label` or `numerical_field(s)` to fetch drift results.  \n```plot_drift(transactions.get_univariate_drift(numerical_field=\"transactionAmount\"))\n```  \n```plot_drift(transactions.get_univariate_drift(label=\"demo_uv_drift\"))\n```  \n```plot_drift(transactions.get_multivariate_drift(label=\"demo_mv_drift\"))\n```  \n```plot_drift(deployed_model.get_drifts())\n```  \nLast updated on January 24, 2025  \n[Performance Improvements](https://docs.turboml.com/pre_deployment_ml/performance_improvements/ \"Performance Improvements\") [Model Explanations](https://docs.turboml.com/post_deployment_ml/model_explanations/ \"Model Explanations\")\n</page_content>"
    },
    {
        "section": "MSTREAM",
        "content": "# MSTREAM\n@ TurboML - page_link: https://docs.turboml.com/anomaly_detection/mstream/\n<page_content>\nAnomaly Detection  \nMStream  \nMSTREAM [1](https://docs.turboml.com/anomaly_detection/mstream/#user-content-fn-1) can detect unusual group anomalies as they occur,\nin a dynamic manner. MSTREAM has the following properties:  \n- (a) it detects anomalies in multi-aspect data including both categorical and\nnumeric attributes;\n- (b) it is online, thus processing each record in\nconstant time and constant memory;\n- (c) it can capture the correlation\nbetween multiple aspects of the data.  \n![mstream](https://docs.turboml.com/_next/static/media/mstream.b86655c2.png)  \n## Parameters [Permalink for this section](https://docs.turboml.com/anomaly_detection/mstream/\\#parameters)  \n- **num\\_rows**( `int`, Default: `2`) → Number of Hash Functions.  \n- **num\\_buckets**( `int`, Default: `factor`) → Number of Buckets for hashing.  \n- **factor**( `float`, Default: `0.8`) → Temporal Decay Factor.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/anomaly_detection/mstream/\\#example-usage)  \nWe can create an instance and deploy LBC model like this.  \n```import turboml as tb\nmodel = tb.MStream()\n```  \n## Footnotes [Permalink for this section](https://docs.turboml.com/anomaly_detection/mstream/\\#footnote-label)  \n1. Bhatia, S., Jain, A., Li, P., Kumar, R., & Hooi, B. (2021, April). Mstream: Fast anomaly detection in multi-aspect streams. In Proceedings of the Web Conference 2021 (pp. 3371-3382). [↩](https://docs.turboml.com/anomaly_detection/mstream/#user-content-fnref-1)  \nLast updated on January 24, 2025  \n[Half Space Trees](https://docs.turboml.com/anomaly_detection/hst/ \"Half Space Trees\") [Random Cut Forest](https://docs.turboml.com/anomaly_detection/rcf/ \"Random Cut Forest\")\n</page_content>"
    },
    {
        "section": "RandomProjectionEmbedding",
        "content": "# RandomProjectionEmbedding\n@ TurboML - page_link: https://docs.turboml.com/pipeline_components/randomprojectionembedding/\n<page_content>\nPipeline Components  \nRandom Projection Embedding  \nThis model supports two methods of generating embeddings.  \n## Sparse Random Projection [Permalink for this section](https://docs.turboml.com/pipeline_components/randomprojectionembedding/\\#sparse-random-projection)  \nReduces the dimensionality of inputs by projecting them onto a sparse random projection matrix using a density (ratio of non-zero components in the matrix) of 0.1.  \n## Gaussian Random Projection [Permalink for this section](https://docs.turboml.com/pipeline_components/randomprojectionembedding/\\#gaussian-random-projection)  \nReduces the dimensionality of inputs through Gaussian random projection where the components of the random projections matrix are drawn from `N(0, 1/n_embeddings)`.  \n## Parameters [Permalink for this section](https://docs.turboml.com/pipeline_components/randomprojectionembedding/\\#parameters)  \n- **n\\_embeddings**(Default: `2`) → Number of components to project the data onto.  \n- **type\\_embedding**(Default: `Gaussian`) → Method to use for random projection. Options are \"Gaussian\" and \"Sparse\".  \n## Example Usage [Permalink for this section](https://docs.turboml.com/pipeline_components/randomprojectionembedding/\\#example-usage)  \nWe can create an instance of the RandomProjectionEmbedding model like this.  \n```import turboml as tb\nembedding = tb.RandomProjectionEmbedding(n_embeddings = 4)\n```  \nLast updated on January 24, 2025  \n[Embedding Model](https://docs.turboml.com/pipeline_components/embeddingmodel/ \"Embedding Model\") [Random Sampler](https://docs.turboml.com/pipeline_components/randomsampler/ \"Random Sampler\")\n</page_content>"
    },
    {
        "section": "Gaussian Naive Bayes",
        "content": "# Gaussian Naive Bayes\n@ TurboML - page_link: https://docs.turboml.com/classification/gaussiannb/\n<page_content>\nClassification  \nGaussian Naive Bayes  \n**Gaussian Naive Bayes**, A Gaussian `$$G_{cf}$$` distribution is maintained for each class `c` and each feature `f` . Each Gaussian is updated using the amount associated with each feature. The joint log-likelihood is then obtained by summing the log probabilities of each feature associated with each class.  \n## Parameters [Permalink for this section](https://docs.turboml.com/classification/gaussiannb/\\#parameters)  \n- **n\\_classes**( `int`) → The number of classes for classification.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/classification/gaussiannb/\\#example-usage)  \nWe can create an instance of GaussianNB model in this format.  \n```import turboml as tb\ngauNB = tb.GaussianNB(n_classes=2)\n```  \n[FFM Classifier](https://docs.turboml.com/classification/ffmclassifier/ \"FFM Classifier\") [Multinomial Naive Bayes](https://docs.turboml.com/classification/multinomialnb/ \"Multinomial Naive Bayes\")\n</page_content>"
    },
    {
        "section": "Feature Engineering - Python UDAF",
        "content": "# Feature Engineering - Python UDAF\n@ TurboML - page_link: https://docs.turboml.com/feature_engineering/udaf/\n<page_content>\nFeature Engineering  \nUDAF  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/UDAF.ipynb)  \n```import turboml as tb\n```  \n```transactions = tb.datasets.FraudDetectionDatasetFeatures()[:100].to_online(\nid=\"udaf_transactions\", load_if_exists=True\n)\n```  \n### User Defined Aggregation function [Permalink for this section](https://docs.turboml.com/feature_engineering/udaf/\\#user-defined-aggregation-function)  \nTo create a UDAF, you need to implement the following essential functions in a separate python file containing the function. These functions manage the lifecycle of the aggregation process, from initialization to final result computation:  \n#### State Initialization (create\\_state): [Permalink for this section](https://docs.turboml.com/feature_engineering/udaf/\\#state-initialization-create_state)  \nPurpose: This function sets up the initial state for the UDAF.\nRequirement: The state should represent the data structure that will store intermediate results (e.g., sum, count, or any other aggregated values).  \n#### Accumulation (accumulate): [Permalink for this section](https://docs.turboml.com/feature_engineering/udaf/\\#accumulation-accumulate)  \nPurpose: This function updates the state with new values as they are processed.\nRequirement: It should handle null or missing values gracefully and update the intermediate state based on the value and any additional parameters.  \n#### Retraction (retract): [Permalink for this section](https://docs.turboml.com/feature_engineering/udaf/\\#retraction-retract)  \nPurpose: This function \"retracts\" or removes a previously accumulated value from the state.\nRequirement: It should reverse the effect of the accumulate function for cases where data needs to be removed (e.g., when undoing a previous operation).  \n#### Merging States (merge\\_states): [Permalink for this section](https://docs.turboml.com/feature_engineering/udaf/\\#merging-states-merge_states)  \nPurpose: This function merges two states together.\nRequirement: Combine the intermediate results from two states into one. This is essential for distributed aggregations.  \n#### Final Result Computation (finish): [Permalink for this section](https://docs.turboml.com/feature_engineering/udaf/\\#final-result-computation-finish)  \nPurpose: This function computes the final result once all values have been accumulated.\nRequirement: It should return the final output of the aggregation based on the state. Handle edge cases such as empty datasets (e.g., return None if no valid values were processed).  \n```function_file_contents = \"\"\"\ndef create_state():\nreturn 0, 0\n\n\ndef accumulate(state, value, weight):\nif value is None or weight is None:\nreturn state\n(s, w) = state\ns += value * weight\nw += weight\nreturn s, w\n\n\ndef retract(state, value, weight):\nif value is None or weight is None:\nreturn state\n(s, w) = state\ns -= value * weight\nw -= weight\nreturn s, w\n\n\ndef merge_states(state_a, state_b):\n(s_a, w_a) = state_a\n(s_b, w_b) = state_b\nreturn s_a + s_b, w_a + w_b\n\n\ndef finish(state):\n(sum, weight) = state\nif weight == 0:\nreturn None\nelse:\nreturn sum / weight\n\"\"\"\n```  \n```transactions.feature_engineering.register_timestamp(\ncolumn_name=\"timestamp\", format_type=\"epoch_seconds\"\n)\n```  \n```transactions.feature_engineering.create_udaf_features(\nnew_feature_name=\"weighted_avg\",\ncolumn_to_operate=[\"transactionAmount\", \"transactionTime\"],\nfunction_name=\"weighted_avg\",\nreturn_type=\"DOUBLE\",\nfunction_file_contents=function_file_contents,\ncolumn_to_group=[\"accountID\"],\ntimestamp_column=\"timestamp\",\nwindow_duration=1,\nwindow_unit=\"hours\",\n)\n```  \n```transactions.feature_engineering.materialize_features([\"weighted_avg\"])\n```  \n```transactions.feature_engineering.get_materialized_features()\n```  \nLast updated on January 24, 2025  \n[UDF](https://docs.turboml.com/feature_engineering/udf/ \"UDF\") [Ibis Quickstart](https://docs.turboml.com/feature_engineering/advanced/ibis_quickstart/ \"Ibis Quickstart\")\n</page_content>"
    },
    {
        "section": "TurboML LLM Tutorial",
        "content": "# TurboML LLM Tutorial\n@ TurboML - page_link: https://docs.turboml.com/llms/turboml_llm_tutorial/\n<page_content>\nLLMs  \nLLM Tutorial  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/turboml_llm_tutorial.ipynb)  \nTurboML can spin up LLM servers with an OpenAI-compatible API. We currently support models\nin the GGUF format, but also support non-GGUF models that can be converted to GGUF. In the latter\ncase you get to decide the quantization type you want to use.  \n```import turboml as tb\n```  \n```LlamaServerRequest = tb.llm.LlamaServerRequest\nHuggingFaceSpec = LlamaServerRequest.HuggingFaceSpec\nServerParams = LlamaServerRequest.ServerParams\n```  \n## Choose a model [Permalink for this section](https://docs.turboml.com/llms/turboml_llm_tutorial/\\#choose-a-model)  \nLet's use a Llama 3.2 quant already in the GGUF format.  \n```hf_spec = HuggingFaceSpec(\nhf_repo_id=\"hugging-quants/Llama-3.2-1B-Instruct-Q4_K_M-GGUF\",\nselect_gguf_file=\"llama-3.2-1b-instruct-q4_k_m.gguf\",\n)\n```  \n## Spawn a server [Permalink for this section](https://docs.turboml.com/llms/turboml_llm_tutorial/\\#spawn-a-server)  \nOn spawning a server, you get a `server_id` to reference it later as well as `server_relative_url` you can\nuse to reach it. This method is synchronous, so it can take a while to yield as we retrieve (and convert) your model.  \n```response = tb.llm.spawn_llm_server(\nLlamaServerRequest(\nsource_type=LlamaServerRequest.SourceType.HUGGINGFACE,\nhf_spec=hf_spec,\nserver_params=ServerParams(\nthreads=-1,\nseed=-1,\ncontext_size=0,\nflash_attention=False,\n),\n)\n)\nresponse\n```  \n```server_id = response.server_id\n```  \n### Interacting with the LLM [Permalink for this section](https://docs.turboml.com/llms/turboml_llm_tutorial/\\#interacting-with-the-llm)  \nOur LLM is exposed with an OpenAI-compatible API, so we can use the OpenAI SDK, or any\nother tool compatible tool to use it.  \n```%pip install openai\n```  \n```from openai import OpenAI\n\nbase_url = tb.common.env.CONFIG.TURBOML_BACKEND_SERVER_ADDRESS\nserver_url = f\"{base_url}/{response.server_relative_url}\"\n\nclient = OpenAI(base_url=server_url, api_key=\"-\")\n\n\nresponse = client.chat.completions.create(\nmessages=[\\\n{\\\n\"role\": \"user\",\\\n\"content\": \"Hello there how are you doing today?\",\\\n}\\\n],\nmodel=\"-\",\n)\n\nprint(response)\n```  \n```embeddings = (\nclient.embeddings.create(input=[\"Hello there how are you doing today?\"], model=\"-\")\n.data[0]\n.embedding\n)\nlen(embeddings), embeddings[:5]\n```  \n## Stop the server [Permalink for this section](https://docs.turboml.com/llms/turboml_llm_tutorial/\\#stop-the-server)  \n```tb.llm.stop_llm_server(server_id)\n```  \nLast updated on January 24, 2025  \n[Image Embeddings](https://docs.turboml.com/llms/image_embeddings/ \"Image Embeddings\") [String Encoding](https://docs.turboml.com/non_numeric_inputs/string_encoding/ \"String Encoding\")\n</page_content>"
    },
    {
        "section": "AMF Classifier",
        "content": "# AMF Classifier\n@ TurboML - page_link: https://docs.turboml.com/classification/amfclassifier/\n<page_content>\nClassification  \nAMF Classifier  \n**Aggregated Mondrian Forest** classifier for online learning.\nThis implementation is truly online, in the sense that a single pass is performed, and that predictions can be produced anytime.  \nEach node in a _tree_ predicts according to the distribution of the labels it contains. This distribution is regularized using a **Jeffreys** prior with parameter `dirichlet`. For each class with count labels in the node and n\\_samples samples in it, the prediction of a node is given by  \nThe prediction for a sample is computed as the aggregated predictions of all the subtrees along the path leading to the leaf node containing the sample. The aggregation weights are exponential weights with learning rate step and log-loss when use\\_aggregation is True.  \nThis computation is performed exactly thanks to a context tree weighting algorithm. More details can be found in the paper cited in the reference[1](https://docs.turboml.com/classification/amfclassifier/#user-content-fn-1) below.  \nThe final predictions are the average class probabilities predicted by each of the n\\_estimators trees in the forest.  \n## Parameters [Permalink for this section](https://docs.turboml.com/classification/amfclassifier/\\#parameters)  \n- **n\\_classes**( `int`) → The number of classes for classification.  \n- **n\\_estimators**( `int`, Default: `10`) → The number of trees in the forest.  \n- **step** ( `float`, Default: `1.0`) → Step-size for the aggregation weights. Default is 1 for classification with the log-loss, which is usually the best choice.  \n- **use\\_aggregation**( `bool`, Default: `True`) → Controls if aggregation is used in the trees. It is highly recommended to leave it as True.  \n- **dirichlet** ( `float`, Default: `0.5`) → Regularization level of the class frequencies used for predictions in each node. A rule of thumb is to set this to 1 / n\\_classes, where n\\_classes is the expected number of classes which might appear. Default is dirichlet = 0.5, which works well for binary classification problems.  \n- **split\\_pure**( `bool`, Default: `False`) → Controls if nodes that contains only sample of the same class should be split (\"pure\" nodes). Default is False, namely pure nodes are not split, but True can be sometimes better.  \n- **seed**( `int` \\| `None`, Default: `None`) → Random seed for reproducibility.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/classification/amfclassifier/\\#example-usage)  \nWe can simply use the below syntax to invoke the list of algorithms preconfigured in TurboML, here `have_labels=True` means supervised models.  \n```import turboml as tb\namf_model = tb.AMFClassifier(n_classes=2)\n```  \nℹ  \nOnly log\\_loss used for the computation of the aggregation weights is supported for now, namely the log-loss for multi-class classification.  \n## Footnotes [Permalink for this section](https://docs.turboml.com/classification/amfclassifier/\\#footnote-label)  \n1. Mourtada, J., Gaïffas, S., & Scornet, E. (2021). AMF: Aggregated Mondrian forests for online learning. Journal of the Royal Statistical Society Series B: Statistical Methodology, 83(3), 505-533. [↩](https://docs.turboml.com/classification/amfclassifier/#user-content-fnref-1)  \nLast updated on January 24, 2025  \n[SGT Regressor](https://docs.turboml.com/regression/sgtregressor/ \"SGT Regressor\") [FFM Classifier](https://docs.turboml.com/classification/ffmclassifier/ \"FFM Classifier\")\n</page_content>"
    },
    {
        "section": "Image Processing (MNIST Example)",
        "content": "# Image Processing (MNIST Example)\n@ TurboML - page_link: https://docs.turboml.com/non_numeric_inputs/image_input/\n<page_content>\nNon-Numeric Inputs  \nImage Input  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/image_input.ipynb)  \n```import turboml as tb\n```  \n```import pandas as pd\nfrom torchvision import datasets, transforms\nimport io\nfrom PIL import Image\n```  \n```class PILToBytes:\ndef __init__(self, format=\"JPEG\"):\nself.format = format\n\ndef __call__(self, img):\nif not isinstance(img, Image.Image):\nraise TypeError(f\"Input should be a PIL Image, but got {type(img)}.\")\nbuffer = io.BytesIO()\nimg.save(buffer, format=self.format)\nreturn buffer.getvalue()\n\n\ntransform = transforms.Compose(\n[\\\ntransforms.Resize((28, 28)),\\\nPILToBytes(format=\"PNG\"),\\\n]\n)\n```  \n## Data Inspection [Permalink for this section](https://docs.turboml.com/non_numeric_inputs/image_input/\\#data-inspection)  \nDownloading the MNIST dataset to be used in ML modelling.  \n```mnist_dataset_train = datasets.MNIST(\nroot=\"./data\", train=True, download=True, transform=transform\n)\nmnist_dataset_test = datasets.MNIST(\nroot=\"./data\", train=False, download=True, transform=transform\n)\n```  \n```images_train = []\nimages_test = []\nlabels_train = []\nlabels_test = []\n\nfor image, label in mnist_dataset_train:\nimages_train.append(image)\nlabels_train.append(label)\n\nfor image, label in mnist_dataset_test:\nimages_test.append(image)\nlabels_test.append(label)\n```  \nTransforming the lists into Pandas DataFrames.  \n```image_dict_train = {\"images\": images_train}\nlabel_dict_train = {\"labels\": labels_train}\nimage_df_train = pd.DataFrame(image_dict_train)\nlabel_df_train = pd.DataFrame(label_dict_train)\n\nimage_dict_test = {\"images\": images_test}\nlabel_dict_test = {\"labels\": labels_test}\nimage_df_test = pd.DataFrame(image_dict_test)\nlabel_df_test = pd.DataFrame(label_dict_test)\n```  \nAdding index columns to the DataFrames to act as primary keys for the datasets.  \n```image_df_train.reset_index(inplace=True)\nlabel_df_train.reset_index(inplace=True)\n\nimage_df_test.reset_index(inplace=True)\nlabel_df_test.reset_index(inplace=True)\n```  \n```image_df_train.head()\n```  \n```label_df_train.head()\n```  \nUsing `LocalDataset` class for compatibility with the TurboML platform.  \n```images_train = tb.LocalDataset.from_pd(df=image_df_train, key_field=\"index\")\nlabels_train = tb.LocalDataset.from_pd(df=label_df_train, key_field=\"index\")\n\nimages_test = tb.LocalDataset.from_pd(df=image_df_test, key_field=\"index\")\nlabels_test = tb.LocalDataset.from_pd(df=label_df_test, key_field=\"index\")\n```  \nExtracting the features and the targets from the TurboML-compatible datasets.  \n```imaginal_fields = [\"images\"]\n\nfeatures_train = images_train.get_model_inputs(imaginal_fields=imaginal_fields)\ntargets_train = labels_train.get_model_labels(label_field=\"labels\")\n\nfeatures_test = images_test.get_model_inputs(imaginal_fields=imaginal_fields)\ntargets_test = labels_test.get_model_labels(label_field=\"labels\")\n```  \n## Model Initialization [Permalink for this section](https://docs.turboml.com/non_numeric_inputs/image_input/\\#model-initialization)  \nDefining a Neural Network (NN) to be used on the MNIST data.  \nThe `output_size` of the final layer in the NN is `10` in the case of MNIST.  \nSince this is a classification task, `Cross Entropy` loss is used with the `Adam` optimizer.  \n```final_layer = tb.NNLayer(output_size=10, activation=\"none\")\n\nmodel = tb.NeuralNetwork(\nloss_function=\"cross_entropy\", optimizer=\"adam\", learning_rate=0.01\n)\nmodel.layers[-1] = final_layer\n```  \n## ImageToNumeric PreProcessor [Permalink for this section](https://docs.turboml.com/non_numeric_inputs/image_input/\\#imagetonumeric-preprocessor)  \nSince we are dealing with images as input to the model, we select the `ImageToNumeric PreProcessor` to accordingly convert the binary images into numerical data useful to the NN.  \n```model = tb.ImageToNumericPreProcessor(base_model=model, image_sizes=[28, 28, 1])\n```  \n## Model Training [Permalink for this section](https://docs.turboml.com/non_numeric_inputs/image_input/\\#model-training)  \nSetting the model combined with the `ImageToNumeric PreProcessor` to learn on the training data.  \n```model = model.learn(features_train, targets_train)\n```  \n## Model Inference [Permalink for this section](https://docs.turboml.com/non_numeric_inputs/image_input/\\#model-inference)  \nPerforming inference on the trained model using the test data.  \n```outputs_test = model.predict(features_test)\n```  \n```outputs_test\n```  \n## Model Testing [Permalink for this section](https://docs.turboml.com/non_numeric_inputs/image_input/\\#model-testing)  \nTesting the trained model's performance on the test data.  \n```from sklearn import metrics\n```  \n```labels_test_list = labels_test.input_df[\"labels\"].to_list()\n```  \n```print(\n\"Accuracy: \",\nmetrics.accuracy_score(labels_test_list, outputs_test[\"predicted_class\"]),\n)\nprint(\n\"F1: \",\nmetrics.f1_score(\nlabels_test_list, outputs_test[\"predicted_class\"], average=\"macro\"\n),\n)\nprint(\n\"Precision: \",\nmetrics.precision_score(\nlabels_test_list, outputs_test[\"predicted_class\"], average=\"macro\"\n),\n)\n```  \nLast updated on January 24, 2025  \n[String Encoding](https://docs.turboml.com/non_numeric_inputs/string_encoding/ \"String Encoding\") [Half Space Trees](https://docs.turboml.com/anomaly_detection/hst/ \"Half Space Trees\")\n</page_content>"
    },
    {
        "section": "Feature Engineering - Python UDFs",
        "content": "# Feature Engineering - Python UDFs\n@ TurboML - page_link: https://docs.turboml.com/feature_engineering/udf/\n<page_content>\nFeature Engineering  \nUDF  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/UDF.ipynb)  \n```import turboml as tb\n```  \n```transactions = tb.datasets.FraudDetectionDatasetFeatures()[:100].to_online(\nid=\"udf_transactions\", load_if_exists=True\n)\n```  \n### Simple User Defined function [Permalink for this section](https://docs.turboml.com/feature_engineering/udf/\\#simple-user-defined-function)  \nFor creating a user defined function first create a separate python file containing the function along with the imports used by it; the function should process the data and return a value. In the below example we have shown a simple example of a function that takes a value and then returns its sine value.  \n```myfunction_contents = \"\"\"\nimport numpy as np\n\n\ndef myfunction(x):\nreturn np.sin(x)\n\"\"\"\n```  \n### User Defined Functions - Multiple Input example [Permalink for this section](https://docs.turboml.com/feature_engineering/udf/\\#user-defined-functions---multiple-input-example)  \nWe saw that the above user defined function is very simple. We can also create a more complicated function with multiple inputs, we can perform string processing etc  \n```my_complex_function_contents = \"\"\"\ndef my_complex_function(x, y):\nif x.lower() == y.lower():\nreturn 1\nelse:\nreturn 0\n\"\"\"\n```  \n### Rich User Defined Functions [Permalink for this section](https://docs.turboml.com/feature_engineering/udf/\\#rich-user-defined-functions)  \n```%pip install psycopg_pool psycopg['binary'] psycopg2-binary\n```  \n```my_rich_function_contents = \"\"\"\nfrom turboml.common.feature_engineering import TurboMLScalarFunction\nfrom psycopg_pool import ConnectionPool\n\n\nclass PostgresLookup(TurboMLScalarFunction):\ndef __init__(self, user, password, host, port, dbname):\nconninfo = (\nf\"user={user} password={password} host={host} port={port} dbname={dbname}\"\n)\nself.connPool = ConnectionPool(conninfo=conninfo)\n\ndef func(self, index: str):\nwith self.connPool.connection() as risingwaveConn:\nwith risingwaveConn.cursor() as cur:\nquery = 'SELECT \"model_length\" FROM r2dt_models WHERE id = %s'\ncur.execute(query, (index,))\nresult = cur.fetchone()\nreturn result[0] if result else 0\n\"\"\"\n```  \nWe can create a rich UDF and materialize it.  \n```transactions.feature_engineering.create_rich_udf_features(\nnew_feature_name=\"lookup_feature\",\nargument_names=[\"index\"],\nfunction_name=\"lookup\",\nclass_file_contents=my_rich_function_contents,\nlibraries=[\"psycopg_pool\", \"psycopg[binary]\", \"psycopg2-binary\"],\nclass_name=\"PostgresLookup\",\ndev_initializer_arguments=[\"reader\", \"NWDMCE5xdipIjRrp\", \"hh-pgsql-public.ebi.ac.uk\", \"5432\", \"pfmegrnargs\"],\nprod_initializer_arguments=[\"reader\", \"NWDMCE5xdipIjRrp\", \"hh-pgsql-public.ebi.ac.uk\", \"5432\", \"pfmegrnargs\"],\n)\n\ntransactions.feature_engineering.materialize_features([\"lookup_feature\"])\n```  \n## Feature Engineering using User Defined Functions (UDF) [Permalink for this section](https://docs.turboml.com/feature_engineering/udf/\\#feature-engineering-using-user-defined-functions-udf)  \nMake sure the libraries that are specified are pip installable and hence named appropriately, for example, if the UDF uses a sklearn function, then the library to be installed should be \"scikit-learn\" (and not \"sklearn\")  \n```transactions.feature_engineering.create_udf_features(\nnew_feature_name=\"sine_of_amount\",\nargument_names=[\"transactionAmount\"],\nfunction_name=\"myfunction\",\nfunction_file_contents=myfunction_contents,\nlibraries=[\"numpy\"],\n)\n```  \n```transactions.feature_engineering.create_udf_features(\nnew_feature_name=\"transaction_location_overlap\",\nargument_names=[\"ipCountryCode\", \"paymentBillingCountryCode\"],\nfunction_name=\"my_complex_function\",\nfunction_file_contents=my_complex_function_contents,\nlibraries=[],\n)\n```  \n```transactions.feature_engineering.get_local_features()\n```  \n```transactions.feature_engineering.materialize_features(\n[\"sine_of_amount\", \"transaction_location_overlap\"]\n)\n```  \n```transactions.feature_engineering.get_materialized_features()\n```  \nLast updated on January 24, 2025  \n[PySAD Example](https://docs.turboml.com/wyo_models/pysad_example/ \"PySAD Example\") [UDAF](https://docs.turboml.com/feature_engineering/udaf/ \"UDAF\")\n</page_content>"
    },
    {
        "section": "Native Python Models",
        "content": "# Native Python Models\n@ TurboML - page_link: https://docs.turboml.com/wyo_models/native_python_model/\n<page_content>\nWrite Your Own Models  \nNative Python Model  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/native_python_model.ipynb)  \nWhile TurboML offers a wide array of algorithms implemented with performant machine-native code, we also\ngive you the flexibility to use your own models in Python when necessary, allowing the use of any public\nlibrary from PyPi. Lets walk through some simple examples for model based on [River (opens in a new tab)](https://riverml.xyz/latest/)\nand [scikit-learn (opens in a new tab)](https://scikit-learn.org/stable/).  \n```import turboml as tb\n```  \n```!pip install river\n```  \n```import pandas as pd\n```  \n## Prepare an Evaluation Dataset [Permalink for this section](https://docs.turboml.com/wyo_models/native_python_model/\\#prepare-an-evaluation-dataset)  \nWe choose a standard Credit Card Fraud dataset that ships with River to evaluate our models on.  \n```features = tb.datasets.CreditCardsDatasetFeatures()\nlabels = tb.datasets.CreditCardsDatasetLabels()\n\nfeatures\n```  \n```features.df.loc[0]\n```  \n### And finally load them as datasets in the TurboML Platform [Permalink for this section](https://docs.turboml.com/wyo_models/native_python_model/\\#and-finally-load-them-as-datasets-in-the-turboml-platform)  \n```features = tb.OnlineDataset.from_local_dataset(\nfeatures, \"cc_features\", load_if_exists=True\n)\nlabels = tb.OnlineDataset.from_local_dataset(labels, \"cc_labels\", load_if_exists=True)\n```  \n### Isolate features [Permalink for this section](https://docs.turboml.com/wyo_models/native_python_model/\\#isolate-features)  \n```numerical_cols = features.preview_df.columns.tolist()\nnumerical_cols.remove(\"index\")\ninput_features = features.get_model_inputs(numerical_fields=numerical_cols)\nlabel = labels.get_model_labels(label_field=\"score\")\n```  \n## Structure of User Defined Models [Permalink for this section](https://docs.turboml.com/wyo_models/native_python_model/\\#structure-of-user-defined-models)  \nA custom Python model must implement 3 instance methods - `learn_one`, `predict_one` and `init_imports`.\nThe interface and usage is described below and explored further in the examples contained in this notebook.  \n```class CustomModel:\ndef init_imports(self):\n\"\"\"\nImport any external symbols/modules used in this class\n\"\"\"\npass\n\ndef learn_one(self, input: types.InputData):\n\"\"\"\nReceives labelled data for the model to learn from\n\"\"\"\npass\n\ndef predict_one(self, input: types.InputData, output: types.OutputData):\n\"\"\"\nReceives input features for a prediction, must pass output to the\noutput object\n\"\"\"\npass\n```  \n## Example - Leveraging [River (opens in a new tab)](https://riverml.xyz/) [Permalink for this section](https://docs.turboml.com/wyo_models/native_python_model/\\#example---leveraging-river)  \nRiver is a popular ML library for online machine learning, river comes with an inbuilt functionality for `learn_one` and `predict_one` out of the box, however it is important to note the differences in input to the User Defined models and the input of river model, which takes a dictionary and label as inputs for a supervised algorithm. In this example we create a custom model using river according to the standards mentioned above and put it in a separate python module.  \n```from river import linear_model\nimport turboml.common.pytypes as types\n\n\nclass MyLogisticRegression:\ndef __init__(self):\nself.model = linear_model.LogisticRegression()\n\ndef init_imports(self):\nfrom river import linear_model\n\ndef learn_one(self, input: types.InputData):\nself.model.learn_one(dict(enumerate(input.numeric)), input.label)\n\ndef predict_one(self, input: types.InputData, output: types.OutputData):\nscore = float(self.model.predict_one(dict(enumerate(input.numeric))))\noutput.set_score(score)\n\n# example: setting embeddings\n# output.resize_embeddings(3)\n# mut = output.embeddings()\n# mut[0] = 1\n# mut[1] = 2\n# mut[2] = 3\n\n# example: appending to feature scores\n# this api is an alternative to resize + set as above,\n# but less efficient\n# output.append_feature_score(0.5)\n```  \nSince python packages can have multiple external dependencies we can make use of `tb.setup_venv(name_of_venv, [List of packages])`. This can create a virtual environment that enables interaction with the platform and the installation of external dependencies with ease.  \n```venv = tb.setup_venv(\"my_river_venv\", [\"river\", \"numpy\"])\nvenv.add_python_class(MyLogisticRegression)\n```  \n```river_model = tb.Python(class_name=MyLogisticRegression.__name__, venv_name=venv.name)\n```  \n```deployed_model_river = river_model.deploy(\n\"river_model\", input=input_features, labels=label\n)\n```  \n```import matplotlib.pyplot as plt\n\ndeployed_model_river.add_metric(\"WindowedRMSE\")\nmodel_auc_scores = deployed_model_river.get_evaluation(\"WindowedRMSE\")\nplt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])\n```  \n## Example - An Online Model with Sci-Kit Learn [Permalink for this section](https://docs.turboml.com/wyo_models/native_python_model/\\#example---an-online-model-with-sci-kit-learn)  \nUsing Scikit learn you can implement online learning something similar to the code example below using `partial_fit()`.  \n```!pip install scikit-learn\n```  \n```from sklearn.linear_model import Perceptron\nimport numpy as np\nimport turboml.common.pytypes as types\n\n\nclass MyPerceptron:\ndef __init__(self):\nself.model = Perceptron()\nself.fitted = False\n\ndef init_imports(self):\nfrom sklearn.linear_model import Perceptron\n\ndef learn_one(self, input: types.InputData):\nif not self.fitted:\nself.model.partial_fit(\nnp.array(input.numeric).reshape(1, -1),\nnp.array(input.label).reshape(-1),\nclasses=[0, 1],\n)\nself.fitted = True\nelse:\nself.model.partial_fit(\nnp.array(input.numeric).reshape(1, -1),\nnp.array(input.label).reshape(-1),\n)\n\ndef predict_one(self, input: types.InputData, output: types.OutputData):\nif self.fitted:\nscore = self.model.predict(np.array(input.numeric).reshape(1, -1))[0]\noutput.set_score(score)\nelse:\noutput.set_score(0.0)\n```  \n```venv = tb.setup_venv(\"my_sklearn_venv\", [\"scikit-learn\"])\nvenv.add_python_class(MyPerceptron)\n```  \n```sklearn_model = tb.Python(class_name=MyPerceptron.__name__, venv_name=venv.name)\n```  \n```deployed_model_sklearn = sklearn_model.deploy(\n\"sklearn_model\", input=input_features, labels=label\n)\n```  \n```import matplotlib.pyplot as plt\n\ndeployed_model_sklearn.add_metric(\"WindowedRMSE\")\nmodel_auc_scores = deployed_model_sklearn.get_evaluation(\"WindowedRMSE\")\nplt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])\n```  \n## Example - Leveraging [Vowpal Wabbit (opens in a new tab)](https://vowpalwabbit.org/) [Permalink for this section](https://docs.turboml.com/wyo_models/native_python_model/\\#example---leveraging-vowpal-wabbit)  \nVowpal Wabbit provides fast, efficient, and flexible online machine learning techniques for reinforcement learning, supervised learning, and more.  \nIn this example we use the new `vowpal-wabbit-next` Python bindings. Note that we need to transform our input to Vowpal's native text format.  \n```!pip install vowpal-wabbit-next\n```  \n```import vowpal_wabbit_next as vw\nimport turboml.common.pytypes as types\n\n\nclass MyVowpalModel:\ndef __init__(self):\nself.vw_workspace = vw.Workspace()\nself.vw_parser = vw.TextFormatParser(self.vw_workspace)\n\ndef init_imports(self):\nimport vowpal_wabbit_next as vw\n\ndef to_vw_format(self, features, label=None):\n\"Convert a feature vector into the Vowpal Wabbit format\"\nlabel_place = f\"{label} \" if label is not None else \"\"\nvw_text = f\"{label_place}| {' '.join([f'{idx}:{feat}' for idx, feat in enumerate(features, start=1)])}\\n\"\nreturn self.vw_parser.parse_line(vw_text)\n\ndef predict_one(self, input: types.InputData, output: types.OutputData):\nvw_format = self.to_vw_format(input.numeric)\noutput.set_score(self.vw_workspace.predict_one(vw_format))\n\ndef learn_one(self, input: types.InputData):\nvw_format = self.to_vw_format(input.numeric, input.label)\nself.vw_workspace.learn_one(vw_format)\n```  \nIn the below cell we make use of the custom virtual environment created before to install new packages in this case vowpalwabbit. We have to ensure that the name of the virtual environment remains the same and we can reuse the virtual environment multiple times.  \n```venv = tb.setup_venv(\"my_vowpal_venv\", [\"vowpal-wabbit-next\"])\nvenv.add_python_class(MyVowpalModel)\n```  \n```vw_model = tb.Python(class_name=MyVowpalModel.__name__, venv_name=venv.name)\n```  \n```deployed_model_vw = vw_model.deploy(\"vw_model\", input=input_features, labels=label)\n```  \n```import matplotlib.pyplot as plt\n\ndeployed_model_vw.add_metric(\"WindowedRMSE\")\nmodel_auc_scores = deployed_model_vw.get_evaluation(\"WindowedRMSE\")\nplt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])\n```  \nLast updated on January 24, 2025  \n[OCR](https://docs.turboml.com/byo_models/ocr_example/ \"OCR\") [Ensemble Python Model](https://docs.turboml.com/wyo_models/ensemble_python_model/ \"Ensemble Python Model\")\n</page_content>"
    },
    {
        "section": "ONNX tutorial with Scikit-Learn",
        "content": "# ONNX tutorial with Scikit-Learn\n@ TurboML - page_link: https://docs.turboml.com/byo_models/onnx_sklearn/\n<page_content>\nBring Your Own Models  \nONNX - Scikit-Learn  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/onnx_sklearn.ipynb)  \n```import turboml as tb\n```  \n```!pip install onnx==1.14.1 scikit-learn skl2onnx\n```  \n## Scikit Learn - Standard Model Training [Permalink for this section](https://docs.turboml.com/byo_models/onnx_sklearn/\\#scikit-learn---standard-model-training)  \nThe following blocks of code define a standard sklearn training code. This is completely independent of TurboML.  \n```import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom skl2onnx import convert_sklearn\nfrom skl2onnx.common.data_types import FloatTensorType\nfrom skl2onnx.helpers.onnx_helper import select_model_inputs_outputs\nimport matplotlib.pyplot as plt\n```  \n```transactions = tb.datasets.FraudDetectionDatasetFeatures()\nlabels = tb.datasets.FraudDetectionDatasetLabels()\n```  \n```joined_df = pd.merge(transactions.df, labels.df, on=\"transactionID\", how=\"right\")\njoined_df\n```  \n```X = joined_df.drop(\"is_fraud\", axis=1)\ny = joined_df[\"is_fraud\"]\n```  \n```numerical_fields = [\\\n\"transactionAmount\",\\\n\"localHour\",\\\n\"isProxyIP\",\\\n\"digitalItemCount\",\\\n\"physicalItemCount\",\\\n]\nX = X[numerical_fields]\n```  \n```X_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.2, random_state=42\n)\n\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\n```  \n```y_pred = clf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n```  \n## Export model to ONNX format [Permalink for this section](https://docs.turboml.com/byo_models/onnx_sklearn/\\#export-model-to-onnx-format)  \nExporting a model to ONNX format depends on the framework. Tutorials for different frameworks can be found at [https://github.com/onnx/tutorials#converting-to-onnx-format (opens in a new tab)](https://github.com/onnx/tutorials#converting-to-onnx-format)  \n```initial_type = [(\"float_input\", FloatTensorType([None, X_train.shape[1]]))]\nonx = convert_sklearn(\nclf, initial_types=initial_type, options={type(clf): {\"zipmap\": False}}\n)\nonx = select_model_inputs_outputs(onx, outputs=[\"probabilities\"])\n```  \n## Create an ONNX model with TurboML [Permalink for this section](https://docs.turboml.com/byo_models/onnx_sklearn/\\#create-an-onnx-model-with-turboml)  \nNow that we've converted the model to ONNX format, we can deploy it with TurboML.  \n```transactions = transactions.to_online(id=\"transactions\", load_if_exists=True)\nlabels = labels.to_online(id=\"transaction_labels\", load_if_exists=True)\n\nfeatures = transactions.get_model_inputs(numerical_fields=numerical_fields)\nlabel = labels.get_model_labels(label_field=\"is_fraud\")\n```  \n```tb.set_onnx_model(\"randomforest\", onx.SerializeToString())\nonnx_model = tb.ONNX(model_save_name=\"randomforest\")\n```  \n```deployed_model = onnx_model.deploy(\"onnx_model\", input=features, labels=label)\n```  \n```deployed_model.add_metric(\"WindowedAUC\")\n```  \n```model_auc_scores = deployed_model.get_evaluation(\"WindowedAUC\")\nplt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])\n```  \n```\n```  \nLast updated on January 24, 2025  \n[ONNX - Pytorch](https://docs.turboml.com/byo_models/onnx_pytorch/ \"ONNX - Pytorch\") [ONNX - Tensorflow](https://docs.turboml.com/byo_models/onnx_tensorflow/ \"ONNX - Tensorflow\")\n</page_content>"
    },
    {
        "section": "Adaptive LightGBM",
        "content": "# Adaptive LightGBM\n@ TurboML - page_link: https://docs.turboml.com/general_purpose/adaptivelgbm/\n<page_content>\nGeneral Purpose  \nAdaptive LightGBM  \nLightGBM implementation to handle concept drift based on Adaptive XGBoost for Evolving Data Streams[1](https://docs.turboml.com/general_purpose/adaptivelgbm/#user-content-fn-1).  \n## Parameters [Permalink for this section](https://docs.turboml.com/general_purpose/adaptivelgbm/\\#parameters)  \n- **n\\_classes**( `int`) → The `num_class` parameter from XGBoost.  \n- **learning\\_rate**(Default: `0.3`) → The `eta` parameter from XGBoost.  \n- **max\\_depth**(Default: `6`) → The `max_depth` parameter from XGBoost.  \n- **max\\_window\\_size**(Default: `1000`) → Max window size for drift detection.  \n- **min\\_window\\_size**(Default: `0`) → Min window size for drift detection.  \n- **max\\_buffer**(Default: `5`) → Buffers after which to stop growing and start replacing.  \n- **pre\\_train**(Default: `2`) → Buffers to wait before the first XGBoost training.  \n- **detect\\_drift**(Default: `True`) → If set will use a drift detector (ADWIN).  \n- **use\\_updater**(Default: `True`) → Uses `refresh` updated for XGBoost.  \n- **trees\\_per\\_train**(Default: `1`) → The number of trees for each training run.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/general_purpose/adaptivelgbm/\\#example-usage)  \nWe can create an instance and deploy AdaptiveLGBM model like this.  \n```import turboml as tb\nmodel = tb.AdaptiveLGBM(n_classes=2)\n```  \n## Footnotes [Permalink for this section](https://docs.turboml.com/general_purpose/adaptivelgbm/\\#footnote-label)  \n1. J. Montiel, R. Mitchell, E. Frank, B. Pfahringer, T. Abdessalem and A. Bifet [Adaptive XGBoost for Evolving Data Streams (opens in a new tab)](https://arxiv.org/abs/2005.07353) [↩](https://docs.turboml.com/general_purpose/adaptivelgbm/#user-content-fnref-1)  \nLast updated on January 24, 2025  \n[Adaptive XGBoost](https://docs.turboml.com/general_purpose/adaptivexgboost/ \"Adaptive XGBoost\") [Neural Networks](https://docs.turboml.com/general_purpose/neuralnetwork/ \"Neural Networks\")\n</page_content>"
    },
    {
        "section": "HoeffdingTreeRegressor",
        "content": "# HoeffdingTreeRegressor\n@ TurboML - page_link: https://docs.turboml.com/regression/hoeffdingtreeregressor/\n<page_content>\nRegression  \nHoeffding Tree Regressor  \nThe **Hoeffding Tree Regressor** (HTR) is an adaptation of the incremental tree algorithm of the same name for classification. Similarly to its classification counterpart, HTR uses the Hoeffding bound to control its split decisions. Differently from the classification algorithm, HTR relies on calculating the reduction of variance in the target space to decide among the split candidates. The smallest the variance at its leaf nodes, the more homogeneous the partitions are. At its leaf nodes, HTR fits either linear models or uses the target average as the predictor.  \n## Parameters [Permalink for this section](https://docs.turboml.com/regression/hoeffdingtreeregressor/\\#parameters)  \n- **grace\\_period**( `int`, Default: `200`) → Number of instances a leaf should observe between split attempts.  \n- **delta**( `float`, Default: `1e-07`) → Significance level to calculate the Hoeffding bound. The significance level is given by `1 - delta`. Values closer to zero imply longer split decision delays.  \n- **tau**( `float`,Default: `0.05`) → Threshold below which a split will be forced to break ties.  \n- **leaf\\_prediction**( `str`,Default: `mean`) → Prediction mechanism used at leafs. For now, only Target mean ( `mean`) is supported.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/regression/hoeffdingtreeregressor/\\#example-usage)  \nWe can create an instance of the Hoeffding Tree Regressor model like this.  \n```import turboml as tb\nhtc_model = tb.HoeffdingTreeRegressor()\n```  \nLast updated on January 24, 2025  \n[FFM Regressor](https://docs.turboml.com/regression/ffmregressor/ \"FFM Regressor\") [SGT Regressor](https://docs.turboml.com/regression/sgtregressor/ \"SGT Regressor\")\n</page_content>"
    },
    {
        "section": "Python Model: PySAD Example",
        "content": "# Python Model: PySAD Example\n@ TurboML - page_link: https://docs.turboml.com/wyo_models/pysad_example/\n<page_content>\nWrite Your Own Models  \nPySAD Example  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/pysad_example.ipynb)  \nIn this example, we use the `PySAD` package to monitor anomalies in our streaming data.  \n```import turboml as tb\n```  \nWe start off by installing and importing the `pysad` package along with its dependencies.  \n```!pip install pysad mmh3==2.5.1\n```  \n```import pandas as pd\nimport numpy as np\nfrom pysad.models import xStream\n```  \n## Model Definition [Permalink for this section](https://docs.turboml.com/wyo_models/pysad_example/\\#model-definition)  \nTurboML's inbuilt `PythonModel` can be used to define custom models which are compatible with TurboML.  \nHere we define `PySADModel` as a wrapper using `PySAD`'s `xStream` model, making sure to properly implement the required instance methods for a Python model.  \n```import turboml.common.pytypes as types\n\n\nclass PySADModel:\ndef __init__(self):\nself.model = xStream()\n\ndef init_imports(self):\nfrom pysad.models import xStream\nimport numpy as np\n\ndef learn_one(self, input: types.InputData):\nself.model = self.model.fit_partial(np.array(input.numeric))\n\ndef predict_one(self, input: types.InputData, output: types.OutputData):\nscore = self.model.score_partial(np.array(input.numeric))\noutput.set_score(score)\n```  \nNow, we create a custom `venv` so that the custom model defined above has access to all the required dependencies. PySAD required mmh3==2.5.1 as per their docs.  \n```venv = tb.setup_venv(\"my_pysad_venv\", [\"mmh3==2.5.1\", \"pysad\", \"numpy\"])\nvenv.add_python_class(PySADModel)\n```  \n## Dataset [Permalink for this section](https://docs.turboml.com/wyo_models/pysad_example/\\#dataset)  \nWe choose our standard `FraudDetection` dataset, using the `to_online` method to push it to the platform.  \n```transactions = tb.datasets.FraudDetectionDatasetFeatures().to_online(\nid=\"transactions\", load_if_exists=True\n)\nlabels = tb.datasets.FraudDetectionDatasetLabels().to_online(\nid=\"transaction_labels\", load_if_exists=True\n)\n```  \n```numerical_fields = [\\\n\"transactionAmount\",\\\n\"localHour\",\\\n\"isProxyIP\",\\\n\"digitalItemCount\",\\\n\"physicalItemCount\",\\\n]\n```  \n```features = transactions.get_model_inputs(numerical_fields=numerical_fields)\nlabel = labels.get_model_labels(label_field=\"is_fraud\")\n```  \n## Model Deployment [Permalink for this section](https://docs.turboml.com/wyo_models/pysad_example/\\#model-deployment)  \nNow, we deploy our model and extract its outputs.  \n```pysad_model = tb.Python(class_name=PySADModel.__name__, venv_name=venv.name)\n```  \n```deployed_model_pysad = pysad_model.deploy(\"pysad_model\", input=features, labels=label)\n```  \n```outputs = deployed_model_pysad.get_outputs()\n```  \n```len(outputs)\n```  \n## Evaluation [Permalink for this section](https://docs.turboml.com/wyo_models/pysad_example/\\#evaluation)  \nFinally, we use any of `PySAD`'s metrics for giving a numerical value to the degree of the presence of anomalies in our data.  \n```from pysad.evaluation import AUROCMetric\n```  \n```auroc = AUROCMetric()\nlabels_df = labels.preview_df\nfor output, y in zip(\noutputs, labels_df[\"is_fraud\"].tolist()[: len(outputs)], strict=False\n):\nauroc.update(y, output.score)\n```  \n```auroc.get()\n```  \nLast updated on January 24, 2025  \n[Batch Python Model](https://docs.turboml.com/wyo_models/batch_python_model/ \"Batch Python Model\") [UDF](https://docs.turboml.com/feature_engineering/udf/ \"UDF\")\n</page_content>"
    },
    {
        "section": "Batch APIs",
        "content": "# Batch APIs\n@ TurboML - page_link: https://docs.turboml.com/general_examples/batch_api/\n<page_content>\nGeneral  \nBatch API  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/batch_api.ipynb)  \nThe main mode of operation in TurboML is streaming, with continuous updates to different components with fresh data. However, TurboML also supports the good ol' fashioned batch APIs. We've already seen examples of this for feature engineering in the quickstart notebook. In this notebook, we'll focus primarily on batch APIs for ML modelling.  \nTo make this more interesting, we'll show how we can still have incremental training on batch data.  \n```import turboml as tb\n```  \n```import pandas as pd\nfrom sklearn import metrics\n```  \n## Dataset [Permalink for this section](https://docs.turboml.com/general_examples/batch_api/\\#dataset)  \nWe'll use our standard `FraudDetection` dataset again, but this time without pushing it to the platform. Interfaces like feature engineering and feature selection work in the exact same ways, just without being linked\nto a platform-managed dataset.  \n```transactions = tb.datasets.FraudDetectionDatasetFeatures()\nlabels = tb.datasets.FraudDetectionDatasetLabels()\n\ntransactions_p1 = transactions[:100000]\nlabels_p1 = labels[:100000]\n\ntransactions_p2 = transactions[100000:]\nlabels_p2 = labels[100000:]\n```  \n```numerical_fields = [\\\n\"transactionAmount\",\\\n\"localHour\",\\\n]\ncategorical_fields = [\\\n\"digitalItemCount\",\\\n\"physicalItemCount\",\\\n\"isProxyIP\",\\\n]\nfeatures = transactions_p1.get_model_inputs(\nnumerical_fields=numerical_fields, categorical_fields=categorical_fields\n)\nlabel = labels_p1.get_model_labels(label_field=\"is_fraud\")\n```  \n## Training [Permalink for this section](https://docs.turboml.com/general_examples/batch_api/\\#training)  \nWith the features and label defined, we can train a model in a batch way using the learn method.  \n```model = tb.HoeffdingTreeClassifier(n_classes=2)\n```  \n```model_trained_100K = model.learn(features, label)\n```  \nWe've trained a model on the first 100K rows. Now, to update this model on the remaining data, we can create another batch dataset and call the `learn` method. Note that this time, learn is called on a trained model.  \n```features = transactions_p2.get_model_inputs(\nnumerical_fields=numerical_fields, categorical_fields=categorical_fields\n)\nlabel = labels_p2.get_model_labels(label_field=\"is_fraud\")\n```  \n```model_fully_trained = model_trained_100K.learn(features, label)\n```  \n## Inference [Permalink for this section](https://docs.turboml.com/general_examples/batch_api/\\#inference)  \nWe've seen batch inference on deployed models in the quickstart notebook. We can also perform batch inference on these models using the `predict` method.  \n```outputs = model_trained_100K.predict(features)\nprint(metrics.roc_auc_score(labels_p2.df[\"is_fraud\"], outputs[\"score\"]))\noutputs = model_fully_trained.predict(features)\nprint(metrics.roc_auc_score(labels_p2.df[\"is_fraud\"], outputs[\"score\"]))\n```  \n## Deployment [Permalink for this section](https://docs.turboml.com/general_examples/batch_api/\\#deployment)  \nSo far, we've only trained a model. We haven't deployed it yet. Deploying a batch trained model is exactly like any other model deployment, except we'll set the `predict_only` option to be True. This means the model won't be updated automatically.  \n```transactions = transactions.to_online(id=\"transactions10\", load_if_exists=True)\nlabels = labels.to_online(id=\"transaction_labels\", load_if_exists=True)\n```  \n```features = transactions.get_model_inputs(\nnumerical_fields=numerical_fields, categorical_fields=categorical_fields\n)\nlabel = labels.get_model_labels(label_field=\"is_fraud\")\n```  \n```deployed_model = model_fully_trained.deploy(\nname=\"predict_only_model\", input=features, labels=label, predict_only=True\n)\n```  \n```outputs = deployed_model.get_outputs()\noutputs[-1]\n```  \n## Next Steps [Permalink for this section](https://docs.turboml.com/general_examples/batch_api/\\#next-steps)  \nIn this notebook, we discussed how to train models in a batch paradigm and deploy them. In a separate notebook we'll cover two different statregies to update models, (i) starting from a batch trained model and using continual learning, (ii) training models incrementally in a batch paradigm and updating the deployment with newer versions.  \nLast updated on January 24, 2025  \n[Quickstart](https://docs.turboml.com/quickstart/ \"Quickstart\") [Local Model](https://docs.turboml.com/general_examples/local_model/ \"Local Model\")\n</page_content>"
    },
    {
        "section": "ContextualBanditModelSelection",
        "content": "# ContextualBanditModelSelection\n@ TurboML - page_link: https://docs.turboml.com/ensembles/contextualbanditmodelselection/\n<page_content>\nEnsembles  \nContextual Bandit Model Selection  \nContextual Bandit-based model selection.  \nSimilar to BanditModelSelection, but now the algorithm uses a contextual bandit leading to more fine-grained model selection.  \n## Parameters [Permalink for this section](https://docs.turboml.com/ensembles/contextualbanditmodelselection/\\#parameters)  \n- **contextualbandit**(Default: `LinTS`) → The underlying bandit algorithm. Options are: LinTS, and LinUCB.  \n- **metric\\_name**(Default: `WindowedMAE`) → The metric to use to evaluate models. Options are: WindowedAUC, WindowedAccuracy, WindowedMAE, WindowedMSE, and WindowedRMSE.  \n- **base\\_models**( `list[Model]`) → The list of models over which to perform model selection.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/ensembles/contextualbanditmodelselection/\\#example-usage)  \nWe can create an instance and deploy BanditModel like this.  \n```import turboml as tb\nhtc_model = tb.HoeffdingTreeRegressor()\namf_model = tb.AMFRegressor()\nffm_model = tb.FFMRegressor()\nbandit_model = tb.ContextualBanditModelSelection(base_models = [htc_model, amf_model, ffm_model])\n```  \nLast updated on January 24, 2025  \n[Bandit Model Selection](https://docs.turboml.com/ensembles/banditmodelselection/ \"Bandit Model Selection\") [Leveraging Bagging Classifier](https://docs.turboml.com/ensembles/leveragingbaggingclassifier/ \"Leveraging Bagging Classifier\")\n</page_content>"
    },
    {
        "section": "ONNX tutorial with TensorFlow",
        "content": "# ONNX tutorial with TensorFlow\n@ TurboML - page_link: https://docs.turboml.com/byo_models/onnx_tensorflow/\n<page_content>\nBring Your Own Models  \nONNX - Tensorflow  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/onnx_tensorflow.ipynb)  \n```import turboml as tb\n```  \n```!pip install -U tensorflow tf2onnx onnx==1.14.1 scikit-learn\n```  \n## Tensorflow - Standard Model Training [Permalink for this section](https://docs.turboml.com/byo_models/onnx_tensorflow/\\#tensorflow---standard-model-training)  \nThe following blocks of code define a standard tensorflow training code. This is completely independent of TurboML.  \n```import pandas as pd\nimport tf2onnx.convert\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n```  \n```transactions = tb.datasets.FraudDetectionDatasetFeatures()\nlabels = tb.datasets.FraudDetectionDatasetLabels()\n```  \n```joined_df = pd.merge(transactions.df, labels.df, on=\"transactionID\", how=\"right\")\njoined_df\n```  \n```X = joined_df.drop(\"is_fraud\", axis=1)\nnumerical_fields = [\\\n\"transactionAmount\",\\\n\"localHour\",\\\n\"isProxyIP\",\\\n\"digitalItemCount\",\\\n\"physicalItemCount\",\\\n]\n\nfeats = X[numerical_fields]\ntargets = joined_df[\"is_fraud\"].astype(int)\n```  \n```from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\nfeats, targets, test_size=0.2, random_state=42\n)\n```  \n```model = Sequential()\nmodel.add(Dense(64, activation=\"relu\", input_shape=(X_train.shape[1],)))\nmodel.add(Dense(64, activation=\"relu\"))\nmodel.add(Dense(2, activation=\"softmax\"))\n```  \n```model.summary()\n```  \n```model.compile(\noptimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n)\n\nnum_epochs = 10\nmodel.fit(X_train, y_train, epochs=num_epochs, batch_size=64, verbose=1)\n```  \n```_, accuracy = model.evaluate(X_test, y_test)\nprint(\"Accuracy:\", accuracy)\n```  \n## Export model to ONNX format [Permalink for this section](https://docs.turboml.com/byo_models/onnx_tensorflow/\\#export-model-to-onnx-format)  \nExporting a model to ONNX format depends on the framework. Tutorials for different frameworks can be found at [https://github.com/onnx/tutorials#converting-to-onnx-format (opens in a new tab)](https://github.com/onnx/tutorials#converting-to-onnx-format)  \n```onnx_model_path = \"tensorflow_model.onnx\"\ninput_signature = [\\\ntf.TensorSpec([1, len(numerical_fields)], tf.float32, name=\"keras_tensor\")\\\n]\nmodel.output_names = [\"output\"]\nonnx_model, _ = tf2onnx.convert.from_keras(model, input_signature, opset=13)\n\nonnx_model_bytes = onnx_model.SerializeToString()\n```  \n## Create an ONNX model with TurboML [Permalink for this section](https://docs.turboml.com/byo_models/onnx_tensorflow/\\#create-an-onnx-model-with-turboml)  \nNow that we've converted the model to ONNX format, we can deploy it with TurboML.  \n```transactions = transactions.to_online(id=\"transactions\", load_if_exists=True)\nlabels = labels.to_online(id=\"transaction_labels\", load_if_exists=True)\n\nfeatures = transactions.get_model_inputs(numerical_fields=numerical_fields)\nlabel = labels.get_model_labels(label_field=\"is_fraud\")\n```  \n```tb.set_onnx_model(\"tensorflowmodel\", onnx_model_bytes)\nonnx_model = tb.ONNX(model_save_name=\"tensorflowmodel\")\n```  \n```deployed_model = onnx_model.deploy(\"onnx_model_tf\", input=features, labels=label)\n```  \n```deployed_model.add_metric(\"WindowedAUC\")\n```  \n```model_auc_scores = deployed_model.get_evaluation(\"WindowedAUC\")\nplt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])\n```  \n```\n```  \nLast updated on January 24, 2025  \n[ONNX - Scikit-Learn](https://docs.turboml.com/byo_models/onnx_sklearn/ \"ONNX - Scikit-Learn\") [TF-IDF Example](https://docs.turboml.com/byo_models/tfidf_example/ \"TF-IDF Example\")\n</page_content>"
    },
    {
        "section": "Local Model",
        "content": "# Local Model\n@ TurboML - page_link: https://docs.turboml.com/general_examples/local_model/\n<page_content>\nGeneral  \nLocal Model  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/local_model.ipynb)  \nLocalModel is our Python interface that gives direct access to TurboML's machine learning models.  \nWe will use the transactions.csv and labels.csv datasets for our experiments.  \n```import turboml as tb\n```  \n```import pandas as pd\nfrom turboml import LocalModel\nfrom turboml.common.models import InputSpec\nimport numpy as np\nfrom sklearn import metrics\nimport time\nimport base64\n```  \n## Load Datasets [Permalink for this section](https://docs.turboml.com/general_examples/local_model/\\#load-datasets)  \n```transactions = tb.datasets.FraudDetectionDatasetFeatures()\nlabels = tb.datasets.FraudDetectionDatasetLabels()\n\ntransactions_train = transactions[:100000]\nlabels_train = labels[:100000]\n\ntransactions_test = transactions[100000:120000]\nlabels_test = labels[100000:120000]\n```  \n## Define Input Specification [Permalink for this section](https://docs.turboml.com/general_examples/local_model/\\#define-input-specification)  \n```numerical_fields = [\\\n\"transactionAmount\",\\\n\"localHour\",\\\n]\n\ncategorical_fields = [\\\n\"digitalItemCount\",\\\n\"physicalItemCount\",\\\n\"isProxyIP\",\\\n]\n\ninput_spec = InputSpec(\nkey_field=\"index\", # If this is mentioned - \"index\" - it uses the \"key_field\", present in the Dataset(LocalDataset/OnlineDataset) in TurboML. If you want to override, set the specific column name\nnumerical_fields=numerical_fields,\ncategorical_fields=categorical_fields,\ntextual_fields=[],\nimaginal_fields=[],\ntime_field=\"\",\nlabel_field=\"is_fraud\",\n)\n```  \n## Prepare Input and Label Data [Permalink for this section](https://docs.turboml.com/general_examples/local_model/\\#prepare-input-and-label-data)  \n```train_features = transactions_train.get_model_inputs(\nnumerical_fields=numerical_fields, categorical_fields=categorical_fields\n)\ntrain_labels = labels_train.get_model_labels(label_field=\"is_fraud\")\n\ntest_features = transactions_test.get_model_inputs(\nnumerical_fields=numerical_fields, categorical_fields=categorical_fields\n)\ntest_labels = labels_test.get_model_labels(label_field=\"is_fraud\")\n```  \n## Define Model Configurations [Permalink for this section](https://docs.turboml.com/general_examples/local_model/\\#define-model-configurations)  \n```hoeffding_tree = tb.HoeffdingTreeClassifier(\ndelta=1e-7,\ntau=0.05,\ngrace_period=200,\nn_classes=2,\nleaf_pred_method=\"mc\",\nsplit_method=\"gini\",\n)\n\namf_classifier = tb.AMFClassifier(\nn_classes=2,\nn_estimators=10,\nstep=1,\nuse_aggregation=True,\ndirichlet=0.5,\nsplit_pure=False,\n)\n\nmultinomial_nb = tb.MultinomialNB(n_classes=2, alpha=1.0)\n```  \n```# Convert each Model instance to LocalModel\nhoeffding_tree_local = hoeffding_tree.to_local_model(input_spec)\namf_classifier_local = amf_classifier.to_local_model(input_spec)\nmultinomial_nb_local = multinomial_nb.to_local_model(input_spec)\n```  \n## Training and Evaluation Function [Permalink for this section](https://docs.turboml.com/general_examples/local_model/\\#training-and-evaluation-function)  \n```# Store trained models and predictions\nmodel_trained_100K = {}\ninitial_results = {}\n\nmodels_to_train = [\\\n(\"HoeffdingTree\", hoeffding_tree_local),\\\n(\"AMF\", amf_classifier_local),\\\n(\"MultinomialNB\", multinomial_nb_local),\\\n]\n```  \n```for name, model in models_to_train:\ntry:\nprint(f\"Training {name} model on first 100K records...\")\nmodel.learn(train_features, train_labels)\n\npredictions = model.predict(test_features)\nroc_auc = metrics.roc_auc_score(\ntest_labels.dataframe[\"is_fraud\"], predictions[\"score\"]\n)\naccuracy = metrics.accuracy_score(\ntest_labels.dataframe[\"is_fraud\"], predictions[\"predicted_class\"]\n)\n\nprint(f\"{name} Model Results:\")\nprint(f\"ROC AUC Score: {roc_auc:.4f}\")\nprint(f\"Accuracy Score: {accuracy:.4f}\")\n\n# Store results\nmodel_trained_100K[name] = model\ninitial_results[name] = predictions\n\nexcept Exception as e:\nprint(f\"Error with {name} model: {str(e)}\")\n```  \n## Further Training in Batches [Permalink for this section](https://docs.turboml.com/general_examples/local_model/\\#further-training-in-batches)  \nWe will continue training the Hoeffding Tree model with additional data in batches.  \n```model_hoeffding_tree = model_trained_100K.get(\"HoeffdingTree\")\nstart = 100000\nstep = 100\nstop = 102000\n\nif model_hoeffding_tree is not None:\n# Split the dataset into 10 parts for batch training\npos = start\ni = 0\nwhile pos < stop - step:\nprint(f\"\\nPreparing batch {i + 1}...\")\nfeat_batch = transactions[pos : pos + step].get_model_inputs(\nnumerical_fields=numerical_fields,\ncategorical_fields=categorical_fields,\n)\nlabel_batch = labels[pos : pos + step].get_model_labels(label_field=\"is_fraud\")\npos = pos + step\ni += 1\n\nprint(f\"Training batch {i + 1}...\")\nstart_time = time.time()\nmodel_hoeffding_tree.learn(feat_batch, label_batch)\nend_time = time.time()\nprint(\nf\"Batch {i + 1} training completed in {end_time - start_time:.2f} seconds.\"\n)\nelse:\nprint(\"Hoeffding Tree model not found in trained models.\")\n```  \n## ONNX Model [Permalink for this section](https://docs.turboml.com/general_examples/local_model/\\#onnx-model)  \n```!pip install onnx==1.14.1 scikit-learn skl2onnx river\n```  \n```from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom skl2onnx import convert_sklearn\nfrom skl2onnx.common.data_types import FloatTensorType\n\n# Prepare features and target\ntransactions_df = transactions.df\nlabels_df = labels.df\n\nX = transactions_df[numerical_fields + categorical_fields + [\"transactionID\"]]\ny = labels_df[\"is_fraud\"]\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.2, random_state=42\n)\n\n# Train sklearn model\nclf = RandomForestClassifier()\nclf.fit(X_train[numerical_fields + categorical_fields], y_train)\n```  \n```# Convert to ONNX format\ninitial_type = [(\"float_input\", FloatTensorType([None, X_train.shape[1]]))]\nonx = convert_sklearn(\nclf, initial_types=initial_type, options={type(clf): {\"zipmap\": False}}\n)\n\n# Get the serialized ONNX model\nonnx_model_data = onx.SerializeToString()\n# Base64-encode the ONNX model data\nmodel_data_base64 = base64.b64encode(onnx_model_data).decode(\"utf-8\")\n```  \n```# Create ONNX model config with the encoded model data\nonnx_model_config = [\\\n{\\\n\"algorithm\": \"ONNX\",\\\n\"onnx_config\": {\\\n\"model_save_name\": \"randomforest\",\\\n\"model_data\": model_data_base64,\\\n},\\\n}\\\n]\n\n\nonnx_input_spec = InputSpec(\nkey_field=\"index\",  # If this is mentioned - \"index\" - it uses the \"key_field\", present in the Dataset(LocalDataset/OnlineDataset) in TurboML. If you want to override, set the specific column name\nnumerical_fields=numerical_fields + categorical_fields,\ncategorical_fields=[],\ntextual_fields=[],\nimaginal_fields=[],\ntime_field=\"\",\nlabel_field=\"is_fraud\",\n)\n\nlocal_onnx_model = LocalModel(\nmodel_configs=onnx_model_config,\ninput_spec=onnx_input_spec,\n)\n```  \n```# train data\ntrain_input_data = tb.LocalDataset.from_pd(\ndf=X_train, key_field=\"transactionID\"\n).get_model_inputs(numerical_fields=numerical_fields + categorical_fields)\n\n\ntrain_label_data = tb.LocalDataset.from_pd(\ndf=pd.DataFrame({\"transactionID\": X_train.transactionID, \"is_fraud\": y_train}),\nkey_field=\"transactionID\",\n).get_model_labels(label_field=\"is_fraud\")\n```  \n```# Create test input data\ntest_input_data = tb.LocalDataset.from_pd(\ndf=X_test, key_field=\"transactionID\"\n).get_model_inputs(numerical_fields=numerical_fields + categorical_fields)\n\n\ntest_label_data = tb.LocalDataset.from_pd(\ndf=pd.DataFrame({\"transactionID\": X_test.transactionID, \"is_fraud\": y_test}),\nkey_field=\"transactionID\",\n).get_model_labels(label_field=\"is_fraud\")\n```  \n```def onnx_model():\ntry:\n# Get predictions\npredictions = local_onnx_model.predict(test_input_data)\n\n# Calculate metrics\nroc_auc = metrics.roc_auc_score(\ntest_label_data.dataframe[\"is_fraud\"],\npredictions[\"score\"],\n)\naccuracy = metrics.accuracy_score(\ntest_label_data.dataframe[\"is_fraud\"],\npredictions[\"predicted_class\"],\n)\n\nprint(\"ONNX Model Results:\")\nprint(f\"ROC AUC Score: {roc_auc:.4f}\")\nprint(f\"Accuracy Score: {accuracy:.4f}\")\n\nreturn predictions\n\nexcept Exception as e:\nprint(f\"Error testing ONNX model: {str(e)}\")\nreturn None\n\n\n# Run the test\npredictions = onnx_model()\n\nif predictions is not None:\nsklearn_preds = clf.predict(X_test)\nonnx_preds = predictions[\"predicted_class\"]\n\nmatch_rate = (sklearn_preds == onnx_preds).mean()\nprint(\"\\nPrediction Comparison:\")\nprint(f\"Sklearn vs ONNX prediction match rate: {match_rate:.4f}\")\n```  \n## Python Model Testing [Permalink for this section](https://docs.turboml.com/general_examples/local_model/\\#python-model-testing)  \n```python_model_code = \"\"\"\nfrom river import linear_model\nimport turboml.common.pytypes as types\n\nclass MyLogisticRegression:\n\ndef init_imports(self):\nfrom river import linear_model\nimport turboml.common.pytypes as types\n\ndef __init__(self):\nself.model = linear_model.LogisticRegression()\n\ndef learn_one(self, input):\n# Combine numerical and categorical features into a dictionary\nfeatures = {}\nfeatures.update({f'num_{i}': val for i, val in enumerate(input.numeric)})\nfeatures.update({f'cat_{i}': val for i, val in enumerate(input.categ)})\nself.model.learn_one(features, input.label)\n\ndef predict_one(self, input, output):\n# Combine numerical and categorical features into a dictionary\nfeatures = {}\nfeatures.update({f'num_{i}': val for i, val in enumerate(input.numeric)})\nfeatures.update({f'cat_{i}': val for i, val in enumerate(input.categ)})\nproba = self.model.predict_proba_one(features)\nscore = float(proba.get(True, 0))\noutput.set_score(score)\noutput.set_predicted_class(int(score >= 0.5))\n\"\"\"\n```  \n```# Define the model configuration\npython_model_config = {\n\"algorithm\": \"Python\",\n\"python_config\": {\n\"class_name\": \"MyLogisticRegression\",\n\"code\": python_model_code,\n},\n}\n\n# Create the LocalModel instance\nlocal_python_model = LocalModel(\nmodel_configs=[python_model_config],\ninput_spec=input_spec,\n)\n```  \n```# Train the model\nlocal_python_model.learn(train_input_data, train_label_data)\n\n# Make predictions\npredictions = local_python_model.predict(test_input_data)\n\n# Evaluate the model\nroc_auc = metrics.roc_auc_score(\ntest_label_data.dataframe[\"is_fraud\"], predictions[\"score\"]\n)\naccuracy = metrics.accuracy_score(\ntest_label_data.dataframe[\"is_fraud\"], predictions[\"predicted_class\"]\n)\n\nprint(f\"Python Model ROC AUC Score: {roc_auc:.4f}\")\nprint(f\"Python Model Accuracy Score: {accuracy:.4f}\")\n```  \n## Python Ensemble Model [Permalink for this section](https://docs.turboml.com/general_examples/local_model/\\#python-ensemble-model)  \n```# Base models (already defined and trained)\nhoeffding_tree_model = model_trained_100K[\"HoeffdingTree\"]\namf_classifier_model = model_trained_100K[\"AMF\"]\nmultinomial_nb_model = model_trained_100K[\"MultinomialNB\"]\n\n# Extract base model configurations\nbase_model_configs = [\\\nhoeffding_tree_model.model_configs[0],\\\namf_classifier_model.model_configs[0],\\\nmultinomial_nb_model.model_configs[0],\\\n]\n```  \n```# Prepare ensemble model code\nensemble_model_code = \"\"\"\nimport turboml.common.pymodel as model\nfrom typing import List\n\nclass MyEnsembleModel:\ndef __init__(self, base_models: List[model.Model]):\nif not base_models:\nraise ValueError(\"PythonEnsembleModel requires at least one base model.\")\nself.base_models = base_models\n\ndef init_imports(self):\nimport turboml.common.pytypes as types\nfrom typing import List\n\ndef learn_one(self, input):\nfor model in self.base_models:\nmodel.learn_one(input)\n\ndef predict_one(self, input, output):\ntotal_score = 0.0\nfor model in self.base_models:\nmodel_output = model.predict_one(input)\ntotal_score += model_output.score()\naverage_score = total_score / len(self.base_models)\noutput.set_score(average_score)\noutput.set_predicted_class(int(average_score >= 0.5))\n\"\"\"\n```  \n```# Define the ensemble model configuration\nensemble_model_config = {\n\"algorithm\": \"PythonEnsembleModel\",\n\"python_ensemble_config\": {\n\"class_name\": \"MyEnsembleModel\",\n\"code\": ensemble_model_code,\n},\n}\n\n# Combine the ensemble model config and base model configs\nmodel_configs = [ensemble_model_config] + base_model_configs\n\n# Create the ensemble LocalModel instance\nensemble_model = tb.LocalModel(\nmodel_configs=model_configs,\ninput_spec=input_spec,\n)\n```  \n```# Train the ensemble model\nensemble_model.learn(train_input_data, train_label_data)\n\n# Make predictions with the ensemble model\nensemble_predictions = ensemble_model.predict(test_input_data)\n\n# Evaluate the ensemble model\nroc_auc = metrics.roc_auc_score(\ntest_label_data.dataframe[\"is_fraud\"], ensemble_predictions[\"score\"]\n)\naccuracy = metrics.accuracy_score(\ntest_label_data.dataframe[\"is_fraud\"], ensemble_predictions[\"predicted_class\"]\n)\n\nprint(f\"Ensemble Model ROC AUC Score: {roc_auc:.4f}\")\nprint(f\"Ensemble Model Accuracy Score: {accuracy:.4f}\")\n```  \nLast updated on January 24, 2025  \n[Batch API](https://docs.turboml.com/general_examples/batch_api/ \"Batch API\") [Stream Dataset Online](https://docs.turboml.com/general_examples/stream_dataset_online/ \"Stream Dataset Online\")\n</page_content>"
    },
    {
        "section": "Neural Network",
        "content": "# Neural Network\n@ TurboML - page_link: https://docs.turboml.com/general_purpose/neuralnetwork/\n<page_content>\nGeneral Purpose  \nNeural Networks  \nTo parameterize a neural network, we use a configuration based on [ludwig (opens in a new tab)](https://ludwig.ai/). The main building block for neural networks is `tb.NNLayer()` which implements a fully connected layer. The parameters of NNLayer are,  \n- **output\\_size**(Default `64`) → Output size of a fully connected layer.\n- **residual\\_connections**(list\\[int\\]) → List of indices of for layers with which to establish residual\\_connections.\n- **activation**(Default: `relu`) → Default activation function applied to the output of the fully connected layers. Options are `elu`, `leakyRelu`, `logSigmoid`, `relu`, `sigmoid`, `tanh`, and `softmax`.\n- **dropout**(Default `0.3`) → Default dropout rate applied to fully connected layers. Increasing dropout is a common form of regularization to combat overfitting. The dropout is expressed as the probability of an element to be zeroed out (0.0 means no dropout)\n- **use\\_bias**(Default: `True`) → Whether the layer uses a bias vector. Options are True and False.  \n## Parameters [Permalink for this section](https://docs.turboml.com/general_purpose/neuralnetwork/\\#parameters)  \n- **dropout**(Default `0`) → Dropout value to use for the overall model.\n- **layers**(list\\[NNLayer\\]) → Neural Network layers. By default, we pass 3 layers. `[tb.NNLayer(), tb.NNLayer(), tb.NNLayer(output_size=1, activation=\"sigmoid\")]`\n- **loss\\_function**(Default: `mse`) → Which loss function to optimize. Options are `l1`, `mse`, `cross_entropy`, `nll`, `poisson_nll` and `bce`.\n- **learning\\_rate**(Default: `1e-2`) → Initial learning rate.\n- **optimizer**(Default: `sgd`) → Which optimizer to use. Options are `sgd` and `adam`.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/general_purpose/neuralnetwork/\\#example-usage)  \nWe can create an instance and deploy Neural Network model like this.  \n```import turboml as tb\nmodel = tb.NeuralNetwork(layers=[tb.NNLayer(), tb.NNLayer(), tb.NNLayer(output_size=1, activation=\"sigmoid\")])\n```  \nLast updated on January 24, 2025  \n[Adaptive LightGBM](https://docs.turboml.com/general_purpose/adaptivelgbm/ \"Adaptive LightGBM\") [Online Neural Networks](https://docs.turboml.com/general_purpose/onlineneuralnetwork/ \"Online Neural Networks\")\n</page_content>"
    },
    {
        "section": "OCR example using RestAPI Client",
        "content": "# OCR example using RestAPI Client\n@ TurboML - page_link: https://docs.turboml.com/byo_models/ocr_example/\n<page_content>\nBring Your Own Models  \nOCR  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/ocr_example.ipynb)  \nThis example demonstrates using our REST API client for OCR processing.  \n```import turboml as tb\n```  \n```!pip install surya-ocr\n```  \n```import os\nfrom PIL import Image\nimport pandas as pd\n```  \n### Launching our FastAPI application with OCR model from jupyter-notebook [Permalink for this section](https://docs.turboml.com/byo_models/ocr_example/\\#launching-our-fastapi-application-with-ocr-model-from-jupyter-notebook)  \n```import subprocess\nimport threading\n\n\ndef run_uvicorn_server(cmd, ready_event):\nprocess = subprocess.Popen(\ncmd,\nshell=True,\nstdout=subprocess.PIPE,\nstderr=subprocess.STDOUT,\nuniversal_newlines=True,\n)\nfor line in process.stdout:\nprint(line, end=\"\")\n# Check for the message indicating the server has started\nif \"Uvicorn running on\" in line:\nready_event.set()\nprocess.wait()\n\n\ncmd = \"uvicorn utils.ocr_server_app:app --port 5379 --host 0.0.0.0\"\n\nserver_ready_event = threading.Event()\nserver_thread = threading.Thread(\ntarget=run_uvicorn_server, args=(cmd, server_ready_event)\n)\nserver_thread.start()\n```  \n### Loading a dataset of Images [Permalink for this section](https://docs.turboml.com/byo_models/ocr_example/\\#loading-a-dataset-of-images)  \n```import io\nimport base64\n\nimage_dir = \"./data/test_images/\"\nimages_test = []\nlabels_test = []\nwidths_test = []\nheights_test = []\n\nfor filename in os.listdir(image_dir):\nif filename.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".tiff\", \".bmp\", \".gif\")):\nimage_path = os.path.join(image_dir, filename)\n\n# Open and process the image\nwith Image.open(image_path) as pil_image:\npil_image = pil_image.convert(\"RGB\")\n\n# Get image dimensions\nwidth, height = pil_image.size\n\n# Save the image to a bytes buffer\nimg_byte_arr = io.BytesIO()\npil_image.save(img_byte_arr, format=\"JPEG\")\nbinary_image = img_byte_arr.getvalue()\n\n# Encode the binary image data to base64\nbase64_image = base64.b64encode(binary_image).decode(\"utf-8\")\n\nimages_test.append(base64_image)\nlabels_test.append(0)  # Assigning a default label of 0\nwidths_test.append(width)\nheights_test.append(height)\n\nimage_dict_test = {\"images\": images_test, \"width\": widths_test, \"height\": heights_test}\nlabel_dict_test = {\"labels\": labels_test}\nimage_df_test = pd.DataFrame(image_dict_test)\nlabel_df_test = pd.DataFrame(label_dict_test)\nimage_df_test.reset_index(inplace=True)\nlabel_df_test.reset_index(inplace=True)\n\nprint(f\"Processed {len(images_test)} images.\")\nprint(f\"Image DataFrame shape: {image_df_test.shape}\")\nprint(f\"Label DataFrame shape: {label_df_test.shape}\")\n```  \n```image_df_test = image_df_test.reset_index(drop=True)\nlabel_df_test = label_df_test.reset_index(drop=True)\n```  \n```images_train = tb.LocalDataset.from_pd(df=image_df_test, key_field=\"index\")\nlabels_train = tb.LocalDataset.from_pd(df=label_df_test, key_field=\"index\")\n\nimages_test = tb.LocalDataset.from_pd(df=image_df_test, key_field=\"index\")\nlabels_test = tb.LocalDataset.from_pd(df=label_df_test, key_field=\"index\")\n```  \n```imaginal_fields = [\"images\"]\ncategorical_fields = [\"width\", \"height\"]\nfeatures_train = images_train.get_model_inputs(\nimaginal_fields=imaginal_fields, categorical_fields=categorical_fields\n)\ntargets_train = labels_train.get_model_labels(label_field=\"labels\")\n\nfeatures_test = images_test.get_model_inputs(\nimaginal_fields=imaginal_fields, categorical_fields=categorical_fields\n)\ntargets_test = labels_test.get_model_labels(label_field=\"labels\")\n```  \n### Using TurboML to make a request to OCR Server [Permalink for this section](https://docs.turboml.com/byo_models/ocr_example/\\#using-turboml-to-make-a-request-to-ocr-server)  \n```request_model = tb.RestAPIClient(\nserver_url=\"http://0.0.0.0:5379/predict\",\nconnection_timeout=10000,\nmax_request_time=10000,\nmax_retries=1,\n)\n```  \n```server_ready_event.wait(timeout=100)\n```  \n```model_trained = request_model.learn(features_train, targets_train)\n```  \n```outputs_test = model_trained.predict(features_test)\n```  \n```outputs_test\n```  \nLast updated on January 24, 2025  \n[ResNet Example](https://docs.turboml.com/byo_models/resnet_example/ \"ResNet Example\") [Native Python Model](https://docs.turboml.com/wyo_models/native_python_model/ \"Native Python Model\")\n</page_content>"
    },
    {
        "section": "HeteroAdaBoostClassifier",
        "content": "# HeteroAdaBoostClassifier\n@ TurboML - page_link: https://docs.turboml.com/ensembles/heteroadaboostclassifer/\n<page_content>\nEnsembles  \nHeterogeneous AdaBoost Classifier  \nSimilar to AdaBoostClassifier, but instead of multiple copies of the same model, it can work with different base models.  \n## Parameters [Permalink for this section](https://docs.turboml.com/ensembles/heteroadaboostclassifer/\\#parameters)  \n- **base\\_models**( `list[Model]`) → The list of classifier models.  \n- **n\\_classes**( `int`) → The number of classes for the classifier.  \n- **seed**( `int`, Default: `0`) → Random number generator seed for reproducibility.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/ensembles/heteroadaboostclassifer/\\#example-usage)  \nWe can create an instance and deploy AdaBoostClassifier model like this.  \n```import turboml as tb\nmodel = tb.HeteroAdaBoostClassifier(n_classes=2, base_models = [tb.HoeffdingTreeClassifier(n_classes=2), tb.AMFClassifier(n_classes=2)])\n```  \nLast updated on January 24, 2025  \n[Heterogeneous Leveraging Bagging Classifier](https://docs.turboml.com/ensembles/heteroleveragingbaggingclassifier/ \"Heterogeneous Leveraging Bagging Classifier\") [LLAMA Embedding](https://docs.turboml.com/pipeline_components/llamaembedding/ \"LLAMA Embedding\")\n</page_content>"
    },
    {
        "section": "ONNX",
        "content": "# ONNX\n@ TurboML - page_link: https://docs.turboml.com/general_purpose/onnx/\n<page_content>\nGeneral Purpose  \nONNX  \nUsing the Open Neural Network Exchange (ONNX) format to load pre-trained weights and use them for prediction. This allows using models trained via frameworks like PyTorch, TensorFlow, Scikit-Learn etc in TurboML. Note: This model doesn't learn.  \n## Parameters [Permalink for this section](https://docs.turboml.com/general_purpose/onnx/\\#parameters)  \n- **model\\_save\\_name**(str) → The name used to save the ONNX model weights.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/general_purpose/onnx/\\#example-usage)  \nWe can create an instance and deploy ONNX model like this.  \n```import turboml as tb\ntb.set_onnx_model(\"randomforest\", onx.SerializeToString())\nonnx_model = tb.ONNX(model_save_name = \"randomforest\")\n```  \nLast updated on January 24, 2025  \n[Online Neural Networks](https://docs.turboml.com/general_purpose/onlineneuralnetwork/ \"Online Neural Networks\") [AdaBoost Classifier](https://docs.turboml.com/ensembles/adaboostclassifer/ \"AdaBoost Classifier\")\n</page_content>"
    },
    {
        "section": "Resnet example using gRPC Client",
        "content": "# Resnet example using gRPC Client\n@ TurboML - page_link: https://docs.turboml.com/byo_models/resnet_example/\n<page_content>\nBring Your Own Models  \nResNet Example  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/resnet_example.ipynb)  \nThis example demonstrates using our gRPC client to perform inference with the pretrained ResNet18 model.  \n```import turboml as tb\n```  \n```!pip install kagglehub\n```  \n```import pandas as pd\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom utils.resnet_grpc_server import serve\n```  \n### Start gRPC server for pretrained Resnet18 from jupyter-notebook [Permalink for this section](https://docs.turboml.com/byo_models/resnet_example/\\#start-grpc-server-for-pretrained-resnet18-from-jupyter-notebook)  \n```import threading\n\n\ndef run_server_in_background(url):\nserve(url)  # This will start the gRPC server\n\n\n# Start the server in a separate thread\nurl = \"0.0.0.0:50021\"\nserver_thread = threading.Thread(\ntarget=run_server_in_background, args=(url,), daemon=True\n)\nserver_thread.start()\n\nprint(\"gRPC server is running in the background...\")\n```  \n### Load image Dataset from Kaggle [Permalink for this section](https://docs.turboml.com/byo_models/resnet_example/\\#load-image-dataset-from-kaggle)  \n```import kagglehub\nimport shutil\n\n# Download latest version\ntarget_path = \"./data/animal-image-classification-dataset\"\npath = kagglehub.dataset_download(\"borhanitrash/animal-image-classification-dataset\")\nshutil.move(path, target_path)\n\nprint(\"Dataset stored in:\", target_path)\n```  \n```animal_dataset = datasets.ImageFolder(root=target_path, transform=transforms.ToTensor())\ndata_loader = DataLoader(animal_dataset, batch_size=32, shuffle=True)\nimages, labels = next(iter(data_loader))\n```  \n### Convert images into bytes array. [Permalink for this section](https://docs.turboml.com/byo_models/resnet_example/\\#convert-images-into-bytes-array)  \n```import io\n\nimages_test = []\nlabels_test = []\n\nfor image_tensor, label in zip(images, labels, strict=False):\nimage = transforms.ToPILImage()(image_tensor)\nimg_byte_arr = io.BytesIO()\nimage.save(img_byte_arr, format=\"JPEG\")\nbinary_image = img_byte_arr.getvalue()\n\nimages_test.append(binary_image)\nlabels_test.append(label.item())\n\nimage_dict_test = {\"images\": images_test}\nlabel_dict_test = {\"labels\": labels_test}\nimage_df_test = pd.DataFrame(image_dict_test)\nlabel_df_test = pd.DataFrame(label_dict_test)\nimage_df_test.reset_index(inplace=True)\nlabel_df_test.reset_index(inplace=True)\n\nprint(f\"Processed {len(images_test)} images.\")\nprint(f\"Image DataFrame shape: {image_df_test.shape}\")\nprint(f\"Label DataFrame shape: {label_df_test.shape}\")\n```  \n```image_df_test = image_df_test.reset_index(drop=True)\nlabel_df_test = label_df_test.reset_index(drop=True)\n```  \n```images_test = tb.LocalDataset.from_pd(df=image_df_test, key_field=\"index\")\nlabels_test = tb.LocalDataset.from_pd(df=label_df_test, key_field=\"index\")\n```  \n```imaginal_fields = [\"images\"]\nfeatures_test = images_test.get_model_inputs(imaginal_fields=imaginal_fields)\ntargets_test = labels_test.get_model_labels(label_field=\"labels\")\n```  \n### Using TurboML Client to request gRPC server [Permalink for this section](https://docs.turboml.com/byo_models/resnet_example/\\#using-turboml-client-to-request-grpc-server)  \n```grpc_model = tb.GRPCClient(\nserver_url=\"0.0.0.0:50021\",\nconnection_timeout=10000,\nmax_request_time=10000,\nmax_retries=1,\n)\n```  \n```model_trained = grpc_model.learn(features_test, targets_test)\n```  \n```outputs = model_trained.predict(features_test)\n```  \n```outputs  # {class,probability}\n```  \nLast updated on January 24, 2025  \n[TF-IDF Example](https://docs.turboml.com/byo_models/tfidf_example/ \"TF-IDF Example\") [OCR](https://docs.turboml.com/byo_models/ocr_example/ \"OCR\")\n</page_content>"
    },
    {
        "section": "OVR (OnevsRestClassifier)",
        "content": "# OVR (OnevsRestClassifier)\n@ TurboML - page_link: https://docs.turboml.com/pipeline_components/ovr/\n<page_content>\nPipeline Components  \nOne-Vs-Rest  \nOne-vs-the-rest (OvR) multiclass strategy.  \nThis strategy consists in fitting one binary classifier per class. The computational complexity for both learning and predicting grows linearly with the number of classes. Not recommended for very large number of classes.  \n## Parameters [Permalink for this section](https://docs.turboml.com/pipeline_components/ovr/\\#parameters)  \n- **base\\_model**( `Model`) → A binary classifier, although a multi-class classifier will work too.  \n- **n\\_classes**( `int`) → The number of classes for the classifier.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/pipeline_components/ovr/\\#example-usage)  \n```import turboml as tb\nhtc_model = tb.HoeffdingTreeClassifier(n_classes=2)\novr_model = tb.OVR(n_classes = 7, base_model = htc_model)\n```  \nLast updated on January 24, 2025  \n[LLAMA Embedding](https://docs.turboml.com/pipeline_components/llamaembedding/ \"LLAMA Embedding\") [PreProcessors](https://docs.turboml.com/pipeline_components/preprocessors/ \"PreProcessors\")\n</page_content>"
    },
    {
        "section": "Hoeffding Tree Classifier",
        "content": "# Hoeffding Tree Classifier\n@ TurboML - page_link: https://docs.turboml.com/classification/hoeffdingtreeclassifier/\n<page_content>\nClassification  \nHoeffding Tree Classifier  \n**Hoeffding Tree** or Very Fast Decision Tree classifier.\nA Hoeffding Tree[1](https://docs.turboml.com/classification/hoeffdingtreeclassifier/#user-content-fn-1) is an incremental, anytime decision tree induction algorithm that is capable of learning from massive data streams, assuming that the distribution generating examples does not change over time. Hoeffding trees exploit the fact that a small sample can often be enough to choose an optimal splitting attribute. This idea is supported mathematically by the Hoeffding bound, which quantifies the number of observations (in our case, examples) needed to estimate some statistics within a prescribed precision (in our case, the goodness of an attribute).  \nA theoretically appealing feature of Hoeffding Trees not shared by other incremental decision tree learners is that it has sound guarantees of performance. Using the Hoeffding bound one can show that its output is asymptotically nearly identical to that of a non-incremental learner using infinitely many examples. Implementation based on MOA[2](https://docs.turboml.com/classification/hoeffdingtreeclassifier/#user-content-fn-2).  \n## Parameters [Permalink for this section](https://docs.turboml.com/classification/hoeffdingtreeclassifier/\\#parameters)  \n- **n\\_classes**( `int`) → The number of classes for the classifier.  \n- **grace\\_period**( `int`, Default: `200`) → Number of instances a leaf should observe between split attempts.  \n- **split\\_method**( `str`, Default: `gini`) → Split criterion to use.\n- `gini` \\- Gini\n- `info_gain` \\- Information Gain\n- `hellinger` \\- Helinger Distance\n- **delta**( `float`, Default: `1e-07`) → Significance level to calculate the Hoeffding bound. The significance level is given by 1 - delta. Values closer to zero imply longer split decision delays.  \n- **tau**( `float`,Default: `0.05`) → Threshold below which a split will be forced to break ties.  \n- **leaf\\_pred\\_method**( `str`,Default: `mc`) → Prediction mechanism used at leafs. For now only Majority Class ( `mc`) is supported.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/classification/hoeffdingtreeclassifier/\\#example-usage)  \nWe can create an instance of the HoeffdingTreeClassifier model like this.  \n```import turboml as tb\nhtc_model = tb.HoeffdingTreeClassifier(n_classes=2)\n```  \n## Footnotes [Permalink for this section](https://docs.turboml.com/classification/hoeffdingtreeclassifier/\\#footnote-label)  \n1. G. Hulten, L. Spencer, and P. Domingos. Mining time-changing data streams. In KDD’01, pages 97@106, San Francisco, CA, 2001. ACM Press. [↩](https://docs.turboml.com/classification/hoeffdingtreeclassifier/#user-content-fnref-1)  \n2. Albert Bifet, Geoff Holmes, Richard Kirkby, Bernhard Pfahringer. MOA: Massive Online Analysis; Journal of Machine Learning Research 11: 1601-1604, 2010. [↩](https://docs.turboml.com/classification/hoeffdingtreeclassifier/#user-content-fnref-2)  \nLast updated on January 24, 2025  \n[Multinomial Naive Bayes](https://docs.turboml.com/classification/multinomialnb/ \"Multinomial Naive Bayes\") [SGT Classifier](https://docs.turboml.com/classification/sgtclassifier/ \"SGT Classifier\")\n</page_content>"
    },
    {
        "section": "AdaBoostClassifier",
        "content": "# AdaBoostClassifier\n@ TurboML - page_link: https://docs.turboml.com/ensembles/adaboostclassifer/\n<page_content>\nEnsembles  \nAdaBoost Classifier  \nAn AdaBoost [1](https://docs.turboml.com/ensembles/adaboostclassifer/#user-content-fn-1) classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.  \nFor each incoming observation, each model's learn\\_one method is called k times where k is sampled from a Poisson distribution of parameter lambda. The lambda parameter is updated when the weaks learners fit successively the same observation.  \n## Parameters [Permalink for this section](https://docs.turboml.com/ensembles/adaboostclassifer/\\#parameters)  \n- **base\\_model**( `Model`) → The classifier to boost.  \n- **n\\_models**(Default: `10`) → The number of models in the ensemble.  \n- **n\\_classes**( `int`) → The number of classes for the classifier.  \n- **seed**( `int`, Default: `0`) → Random number generator seed for reproducibility.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/ensembles/adaboostclassifer/\\#example-usage)  \nWe can create an instance and deploy AdaBoostClassifier model like this.  \n```import turboml as tb\nmodel = tb.AdaBoostClassifier(n_classes=2,base_model = tb.HoeffdingTreeClassifier(n_classes=2))\n```  \n## Footnotes [Permalink for this section](https://docs.turboml.com/ensembles/adaboostclassifer/\\#footnote-label)  \n1. Y. Freund, R. Schapire, “A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting”, 1995. [↩](https://docs.turboml.com/ensembles/adaboostclassifer/#user-content-fnref-1)  \nLast updated on January 24, 2025  \n[ONNX](https://docs.turboml.com/general_purpose/onnx/ \"ONNX\") [Bandit Model Selection](https://docs.turboml.com/ensembles/banditmodelselection/ \"Bandit Model Selection\")\n</page_content>"
    },
    {
        "section": "Random Sampler",
        "content": "# Random Sampler\n@ TurboML - page_link: https://docs.turboml.com/pipeline_components/randomsampler/\n<page_content>\nPipeline Components  \nRandom Sampler  \nRandom sampling by mixing under-sampling and over-sampling.  \nThis is a wrapper for classifiers. It will train the provided classifier by both under-sampling and over-sampling the stream of given observations so that the class distribution seen by the classifier follows a given desired distribution.  \n## Parameters [Permalink for this section](https://docs.turboml.com/pipeline_components/randomsampler/\\#parameters)  \n- **classifier**( `Model`) \\- Classifier Model.  \n- **desired\\_dist**( `dict`) → The desired class distribution. The keys are the classes whilst the values are the desired class percentages. The values must sum up to 1. If set to None, then the observations will be sampled uniformly at random, which is stricly equivalent to using ensemble.BaggingClassifier.  \n- **sampling\\_rate**( `int`, Default: `1.0`) → The desired ratio of data to sample.  \n- **seed**( `int` \\| `None`, Default: `None`) → Random seed for reproducibility.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/pipeline_components/randomsampler/\\#example-usage)  \nWe can create an instance of the Random Sampler like this.  \n```import turboml as tb\nhtc_model = tb.HoeffdingTreeClassifier(n_classes=2)\nsampler_model = tb.RandomSampler(base_model = htc_model)\n```  \nLast updated on January 24, 2025  \n[Random Projection Embedding](https://docs.turboml.com/pipeline_components/randomprojectionembedding/ \"Random Projection Embedding\")\n</page_content>"
    },
    {
        "section": "FFM Classifier",
        "content": "# FFM Classifier\n@ TurboML - page_link: https://docs.turboml.com/classification/ffmclassifier/\n<page_content>\nClassification  \nFFM Classifier  \n**Field-aware Factorization Machine** [1](https://docs.turboml.com/classification/ffmclassifier/#user-content-fn-1) for binary classification.  \nThe model equation is defined by:\nWhere is the latent vector corresponding to feature for field, and is the latent vector corresponding to feature for field.\n`$$ \\sum_{f1=1}^{F} \\sum_{f2=f1+1}^{F} \\mathbf{w_{i1}} \\cdot \\mathbf{w_{i2}}, \\text{where } i1 = \\Phi(v_{f1}, f1, f2), \\quad i2 = \\Phi(v_{f2}, f2, f1), $$`\nOur implementation automatically applies MinMax scaling to the inputs, use normal distribution for latent initialization and logarithm loss for optimization.  \n## Parameters [Permalink for this section](https://docs.turboml.com/classification/ffmclassifier/\\#parameters)  \n- **n\\_factors**( `int`, Default: `10`) → Dimensionality of the factorization or number of latent factors.  \n- **l1\\_weight**( `int`, Default: `0.0`) → Amount of L1 regularization used to push weights towards 0.  \n- **l2\\_weight**( `int`, Default: `0.0`) → Amount of L2 regularization used to push weights towards 0.  \n- **l1\\_latent**( `int`, Default: `0.0`) → Amount of L1 regularization used to push latent weights towards 0.  \n- **l2\\_latent**( `int`, Default: `0.0`) → Amount of L2 regularization used to push latent weights towards 0.  \n- **intercept**( `int`, Default: `0.0`) → Initial intercept value.  \n- **intercept\\_lr**( `float`, Default: `0.01`) → Learning rate scheduler used for updating the intercept. No intercept will be used if this is set to 0.  \n- **clip\\_gradient**(Default: `1000000000000.0`) → Clips the absolute value of each gradient value.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/classification/ffmclassifier/\\#example-usage)  \nWe can create an instance of the FFM model like this.  \n```import turboml as tb\nffm_model = tb.FFMClassifier()\n```  \n## Footnotes [Permalink for this section](https://docs.turboml.com/classification/ffmclassifier/\\#footnote-label)  \n1. Juan, Y., Zhuang, Y., Chin, W.S. and Lin, C.J., 2016, September. Field-aware factorization machines for CTR prediction. In Proceedings of the 10th ACM Conference on Recommender Systems (pp. 43-50). [↩](https://docs.turboml.com/classification/ffmclassifier/#user-content-fnref-1)  \nLast updated on January 24, 2025  \n[AMF Classifier](https://docs.turboml.com/classification/amfclassifier/ \"AMF Classifier\") [Gaussian Naive Bayes](https://docs.turboml.com/classification/gaussiannb/ \"Gaussian Naive Bayes\")\n</page_content>"
    },
    {
        "section": "SNARIMAX",
        "content": "# SNARIMAX\n@ TurboML - page_link: https://docs.turboml.com/forecasting/snarimax/\n<page_content>\nForecasting  \nSNARIMAX  \n**SNARIMAX** stands for **S** easonal **N** on-linear **A** uto **R** egressive **I** ntegrated **M** oving- **A** verage with e **X** ogenous inputs model.  \nThis model generalizes many established time series models in a single interface that can be trained online. It assumes that the provided training data is ordered in time and is uniformly spaced. It is made up of the following components:  \n- S (Seasonal)  \n- N (Non-linear): Any online regression model can be used, not necessarily a linear regression as is done in textbooks.  \n- AR (Autoregressive): Lags of the target variable are used as features.  \n- I (Integrated): The model can be fitted on a differenced version of a time series. In this context, integration is the reverse of differencing.  \n- MA (Moving average): Lags of the errors are used as features.  \n- X (Exogenous): Users can provide additional features. Care has to be taken to include features that will be available both at training and prediction time.  \nEach of these components can be switched on and off by specifying the appropriate parameters. Classical time series models such as `AR`, `MA`, `ARMA`, and `ARIMA` can thus be seen as special parametrizations of the SNARIMAX model.  \nThis model is tailored for time series that are homoskedastic. In other words, it might not work well if the variance of the time series varies widely along time.  \n## Parameters [Permalink for this section](https://docs.turboml.com/forecasting/snarimax/\\#parameters)  \n- **p**( `int`) → Order of the autoregressive part. This is the number of past target values that will be included as features.  \n- **d**( `int`) → Differencing order.  \n- **q**( `int`) → Order of the moving average part. This is the number of past error terms that will be included as features.  \n- **m**( `int`, Default: `1`) → Season length used for extracting seasonal features. If you believe your data has a seasonal pattern, then set this accordingly. For instance, if the data seems to exhibit a yearly seasonality, and that your data is spaced by month, then you should set this to 12. Note that for this parameter to have any impact you should also set at least one of the `p`, `d`, and `q` parameters.  \n- **sp**( `int`, Default: `0`) → Seasonal order of the autoregressive part. This is the number of past target values that will be included as features.  \n- **sd**( `int`, Default: `0`) → Seasonal differencing order.  \n- **sq**( `int`, Default: `0`) → Seasonal order of the moving average part. This is the number of past error terms that will be included as features.  \n- **base\\_model**( `Model`) → The online regression model to use.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/forecasting/snarimax/\\#example-usage)  \nWe can create an instance and deploy SNARIMAX model like this.  \n```import turboml as tb\nsnarimax_model = tb.SNARIMAX(p = 12, q = 12, m = 12, sd = 1, base_model = tb.HoeffdingTreeRegressor())\n```  \nLast updated on January 24, 2025  \n[SGT Classifier](https://docs.turboml.com/classification/sgtclassifier/ \"SGT Classifier\") [Adaptive XGBoost](https://docs.turboml.com/general_purpose/adaptivexgboost/ \"Adaptive XGBoost\")\n</page_content>"
    },
    {
        "section": "LLM Embeddings",
        "content": "# LLM Embeddings\n@ TurboML - page_link: https://docs.turboml.com/llms/llm_embedding/\n<page_content>\nLLMs  \nLLM Embeddings  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/llm_embedding.ipynb)  \nOne of the most important ways to model NLP tasks is to use pre-trained language model embeddings. This notebook covers how to download pre-trained models, use them to get text embeddings and build ML models on top of these embeddings using TurboML. We'll demonstrate this on a SMS Spam classification use-case.  \n```import turboml as tb\n```  \n## The Dataset [Permalink for this section](https://docs.turboml.com/llms/llm_embedding/\\#the-dataset)  \nWe choose the standard SMS Spam dataset for this example  \n```!pip install river\n```  \n```import pandas as pd\nfrom river import datasets\n\ndataset = datasets.SMSSpam()\ndataset\n```  \n```dict_list_x = []\ndict_list_y = []\nfor x, y in dataset:\ndict_list_x.append(x)\ndict_list_y.append({\"label\": float(y)})\n```  \n```df_features = pd.DataFrame.from_dict(dict_list_x).reset_index()\ndf_labels = pd.DataFrame.from_dict(dict_list_y).reset_index()\n```  \n```df_features\n```  \n```df_labels\n```  \n```features = tb.OnlineDataset.from_pd(\ndf=df_features, key_field=\"index\", id=\"sms_spam_feat\", load_if_exists=True\n)\nlabels = tb.OnlineDataset.from_pd(\ndf=df_labels, key_field=\"index\", id=\"sms_spam_labels\", load_if_exists=True\n)\n```  \n```model_features = features.get_model_inputs(textual_fields=[\"body\"])\nmodel_label = labels.get_model_labels(label_field=\"label\")\n```  \n## Downloading pre-trained models [Permalink for this section](https://docs.turboml.com/llms/llm_embedding/\\#downloading-pre-trained-models)  \nHuggingface Hub ( [https://huggingface.co/models (opens in a new tab)](https://huggingface.co/models)) is one of the largest collection of pre-trained language models. It also has native intergrations with the GGUF format ( [https://huggingface.co/docs/hub/en/gguf (opens in a new tab)](https://huggingface.co/docs/hub/en/gguf)). This format is quickly becoming the standard for saving and loading models, and popular open-source projects like llama.cpp and GPT4All use this format. TurboML also uses the GGUF format to load pre-trained models. Here's how you can specify a model from Huggingface Hub, and TurboML will download and convert this in the right format.  \nWe also support quantization of the model for conversion. The supported options are \"f32\", \"f16\", \"bf16\", \"q8\\_0\", \"auto\", where \"f32\" is for float32, \"f16\" for float16, \"bf16\" for bfloat16, \"q8\\_0\" for Q8\\_0, \"auto\" for the highest-fidelity 16-bit float type depending on the first loaded tensor type. \"auto\" is the default option.  \nFor this notebook, we'll use the [https://huggingface.co/BAAI/bge-small-en-v1.5 (opens in a new tab)](https://huggingface.co/BAAI/bge-small-en-v1.5) model, with \"f16\" quantization.  \n```gguf_model = tb.llm.acquire_hf_model_as_gguf(\"BAAI/bge-small-en-v1.5\", \"f16\")\ngguf_model\n```  \nOnce we have converted the pre-trained model, we can now use this to generate embeddings. Here's how  \n```embedding_model = tb.LLAMAEmbedding(gguf_model_id=gguf_model)\ndeployed_model = embedding_model.deploy(\n\"bert_embedding\", input=model_features, labels=model_label\n)\n```  \n```outputs = deployed_model.get_outputs()\nembedding = outputs[0].get(\"record\").embeddings\nprint(\n\"Length of the embedding vector is:\",\nlen(embedding),\n\". The first 5 values are:\",\nembedding[:5],\n)\n```  \nBut embeddings directly don't solve our use-case! We ultimately need a classification model for spam detection. We can build a pre-processor that converts all our text data into numerical embeddings, and then these numerical values can be passed to a classifier model.  \n```model = tb.LlamaCppPreProcessor(base_model=tb.SGTClassifier(), gguf_model_id=gguf_model)\n```  \n```deployed_model = model.deploy(\n\"bert_sgt_classifier\", input=model_features, labels=model_label\n)\n```  \n```outputs = deployed_model.get_outputs()\noutputs[0]\n```  \nLast updated on January 24, 2025  \n[Custom Evaluation Metric](https://docs.turboml.com/post_deployment_ml/custom_metric/ \"Custom Evaluation Metric\") [Image Embeddings](https://docs.turboml.com/llms/image_embeddings/ \"Image Embeddings\")\n</page_content>"
    },
    {
        "section": "LLAMA Embedding",
        "content": "# LLAMA Embedding\n@ TurboML - page_link: https://docs.turboml.com/pipeline_components/llamaembedding/\n<page_content>\nPipeline Components  \nLLAMA Embedding  \nUse the GGUF format to load pre-trained language models. Invoke them on the textual features in the input to get embeddings for them.  \n## Parameters [Permalink for this section](https://docs.turboml.com/pipeline_components/llamaembedding/\\#parameters)  \n- **gguf\\_model\\_id**( `List[int]`) → A model id issued by `tb.acquire_hf_model_as_gguf`.  \n- **max\\_tokens\\_per\\_input**( `int`) → The maximum number of tokens to consider in the input text. Tokens beyond this limit will be truncated. Default is 512.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/pipeline_components/llamaembedding/\\#example-usage)  \nWe can create an instance and deploy LLAMAEmbedding model like this.  \n```import turboml as tb\nembedding = tb.LLAMAEmbedding(gguf_model_id=tb.acquire_hf_model_as_gguf(\"BAAI/bge-small-en-v1.5\", \"f16\"), max_tokens_per_input=512)\n```  \nLast updated on January 24, 2025  \n[Heterogeneous AdaBoost Classifier](https://docs.turboml.com/ensembles/heteroadaboostclassifer/ \"Heterogeneous AdaBoost Classifier\") [One-Vs-Rest](https://docs.turboml.com/pipeline_components/ovr/ \"One-Vs-Rest\")\n</page_content>"
    },
    {
        "section": "Model Explanations using iXAI",
        "content": "# Model Explanations using iXAI\n@ TurboML - page_link: https://docs.turboml.com/post_deployment_ml/model_explanations/\n<page_content>\nPost-Deployment ML  \nModel Explanations  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/model_explanations.ipynb)  \nThe `iXAI` module can be used in combination with TurboML to provide incremental explanations for the models being trained.  \n```import turboml as tb\n```  \nWe start by importing the `ixai` package and relevant datasets from `river`.  \n```!pip install river git+https://github.com/mmschlk/iXAI\n```  \n```import pandas as pd\nfrom ixai.explainer import IncrementalPFI\nfrom river.metrics import Accuracy\nfrom river.utils import Rolling\nfrom river.datasets.synth import Agrawal\nfrom river.datasets.synth import ConceptDriftStream\n```  \nThe sample size for the model to train on is defined.  \nAlso, we initialize a concept drift data stream using the `Agrawal` synthetic dataset from `river`.  \n```n_samples = 150_000\nstream = Agrawal(classification_function=1, seed=42)\ndrift_stream = Agrawal(classification_function=2, seed=42)\nstream = ConceptDriftStream(\nstream,\ndrift_stream,\nposition=int(n_samples * 0.5),\nwidth=int(n_samples * 0.1),\nseed=42,\n)\n```  \n```feature_names = list([x_0 for x_0, _ in stream.take(1)][0].keys())\n```  \nA batch DataFrame is constructed from the stream defined above to train our model.  \n```features_list = []\nlabels_list = []\n\nfor features, label in stream:\nif len(features_list) == n_samples:\nbreak\nfeatures_list.append(features)\nlabels_list.append(label)\n\nfeatures_df = pd.DataFrame(features_list).reset_index()\nlabels_df = pd.DataFrame(labels_list, columns=[\"label\"]).reset_index()\n```  \n```numerical_fields = feature_names\n```  \nWe use the `LocalDataset` class provided by TurboML to convert the DataFrame into a compatible dataset.  \nAs part of defining the dataset, we specify the column to be used for primary keys.  \nThen, we get the relevant features from our dataset as defined by the `numerical_fields` list.  \n```dataset_full = tb.LocalDataset.from_pd(df=features_df, key_field=\"index\")\nlabels_full = tb.LocalDataset.from_pd(df=labels_df, key_field=\"index\")\n```  \n```features = dataset_full.get_model_inputs(numerical_fields=numerical_fields)\nlabel = labels_full.get_model_labels(label_field=\"label\")\n```  \nWe will be using and training the `Hoeffding Tree Classifier` for this task.  \n```model = tb.HoeffdingTreeClassifier(n_classes=2)\n```  \n```model_learned = model.learn(features, label)\n```  \nOnce the model has finished training, we get ready to deploy it so that it can be used for prediction.  \nTo begin with, we re-define our dataset to now support streaming data, and get the relevant features as before.  \n```dataset_full = dataset_full.to_online(\nid=\"agrawal_model_explaination\", load_if_exists=True\n)\nlabels_full = labels_full.to_online(id=\"labels_model_explaination\", load_if_exists=True)\n```  \n```features = dataset_full.get_model_inputs(numerical_fields=numerical_fields)\nlabel = labels_full.get_model_labels(label_field=\"label\")\n```  \nWe specify that the model being deployed is to be used only for prediction using the `predict_only` parameter of the `deploy()` method.  \n```deployed_model = model_learned.deploy(\nname=\"demo_model_ixai\", input=features, labels=label, predict_only=True\n)\n```  \nNow, the `get_endpoints()` method is used to fetch an endpoint to which inference requests will be sent.  \n```model_endpoints = deployed_model.get_endpoints()\n```  \nWe define `model_function` as a wrapper for the inference requests being sent to the deployed model such that the outputs are compatible with `iXAI`'s explanations API.  \n```import requests\n\n\ndef model_function(x):\nresp = requests.post(\nmodel_endpoints[0], json=x, headers=tb.common.api.headers\n).json()\nresp[\"output\"] = resp.pop(\"predicted_class\")\nreturn resp\n```  \nWe instantiate the `IncrementalPFI` class from `iXAI` with our prediction function defined above, along with the relevant fields from the dataset and the loss function to calculate the feature importance values.  \n```incremental_pfi = IncrementalPFI(\nmodel_function=model_function,\nloss_function=Accuracy(),\nfeature_names=numerical_fields,\nsmoothing_alpha=0.001,\nn_inner_samples=5,\n)\n```  \nFinally, we loop through the stream for the first 10000 samples, updating our metric and `incremental_pfi` after each encountered sample.  \nAt every 1000th step, we print out the metric with the feature importance values.  \n```training_metric = Rolling(Accuracy(), window_size=1000)\nfor n, (x_i, y_i) in enumerate(stream, start=1):\nif n == 10000:\nbreak\n\nincremental_pfi.explain_one(x_i, y_i)\n\nif n % 1000 == 0:\nprint(\nf\"{n}: Accuracy: {training_metric.get()} PFI: {incremental_pfi.importance_values}\"\n)\n```  \nLast updated on January 24, 2025  \n[Drift](https://docs.turboml.com/post_deployment_ml/drift/ \"Drift\") [Custom Evaluation Metric](https://docs.turboml.com/post_deployment_ml/custom_metric/ \"Custom Evaluation Metric\")\n</page_content>"
    },
    {
        "section": "ONNX tutorial with PyTorch",
        "content": "# ONNX tutorial with PyTorch\n@ TurboML - page_link: https://docs.turboml.com/byo_models/onnx_pytorch/\n<page_content>\nBring Your Own Models  \nONNX - Pytorch  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/onnx_pytorch.ipynb)  \n```import turboml as tb\n```  \n```!pip install onnx==1.14.1\n```  \n## PyTorch - Standard Model Training [Permalink for this section](https://docs.turboml.com/byo_models/onnx_pytorch/\\#pytorch---standard-model-training)  \nThe following blocks of code define a standard pytorch dataloader, and model training code. This is completely independent of TurboML.  \n```import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport io\n```  \n```transactions = tb.datasets.FraudDetectionDatasetFeatures()\nlabels = tb.datasets.FraudDetectionDatasetLabels()\n```  \n```joined_df = pd.merge(transactions.df, labels.df, on=\"transactionID\", how=\"right\")\njoined_df\n```  \n```X = joined_df.drop(\"is_fraud\", axis=1)\nnumerical_fields = [\\\n\"transactionAmount\",\\\n\"localHour\",\\\n\"isProxyIP\",\\\n\"digitalItemCount\",\\\n\"physicalItemCount\",\\\n]\n\nfeats = X[numerical_fields]\ntargets = joined_df[\"is_fraud\"].astype(int)\n```  \n```class TransactionsDataset(Dataset):\ndef __init__(self, feats, targets):\nself.feats = feats\nself.targets = targets\n\ndef __len__(self):\nreturn len(self.feats)\n\ndef __getitem__(self, idx):\nreturn {\n\"x\": torch.tensor(self.feats.iloc[idx], dtype=torch.float),\n\"y\": torch.tensor(self.targets.iloc[idx], dtype=torch.float),\n}\n```  \n```class NeuralNet(nn.Module):\ndef __init__(self, input_size):\nsuper(NeuralNet, self).__init__()\nself.fc1 = nn.Linear(input_size, 64)\nself.fc2 = nn.Linear(64, 64)\nself.fc3 = nn.Linear(\n64, 2\n)  # Output size is 2 for binary classification (fraud or not fraud)\n\ndef forward(self, x):\n# x --> (batch_size, input_size)\nx = torch.relu(self.fc1(x))\n# x --> (batch_size, 64)\nx = torch.relu(self.fc2(x))\n# x --> (batch_size, 64)\nx = self.fc3(x)\n# x --> (batch_size, 2)\nreturn x\n```  \n```model = NeuralNet(input_size=feats.shape[1])\nmodel\n```  \n```ds = TransactionsDataset(feats, targets)\nds[0]\n```  \n```train_size = int(0.8 * len(ds))\ntest_size = len(ds) - train_size\ntrain_dataset, test_dataset = torch.utils.data.random_split(ds, [train_size, test_size])\n```  \n```batch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n```  \n```criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n```  \n```device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nnum_epochs = 10\nfor epoch in range(num_epochs):\nmodel.train()\nrunning_loss = 0.0\n\nfor data in train_loader:\ninputs = data[\"x\"].float().to(device)\ntars = data[\"y\"].long().to(device)\n\noptimizer.zero_grad()\noutputs = model(inputs)\nloss = criterion(outputs, tars)\nloss.backward()\noptimizer.step()\n\nrunning_loss += loss.item()\n\navg_loss = running_loss / len(train_loader)\nprint(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n```  \n```model.eval()  # Set the model to evaluation mode\ntotal_correct = 0\ntotal_samples = 0\n\nwith torch.no_grad():\nfor data in test_loader:\ninputs = data[\"x\"].float()\ntars = data[\"y\"].long()\noutputs = model(inputs)\n_, predicted = torch.max(outputs.data, 1)\ntotal_samples += tars.size(0)\ntotal_correct += (predicted == tars).sum().item()\n\naccuracy = total_correct / total_samples\nprint(\"Accuracy:\", accuracy)\n```  \n## Export model to ONNX format [Permalink for this section](https://docs.turboml.com/byo_models/onnx_pytorch/\\#export-model-to-onnx-format)  \nExporting a model to ONNX format depends on the framework. Tutorials for different frameworks can be found at [https://github.com/onnx/tutorials#converting-to-onnx-format (opens in a new tab)](https://github.com/onnx/tutorials#converting-to-onnx-format)  \n```model.eval()\nsample_input = torch.randn(1, len(numerical_fields))\nbuffer = io.BytesIO()\ntorch.onnx.export(model, sample_input, buffer, export_params=True, verbose=True)\nonnx_model_string = buffer.getvalue()\n```  \n## Create an ONNX model with TurboML [Permalink for this section](https://docs.turboml.com/byo_models/onnx_pytorch/\\#create-an-onnx-model-with-turboml)  \nNow that we've converted the model to ONNX format, we can deploy it with TurboML.  \n```transactions = transactions.to_online(id=\"transactions\", load_if_exists=True)\nlabels = labels.to_online(id=\"transaction_labels\", load_if_exists=True)\n\nfeatures = transactions.get_model_inputs(numerical_fields=numerical_fields)\nlabel = labels.get_model_labels(label_field=\"is_fraud\")\n```  \n```tb.set_onnx_model(\"torchmodel\", onnx_model_string)\nonnx_model = tb.ONNX(model_save_name=\"torchmodel\")\n```  \n```deployed_model = onnx_model.deploy(\"onnx_model_torch\", input=features, labels=label)\n```  \n```deployed_model.add_metric(\"WindowedAUC\")\n```  \n```model_auc_scores = deployed_model.get_evaluation(\"WindowedAUC\")\nplt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])\n```  \nLast updated on January 24, 2025  \n[Stream Dataset Online](https://docs.turboml.com/general_examples/stream_dataset_online/ \"Stream Dataset Online\") [ONNX - Scikit-Learn](https://docs.turboml.com/byo_models/onnx_sklearn/ \"ONNX - Scikit-Learn\")\n</page_content>"
    },
    {
        "section": "Half-Space Trees (HST)",
        "content": "# Half-Space Trees (HST)\n@ TurboML - page_link: https://docs.turboml.com/anomaly_detection/hst/\n<page_content>\nAnomaly Detection  \nHalf Space Trees  \nHalf-space trees are an online variant of isolation forests. They work well when anomalies are spread out. However, they do not work well if anomalies are packed together in windows.  \nBy default, we apply MinMax scaling to the inputs to ensure each feature has values that are comprised between `0` and `1`.  \nNote that high scores indicate anomalies, whereas low scores indicate normal observations.  \n## Parameters [Permalink for this section](https://docs.turboml.com/anomaly_detection/hst/\\#parameters)  \n- **n\\_trees**(Default: `20`) → Number of trees to use.  \n- **height**(Default: `12`) → Height of each tree. Note that a tree of height h is made up of h + 1 levels and therefore contains 2 \\*\\* (h + 1) - 1 nodes.  \n- **window\\_size**(Default: `50`) → Number of observations to use for calculating the mass at each node in each tree.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/anomaly_detection/hst/\\#example-usage)  \n```import turboml as tb\nhst_model = tb.HST()\n```  \nLast updated on January 24, 2025  \n[Image Input](https://docs.turboml.com/non_numeric_inputs/image_input/ \"Image Input\") [MStream](https://docs.turboml.com/anomaly_detection/mstream/ \"MStream\")\n</page_content>"
    },
    {
        "section": "SGT Regressor",
        "content": "# SGT Regressor\n@ TurboML - page_link: https://docs.turboml.com/regression/sgtregressor/\n<page_content>\nRegression  \nSGT Regressor  \nStochastic Gradient Tree for regression.  \nIncremental decision tree regressor that minimizes the mean square error to guide its growth.  \nStochastic Gradient Trees (SGT) directly minimize a loss function to guide tree growth and update their predictions. Thus, they differ from other incrementally tree learners that do not directly optimize the loss, but a data impurity-related heuristic.  \n## Parameters [Permalink for this section](https://docs.turboml.com/regression/sgtregressor/\\#parameters)  \n- **delta**( `float`, Default: `1e-07`) → Define the significance level of the F-tests performed to decide upon creating splits or updating predictions.  \n- **grace\\_period**( `int`, Default: `200`) → Interval between split attempts or prediction updates.  \n- **lambda\\_**( `float`, Default: `0.1`) → Positive float value used to impose a penalty over the tree's predictions and force them to become smaller. The greater the lambda value, the more constrained are the predictions.  \n- **gamma**( `float`, Default: `1.0`) → Positive float value used to impose a penalty over the tree's splits and force them to be avoided when possible. The greater the gamma value, the smaller the chance of a split occurring.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/regression/sgtregressor/\\#example-usage)  \nWe can create an instance of the SGT Regressor model like this.  \n```import turboml as tb\nsgt_model = tb.SGTRegressor()\n```  \nℹ  \nThis implementation enhances the original proposal[1](https://docs.turboml.com/regression/sgtregressor/#user-content-fn-1) by using an incremental strategy to discretize numerical features dynamically, rather than relying on a calibration set and parameterized number of bins. The strategy used is an adaptation of the Quantization Observer (QO)[2](https://docs.turboml.com/regression/sgtregressor/#user-content-fn-2). Different bin size setting policies are available for selection. They directly related to number of split candidates the tree is going to explore, and thus, how accurate its split decisions are going to be. Besides, the number of stored bins per feature is directly related to the tree's memory usage and runtime.  \n## Footnotes [Permalink for this section](https://docs.turboml.com/regression/sgtregressor/\\#footnote-label)  \n1. Gouk, H., Pfahringer, B., & Frank, E. (2019, October). Stochastic Gradient Trees. In Asian Conference on Machine Learning (pp. 1094-1109). [↩](https://docs.turboml.com/regression/sgtregressor/#user-content-fnref-1)  \n2. Mastelini, S.M. and de Leon Ferreira, A.C.P., 2021. Using dynamical quantization to perform split attempts in online tree regressors. Pattern Recognition Letters. [↩](https://docs.turboml.com/regression/sgtregressor/#user-content-fnref-2)  \nLast updated on January 24, 2025  \n[Hoeffding Tree Regressor](https://docs.turboml.com/regression/hoeffdingtreeregressor/ \"Hoeffding Tree Regressor\") [AMF Classifier](https://docs.turboml.com/classification/amfclassifier/ \"AMF Classifier\")\n</page_content>"
    },
    {
        "section": "Ensembling Custom Python Models in TurboML",
        "content": "# Ensembling Custom Python Models in TurboML\n@ TurboML - page_link: https://docs.turboml.com/wyo_models/ensemble_python_model/\n<page_content>\nWrite Your Own Models  \nEnsemble Python Model  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/ensemble_python_model.ipynb)  \nTurboML allows you to create custom ensemble models using Python classes, leveraging the flexibility of Python while benefiting from TurboML's performance and scalability. In this notebook, we'll walk through how to create a custom ensemble model using TurboML's PythonEnsembleModel interface.  \n```import turboml as tb\n```  \n```!pip install river\n```  \n```import pandas as pd\nimport turboml.common.pytypes as types\nimport turboml.common.pymodel as model\nimport logging\nfrom typing import List\nimport matplotlib.pyplot as plt\n```  \n## Prepare an Evaluation Dataset [Permalink for this section](https://docs.turboml.com/wyo_models/ensemble_python_model/\\#prepare-an-evaluation-dataset)  \nWe choose a standard Credit Card Fraud dataset that ships with River to evaluate our models on.  \n```features = tb.datasets.CreditCardsDatasetFeatures()\nlabels = tb.datasets.CreditCardsDatasetLabels()\n\nfeatures\n```  \n```features.df.loc[0]\n```  \n## Load Datasets into TurboML [Permalink for this section](https://docs.turboml.com/wyo_models/ensemble_python_model/\\#load-datasets-into-turboml)  \nWe'll load the features and labels with our `OnlineDataset` interface.  \n```features = tb.OnlineDataset.from_local_dataset(\nfeatures, \"cc_features\", load_if_exists=True\n)\nlabels = tb.OnlineDataset.from_local_dataset(labels, \"cc_labels\", load_if_exists=True)\n```  \n## Isolate features [Permalink for this section](https://docs.turboml.com/wyo_models/ensemble_python_model/\\#isolate-features)  \n```numerical_cols = features.preview_df.columns.tolist()\nnumerical_cols.remove(\"index\")\ninput_features = features.get_model_inputs(numerical_fields=numerical_cols)\nlabel = labels.get_model_labels(label_field=\"score\")\n```  \n## Structure of Ensemble Models [Permalink for this section](https://docs.turboml.com/wyo_models/ensemble_python_model/\\#structure-of-ensemble-models)  \nA custom ensemble model in TurboML must implement three instance methods:  \n- `init_imports`: Import any external modules used in the class.\n- `learn_one`: Receive labeled data for the model to learn from.\n- `predict_one`: Receive input features for prediction and output the result.\nHere's the general structure:  \n```class CustomEnsembleModel:\ndef __init__(self, base_models: List[types.Model]):\n# Ensure at least one base model is provided\nif not base_models:\nraise ValueError(\"PythonEnsembleModel requires at least one base model.\")\nself.base_models = base_models\n\ndef init_imports(self):\n\"\"\"\nImport any external symbols/modules used in this class\n\"\"\"\npass\n\ndef learn_one(self, input: types.InputData):\n\"\"\"\nReceives labelled data for the model to learn from\n\"\"\"\npass\n\ndef predict_one(self, input: types.InputData, output: types.OutputData):\n\"\"\"\nReceives input features for a prediction, must pass output to the\noutput object\n\"\"\"\npass\n```  \n## Example - Creating a Custom Ensemble Model  \nWe'll create a custom ensemble model that averages the predictions of its base models.  \n```class MyEnsembleModel:\ndef __init__(self, base_models: List[model.Model]):\nif not base_models:\nraise ValueError(\"PythonEnsembleModel requires at least one base model.\")\nself.base_models = base_models\nself.logger = logging.getLogger(__name__)\n\ndef init_imports(self):\npass\n\ndef learn_one(self, input: types.InputData):\ntry:\nfor model in self.base_models:\nmodel.learn_one(input)\nexcept Exception as e:\nself.logger.exception(f\"Exception in learn_one: {e}\")\n\ndef predict_one(self, input: types.InputData, output: types.OutputData):\ntry:\ntotal_score = 0.0\nfor model in self.base_models:\nmodel_output = model.predict_one(input)\nmodel_score = model_output.score()\ntotal_score += model_score\naverage_score = total_score / len(self.base_models)\noutput.set_score(average_score)\nexcept Exception as e:\nself.logger.exception(f\"Exception in predict_one: {e}\")\n```  \n## Set Up the Virtual Environment [Permalink for this section](https://docs.turboml.com/wyo_models/ensemble_python_model/\\#set-up-the-virtual-environment)  \nWe'll set up a virtual environment and add our custom ensemble class to it. Since our class requires arguments in the constructor, we'll disable validation when adding it.  \n```# Set up the virtual environment\nvenv_name = \"my_ensemble_venv\"\nvenv = tb.setup_venv(venv_name, [\"river\"])\n\n# Add the ensemble class without validation\nvenv.add_python_class(MyEnsembleModel, do_validate_as_model=False)\n```  \n## Create Base Models  \nWe'll use TurboML's built-in models as base models for our ensemble.  \n```# Create individual base models\nmodel1 = tb.HoeffdingTreeClassifier(n_classes=2)\nmodel2 = tb.AMFClassifier(n_classes=2)\n```  \n```# Create the PythonEnsembleModel\nensemble_model = tb.PythonEnsembleModel(\nbase_models=[model1, model2],\nmodule_name=\"\",\nclass_name=\"MyEnsembleModel\",\nvenv_name=venv_name,\n)\n```  \n## Deploy the Ensemble Model [Permalink for this section](https://docs.turboml.com/wyo_models/ensemble_python_model/\\#deploy-the-ensemble-model)  \nWe'll deploy the ensemble model, providing the input features and labels.  \n```deployed_ensemble_model = ensemble_model.deploy(\nname=\"ensemble_model\", input=input_features, labels=label\n)\n```  \n## Evaluate the Ensemble Model [Permalink for this section](https://docs.turboml.com/wyo_models/ensemble_python_model/\\#evaluate-the-ensemble-model)  \nWe'll add a metric to evaluate the model and plot the results.  \n```# Add a metric to the deployed model\ndeployed_ensemble_model.add_metric(\"WindowedRMSE\")\n\n# Retrieve the evaluation results\nmodel_rmse_scores = deployed_ensemble_model.get_evaluation(\"WindowedRMSE\")\n\n# Plot the RMSE scores\nplt.figure(figsize=(10, 6))\nplt.plot([score.metric for score in model_rmse_scores], label=\"Ensemble Model RMSE\")\nplt.xlabel(\"Time Steps\")\nplt.ylabel(\"RMSE\")\nplt.title(\"Ensemble Model Evaluation\")\nplt.legend()\nplt.show()\n```  \nLast updated on January 24, 2025  \n[Native Python Model](https://docs.turboml.com/wyo_models/native_python_model/ \"Native Python Model\") [Batch Python Model](https://docs.turboml.com/wyo_models/batch_python_model/ \"Batch Python Model\")\n</page_content>"
    },
    {
        "section": "Custom Evaluation Metric",
        "content": "# Custom Evaluation Metric\n@ TurboML - page_link: https://docs.turboml.com/post_deployment_ml/custom_metric/\n<page_content>\nPost-Deployment ML  \nCustom Evaluation Metric  \nTurboML allows you to define your own aggregate metrics in Python.  \n```import turboml as tb\n```  \n```from turboml.common import ModelMetricAggregateFunction\nimport math\nimport pandas as pd\n```  \n### Model Metric Aggregation function [Permalink for this section](https://docs.turboml.com/post_deployment_ml/custom_metric/\\#model-metric-aggregation-function)  \nMetric aggregate functions are used to add and compute any custom metric over model predictions and labels.  \n#### Overview of Metric Aggregate Functions [Permalink for this section](https://docs.turboml.com/post_deployment_ml/custom_metric/\\#overview-of-metric-aggregate-functions)  \nA metric aggregate function consists of the following lifecycle methods:  \n1. `create_state()`: Initializes the aggregation state.\n2. `accumulate(state, prediction, label)`: Updates the state based on input values.\n3. `retract(state, prediction, label) (optional)`: Reverses the effect of previously accumulated values (useful in sliding windows or similar contexts).\n4. `merge_states(state1, state2)`: Merges two states (for distributed computation).\n5. `finish(state)`: Computes and returns the final metric value.  \n### Steps to Define a Metric Aggregate Function [Permalink for this section](https://docs.turboml.com/post_deployment_ml/custom_metric/\\#steps-to-define-a-metric-aggregate-function)  \n**1\\. Define a Subclass**  \nCreate a subclass of `ModelMetricAggregateFunction` and override its methods.  \n**2\\. Implement Required Methods**  \nAt a minimum, one needs to implement:  \n- create\\_state\n- accumulate\n- finish\n- merge\\_states  \n### Example: Focal Loss Metric [Permalink for this section](https://docs.turboml.com/post_deployment_ml/custom_metric/\\#example-focal-loss-metric)  \nHere’s an example of a custom focal loss metric function.  \n```class FocalLoss(ModelMetricAggregateFunction):\ndef __init__(self):\nsuper().__init__()\n\ndef create_state(self):\n\"\"\"\nInitialize the aggregation state.\nReturns:\nAny: A serializable object representing the initial state of the metric.\nThis can be a tuple, dictionary, or any other serializable data structure.\nNote:\n- The serialized size of the state should be less than 8MB to ensure\ncompatibility with distributed systems and to avoid exceeding storage\nor transmission limits.\n- Ensure the state is lightweight and efficiently encodable for optimal\nperformance.\n\"\"\"\nreturn (0.0, 0)\n\ndef _compute_focal_loss(self, prediction, label, gamma=2.0, alpha=0.25):\nif prediction is None or label is None:\nreturn None\npt = prediction if label == 1 else 1 - prediction\npt = max(min(pt, 1 - 1e-6), 1e-6)\nreturn -alpha * ((1 - pt) ** gamma) * math.log(pt)\n\ndef accumulate(self, state, prediction, label):\n\"\"\"\nUpdate the state with a new prediction-target pair.\nArgs:\nstate (Any): The current aggregation state.\nprediction (float): Predicted value.\nlabel (float): Ground truth.\nReturns:\nAny: The updated aggregation state, maintaining the same format and requirements as `create_state`.\n\"\"\"\nloss_sum, weight_sum = state\nfocal_loss = self._compute_focal_loss(prediction, label)\nif focal_loss is None:\nreturn state\nreturn loss_sum + focal_loss, weight_sum + 1\n\ndef finish(self, state):\n\"\"\"\nCompute the final metric value.\nArgs:\nstate (Any): Final state.\nReturns:\nfloat: The result.\n\"\"\"\nloss_sum, weight_sum = state\nreturn 0 if weight_sum == 0 else loss_sum / weight_sum\n\ndef merge_states(self, state1, state2):\n\"\"\"\nMerge two states (for distributed computations).\nArgs:\nstate1 (Any): The first aggregation state.\nstate2 (Any): The second aggregation state.\n\nReturns:\ntuple: Merged state, maintaining the same format and requirements as `create_state`.\n\"\"\"\nloss_sum1, weight_sum1 = state1\nloss_sum2, weight_sum2 = state2\nreturn loss_sum1 + loss_sum2, weight_sum1 + weight_sum2\n```  \n### Guidelines for Implementation [Permalink for this section](https://docs.turboml.com/post_deployment_ml/custom_metric/\\#guidelines-for-implementation)  \n1. State Management:\n- Ensure the state is serializable and the serialized size of the state should be less than 8MB\n2. Edge Cases:\n- Handle cases where inputs might be None.\n- Ensure finish() handles empty states gracefully.  \nWe will create one model to test the metric. Please follow the quickstart doc for details.  \n```transactions = tb.datasets.FraudDetectionDatasetFeatures().to_online(\n\"transactions\", load_if_exists=True\n)\nlabels = tb.datasets.FraudDetectionDatasetLabels().to_online(\n\"transaction_labels\", load_if_exists=True\n)\n```  \n```model = tb.HoeffdingTreeClassifier(n_classes=2)\n```  \n```numerical_fields = [\\\n\"transactionAmount\",\\\n\"localHour\",\\\n]\nfeatures = transactions.get_model_inputs(numerical_fields=numerical_fields)\nlabel = labels.get_model_labels(label_field=\"is_fraud\")\n```  \n```deployed_model_hft = model.deploy(name=\"demo_model_hft\", input=features, labels=label)\n```  \n```outputs = deployed_model_hft.get_outputs()\n```  \nWe can register a metric and get evaluations  \n```tb.register_custom_metric(\"FocalLoss\", FocalLoss)\n```  \n```model_scores = deployed_model_hft.get_evaluation(\"FocalLoss\")\nmodel_scores[-1]\n```  \n```import matplotlib.pyplot as plt\n\nplt.plot([model_score.metric for model_score in model_scores])\n```  \nLast updated on January 24, 2025  \n[Model Explanations](https://docs.turboml.com/post_deployment_ml/model_explanations/ \"Model Explanations\") [LLM Embeddings](https://docs.turboml.com/llms/llm_embedding/ \"LLM Embeddings\")\n</page_content>"
    },
    {
        "section": "Adaptive XGBoost",
        "content": "# Adaptive XGBoost\n@ TurboML - page_link: https://docs.turboml.com/general_purpose/adaptivexgboost/\n<page_content>\nGeneral Purpose  \nAdaptive XGBoost  \nXGBoost implementation to handle concept drift based on Adaptive XGBoost for Evolving Data Streams[1](https://docs.turboml.com/general_purpose/adaptivexgboost/#user-content-fn-1).  \n## Parameters [Permalink for this section](https://docs.turboml.com/general_purpose/adaptivexgboost/\\#parameters)  \n- **n\\_classes**( `int`) → The `num_class` parameter from XGBoost.  \n- **learning\\_rate**(Default: `0.3`) → The `eta` parameter from XGBoost.  \n- **max\\_depth**(Default: `6`) → The `max_depth` parameter from XGBoost.  \n- **max\\_window\\_size**(Default: `1000`) → Max window size for drift detection.  \n- **min\\_window\\_size**(Default: `0`) → Min window size for drift detection.  \n- **max\\_buffer**(Default: `5`) → Buffers after which to stop growing and start replacing.  \n- **pre\\_train**(Default: `2`) → Buffers to wait before the first XGBoost training.  \n- **detect\\_drift**(Default: `True`) → If set will use a drift detector (ADWIN).  \n- **use\\_updater**(Default: `True`) → Uses `refresh` updated for XGBoost.  \n- **trees\\_per\\_train**(Default: `1`) → The number of trees for each training run.  \n- **percent\\_update\\_trees**(Default: `1.0`) → The fraction of boosted rounds to be used for updates.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/general_purpose/adaptivexgboost/\\#example-usage)  \nWe can create an instance and deploy AdaptiveXGBoost model like this.  \n```import turboml as tb\nmodel = tb.AdaptiveXGBoost(n_classes=2)\n```  \n## Footnotes [Permalink for this section](https://docs.turboml.com/general_purpose/adaptivexgboost/\\#footnote-label)  \n1. J. Montiel, R. Mitchell, E. Frank, B. Pfahringer, T. Abdessalem and A. Bifet [Adaptive XGBoost for Evolving Data Streams (opens in a new tab)](https://arxiv.org/abs/2005.07353) [↩](https://docs.turboml.com/general_purpose/adaptivexgboost/#user-content-fnref-1)  \nLast updated on January 24, 2025  \n[SNARIMAX](https://docs.turboml.com/forecasting/snarimax/ \"SNARIMAX\") [Adaptive LightGBM](https://docs.turboml.com/general_purpose/adaptivelgbm/ \"Adaptive LightGBM\")\n</page_content>"
    },
    {
        "section": "Random Cut Forest",
        "content": "# Random Cut Forest\n@ TurboML - page_link: https://docs.turboml.com/anomaly_detection/rcf/\n<page_content>\nAnomaly Detection  \nRandom Cut Forest  \nRCF detects anomalous data points within a data set that diverge from otherwise well-structured or patterned data. This algorithm takes a bunch of random data points cuts them into the same number of points and creates trees. If we combine all trees creates a forest of data points to determine that if a particular data point is an anomaly or not.  \n## Parameters [Permalink for this section](https://docs.turboml.com/anomaly_detection/rcf/\\#parameters)  \n- **time\\_decay**(Default: `1/2560`) → Determines how long a sample will remain before being replaced.  \n- **number\\_of\\_trees**(Default: `50`) → Number of trees to use.  \n- **output\\_after**(Default: `64`) → The number of points required by stream samplers before results are returned.  \n- **sample\\_size**(Default: `256`) → The sample size used by stream samplers in this forest .  \n## Example Usage [Permalink for this section](https://docs.turboml.com/anomaly_detection/rcf/\\#example-usage)  \n```import turboml as tb\nrcf_model = tb.RCF()\n```  \nLast updated on January 24, 2025  \n[MStream](https://docs.turboml.com/anomaly_detection/mstream/ \"MStream\") [AMF Regressor](https://docs.turboml.com/regression/amfregressor/ \"AMF Regressor\")\n</page_content>"
    },
    {
        "section": "Feature Engineering - Complex Stream Processing",
        "content": "# Feature Engineering - Complex Stream Processing\n@ TurboML - page_link: https://docs.turboml.com/feature_engineering/advanced/ibis_feature_engineering/\n<page_content>\nFeature Engineering  \nAdvanced  \nIbis Feature Engineering  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/ibis_feature_engineering.ipynb)  \nWith real-time features, there can be situtations where the feature logic cannot be expressed by simple SQL, Aggregates or Scalar Python UDFs. In such scenarios, it may be required to write custom streaming pipelines. This is where TurboML is building on Ibis ( [https://github.com/ibis-project/ibis/ (opens in a new tab)](https://github.com/ibis-project/ibis/)), to expose a DataFrame like API to support complex streaming logic for features. We currently support Apache Flink and RisingWave backends for streaming execution.  \n```import turboml as tb\n```  \n```import pandas as pd\nfrom turboml.common.sources import (\nFileSource,\nDataSource,\nTimestampFormatConfig,\nWatermark,\nDataDeliveryMode,\nS3Config,\n)\nfrom turboml.common.models import BackEnd\nimport ibis\n```  \n```transactions_df = tb.datasets.FraudDetectionDatasetFeatures().df\nlabels = tb.datasets.FraudDetectionDatasetLabels().to_online(\n\"transaction_labels\", load_if_exists=True\n)\n```  \n### Add feature definitions [Permalink for this section](https://docs.turboml.com/feature_engineering/advanced/ibis_feature_engineering/\\#add-feature-definitions)  \nTo add feature definitions, we have a class from turboml package called **IbisFeatureEngineering**. This allows us to define features.  \n```fe = tb.IbisFeatureEngineering()\n```  \nLet's upload the data for this demo  \n```%pip install minio\nfrom minio import Minio\n\nclient = Minio(\n\"play.min.io\",\naccess_key=\"Q3AM3UQ867SPQQA43P2F\",\nsecret_key=\"zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\",\nsecure=True,\n)\nbucket_name = \"ibis-demo\"\nfound = client.bucket_exists(bucket_name)\n```  \n```if not found:\nclient.make_bucket(bucket_name)\nprint(\"Created bucket\", bucket_name)\nelse:\nprint(\"Bucket\", bucket_name, \"already exists\")\n```  \n```import duckdb\n\ncon = duckdb.connect()\ncon.sql(\"SET s3_region='us-east-1';\")\ncon.sql(\"SET s3_url_style='path';\")\ncon.sql(\"SET s3_use_ssl=true;\")\ncon.sql(\"SET s3_endpoint='play.min.io';\")\ncon.sql(\"SET s3_access_key_id='Q3AM3UQ867SPQQA43P2F';\")\ncon.sql(\"SET s3_secret_access_key='zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG';\")\n```  \n```con.sql(\n\"COPY (SELECT * EXCLUDE(timestamp), TO_TIMESTAMP(CAST(timestamp AS DOUBLE)) AS timestamp FROM transactions_df) TO 's3://ibis-demo/transactions/transactions.parquet' (FORMAT 'parquet');\"\n)\n```  \n### DataSource [Permalink for this section](https://docs.turboml.com/feature_engineering/advanced/ibis_feature_engineering/\\#datasource)  \nThe **DataSource** serves as the foundational entity in the feature engineering workflow. It defines where and how the raw data is accessed for processing. After creating a DataSource, users can register their source configurations to start leveraging them in the pipeline.  \n#### Type of Delivery Modes [Permalink for this section](https://docs.turboml.com/feature_engineering/advanced/ibis_feature_engineering/\\#type-of-delivery-modes)  \n1. Dynamic:\n- Suitable for real-time or streaming data scenarios.\n- Automatically creates connectors based on the source configuration.\n- The Kafka topic becomes the primary input for feature engineering, ensuring seamless integration with downstream processing pipelines.\n2. Static:\n- Designed for batch data sources.\n- RisingWave/Flink reads directly from the source for feature engineering, eliminating the need for an intermediary Kafka topic.  \n```time_col_config = TimestampFormatConfig(\nformat_type=TimestampFormatConfig.FormatType.EpochMillis\n)\nwatermark = Watermark(\ntime_col=\"timestamp\", allowed_delay_seconds=60, time_col_config=time_col_config\n)\nds1 = DataSource(\nname=\"transactions_stream\",\nkey_fields=[\"transactionID\"],\ndelivery_mode=DataDeliveryMode.DYNAMIC,\nfile_source=FileSource(\npath=\"transactions\",\nformat=FileSource.Format.PARQUET,\ns3_config=S3Config(\nbucket=\"ibis-demo\",\nregion=\"us-east-1\",\naccess_key_id=\"Q3AM3UQ867SPQQA43P2F\",\nsecret_access_key=\"zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\",\nendpoint=\"https://play.min.io\",\n),\n),\nwatermark=watermark,\n)\n\ntb.register_source(ds1)\n```  \nTo define features we can fetch the sources and perform operations.  \n```transactions = fe.get_ibis_table(\"transactions_stream\")\n```  \n```transactions\n```  \nIn this example we use one kafka topic (transactions\\_stream) to build features using Flink.  \nWe will also use UDF to define custom functions.  \n```@ibis.udf.scalar.python()\ndef calculate_frequency_score(digital_count: float, physical_count: float) -> float:\nif digital_count > 10 or physical_count > 10:\nreturn 0.7  # High item count\nelif digital_count == 0 and physical_count > 0:\nreturn 0.3  # Physical item-only transaction\nelif digital_count > 0 and physical_count == 0:\nreturn 0.3  # Digital item-only transaction\nelse:\nreturn 0.1  # Regular transaction\n```  \nWe can define features using ibis DSL or SQL  \n```transactions_with_frequency_score = transactions.select(\nfrequency_score=calculate_frequency_score(\ntransactions.digitalItemCount, transactions.physicalItemCount\n),\ntransactionID=transactions.transactionID,\ndigitalItemCount=transactions.digitalItemCount,\nphysicalItemCount=transactions.physicalItemCount,\ntransactionAmount=transactions.transactionAmount,\ntransactionTime=transactions.transactionTime,\nisProxyIP=transactions.isProxyIP,\n)\n```  \nWe can preview features locally  \n```transactions_with_frequency_score.execute().head()\n```  \nAfter satisfied, we can materialize the features.\nIt will write the features using flink.  \nFlink uses a hybrid source to read first from iceberg table and then switches to kafka.  \n```fe.materialize_features(\ntransactions_with_frequency_score,\n\"transactions_with_frequency_score\",\n\"transactionID\",\nBackEnd.Flink,\n\"transactions_stream\",\n)\n```  \nWe can now train a model using features built using flink  \n```model = tb.RCF(number_of_trees=50)\n```  \n```numerical_fields = [\"frequency_score\"]\nfeatures = fe.get_model_inputs(\n\"transactions_with_frequency_score\", numerical_fields=numerical_fields\n)\nlabel = labels.get_model_labels(label_field=\"is_fraud\")\n```  \n```deployed_model_rcf = model.deploy(\nname=\"demo_model_ibis_flink\", input=features, labels=label\n)\n```  \n```outputs = deployed_model_rcf.get_outputs()\n```  \n```sample_output = outputs[-1]\nsample_output\n```  \n```import matplotlib.pyplot as plt\n\nplt.plot([output[\"record\"].score for output in outputs])\n```  \n```model_endpoints = deployed_model_rcf.get_endpoints()\nmodel_endpoints\n```  \n```model_query_datapoint = (\ntransactions_df[[\"transactionID\", \"digitalItemCount\", \"physicalItemCount\"]]\n.iloc[-1]\n.to_dict()\n)\nmodel_query_datapoint\n```  \n```import requests\n\nresp = requests.post(\nmodel_endpoints[0], json=model_query_datapoint, headers=tb.common.api.headers\n)\nresp.json()\n```  \n```outputs = deployed_model_rcf.get_inference(transactions_df)\noutputs\n```  \n## Risingwave FE [Permalink for this section](https://docs.turboml.com/feature_engineering/advanced/ibis_feature_engineering/\\#risingwave-fe)  \nWe can now enrich the earlier built features using flink with features built using RisingWave.  \nLet's fetch the features from server for the feature group  \n```transactions_with_frequency_score = fe.get_ibis_table(\n\"transactions_with_frequency_score\"\n)\n```  \n```@ibis.udf.scalar.python()\ndef detect_fraud(\ntransactionAmount: float, transactionTime: int, isProxyIP: float\n) -> int:\n# Example logic for flagging fraud:\n# - High transaction amount\n# - Unusual transaction times (e.g., outside of working hours)\n# - Use of proxy IP\nis_high_amount = transactionAmount > 1000  # arbitrary high amount threshold\nis_suspicious_time = (transactionTime < 6) | (\ntransactionTime > 22\n)  # non-standard hours\nis_proxy = isProxyIP == 1  # proxy IP flag\n\nreturn int(is_high_amount & is_suspicious_time & is_proxy)\n```  \n```fraud_detection_expr = detect_fraud(\ntransactions_with_frequency_score.transactionAmount,\ntransactions_with_frequency_score.transactionTime,\ntransactions_with_frequency_score.isProxyIP,\n)\n```  \n```transactions_with_fraud_flag = transactions_with_frequency_score.select(\ntransactionAmount=transactions_with_frequency_score.transactionAmount,\ntransactionTime=transactions_with_frequency_score.transactionTime,\nisProxyIP=transactions_with_frequency_score.isProxyIP,\ntransactionID=transactions_with_frequency_score.tramsactionID,\ndigitalItemCount=transactions_with_frequency_score.digitalItemCount,\nphysicalItemCount=transactions_with_frequency_score.physicalItemCount,\nfrequency_score=transactions_with_frequency_score.frequency_score,\nfraud_flag=fraud_detection_expr,\n)\n```  \n```transactions_with_fraud_flag.execute().head()\n```  \n```fe.materialize_features(\ntransactions_with_fraud_flag,\n\"transactions_with_fraud_flag\",\n\"transactionID\",\nBackEnd.Risingwave,\n\"transactions_stream\",\n)\n```  \n```model = tb.RCF(number_of_trees=50)\n```  \n```numerical_fields = [\"frequency_score\", \"fraud_flag\"]\nfeatures = fe.get_model_inputs(\n\"transactions_with_fraud_flag\", numerical_fields=numerical_fields\n)\nlabel = labels.get_model_labels(label_field=\"is_fraud\")\n```  \n```deployed_model_rcf = model.deploy(\nname=\"demo_model_ibis_risingwave\", input=features, labels=label\n)\n```  \n```outputs = deployed_model_rcf.get_outputs()\n```  \n```sample_output = outputs[-1]\nsample_output\n```  \n```import matplotlib.pyplot as plt\n\nplt.plot([output[\"record\"].score for output in outputs])\n```  \n```model_endpoints = deployed_model_rcf.get_endpoints()\nmodel_endpoints\n```  \n```model_query_datapoint = (\ntransactions_df[\\\n[\\\n\"transactionID\",\\\n\"digitalItemCount\",\\\n\"physicalItemCount\",\\\n\"transactionAmount\",\\\n\"transactionTime\",\\\n\"isProxyIP\",\\\n]\\\n]\n.iloc[-1]\n.to_dict()\n)\nmodel_query_datapoint\n```  \n```import requests\n\nresp = requests.post(\nmodel_endpoints[0], json=model_query_datapoint, headers=tb.common.api.headers\n)\nresp.json()\n```  \n```outputs = deployed_model_rcf.get_inference(transactions_df)\noutputs\n```  \nLast updated on January 24, 2025  \n[Ibis Quickstart](https://docs.turboml.com/feature_engineering/advanced/ibis_quickstart/ \"Ibis Quickstart\") [Algorithm Tuning](https://docs.turboml.com/pre_deployment_ml/algorithm_tuning/ \"Algorithm Tuning\")\n</page_content>"
    },
    {
        "section": "TurboML Ibis Quickstart",
        "content": "# TurboML Ibis Quickstart\n@ TurboML - page_link: https://docs.turboml.com/feature_engineering/advanced/ibis_quickstart/\n<page_content>\nFeature Engineering  \nAdvanced  \nIbis Quickstart  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/ibis_quickstart.ipynb)  \n```import turboml as tb\n```  \n```import pandas as pd\nimport ibis\n```  \n```transactions = tb.datasets.FraudDetectionDatasetFeatures().to_online(\n\"ibisqs_transactions\", load_if_exists=True\n)\nlabels = tb.datasets.FraudDetectionDatasetLabels().to_online(\n\"ibisqs_transaction_labels\", load_if_exists=True\n)\n```  \nThe following cells shows how to define features in ibis. The table parameter in the **create\\_ibis\\_features** function takes in the ibis expression to be used to prepare the feature.  \n```table = transactions.to_ibis()\n```  \n```@ibis.udf.scalar.python()\ndef add_one(x: float) -> float:\nreturn x + 1\n```  \n```table = table.mutate(updated_transaction_amount=add_one(table.transactionAmount))\n```  \n```agged = table.select(\ntotal_transaction_amount=table.updated_transaction_amount.sum().over(\nwindow=ibis.window(preceding=100, following=0, group_by=[table.transactionID]),\norder_by=table.timestamp,\n),\ntransactionID=table.transactionID,\nis_potential_fraud=(\ntable.ipCountryCode != table.paymentBillingCountryCode.lower()\n).ifelse(1, 0),\nipCountryCode=table.ipCountryCode,\npaymentBillingCountryCode=table.paymentBillingCountryCode,\n)\n```  \n```transactions.feature_engineering.create_ibis_features(agged)\n```  \n```transactions.feature_engineering.get_local_features()\n```  \nWe need to tell the platform to start computations for all pending features for the given topic. This can be done by calling the **materialize\\_ibis\\_features** function.  \n```transactions.feature_engineering.materialize_ibis_features()\n```  \n```model = tb.RCF(number_of_trees=50)\n```  \n```numerical_fields = [\"total_transaction_amount\", \"is_potential_fraud\"]\nfeatures = transactions.get_model_inputs(numerical_fields=numerical_fields)\nlabel = labels.get_model_labels(label_field=\"is_fraud\")\n```  \n```deployed_model_rcf = model.deploy(name=\"demo_model_ibis\", input=features, labels=label)\n```  \n```outputs = deployed_model_rcf.get_outputs()\n```  \n```len(outputs)\n```  \n```sample_output = outputs[-1]\nsample_output\n```  \n```import matplotlib.pyplot as plt\n\nplt.plot([output[\"record\"].score for output in outputs])\n```  \n```model_endpoints = deployed_model_rcf.get_endpoints()\nmodel_endpoints\n```  \n```transactions_df = transactions.preview_df\nmodel_query_datapoint = (\ntransactions_df[[\"transactionID\", \"ipCountryCode\", \"paymentBillingCountryCode\"]]\n.iloc[-1]\n.to_dict()\n)\nmodel_query_datapoint\n```  \n```import requests\n\nresp = requests.post(\nmodel_endpoints[0], json=model_query_datapoint, headers=tb.common.api.headers\n)\nresp.json()\n```  \n#### Batch Inference on Models [Permalink for this section](https://docs.turboml.com/feature_engineering/advanced/ibis_quickstart/\\#batch-inference-on-models)  \nWhile the above method is more suited for individual requests, we can also perform batch inference on the models. We use the **get\\_inference** function for this purpose.  \n```outputs = deployed_model_rcf.get_inference(transactions_df)\noutputs\n```  \nLast updated on January 24, 2025  \n[UDAF](https://docs.turboml.com/feature_engineering/udaf/ \"UDAF\") [Ibis Feature Engineering](https://docs.turboml.com/feature_engineering/advanced/ibis_feature_engineering/ \"Ibis Feature Engineering\")\n</page_content>"
    },
    {
        "section": "Performance Improvements",
        "content": "# Performance Improvements\n@ TurboML - page_link: https://docs.turboml.com/pre_deployment_ml/performance_improvements/\n<page_content>\nPre-Deployment ML  \nPerformance Improvements  \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/performance_improvements.ipynb)  \nIn this notebook, we'll cover some examples of how model performance can be improved. The techniques covered are  \n- Sampling for imbalanced learning\n- Bagging\n- Boosting\n- Continuous Model Selection using Bandits.  \n```import turboml as tb\n```  \n```import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\n```  \n```transactions = tb.datasets.FraudDetectionDatasetFeatures().to_online(\nid=\"transactions\", load_if_exists=True\n)\nlabels = tb.datasets.FraudDetectionDatasetLabels().to_online(\nid=\"transaction_labels\", load_if_exists=True\n)\n```  \n```numerical_fields = [\\\n\"transactionAmount\",\\\n\"localHour\",\\\n]\ncategorical_fields = [\\\n\"digitalItemCount\",\\\n\"physicalItemCount\",\\\n\"isProxyIP\",\\\n]\nfeatures = transactions.get_model_inputs(\nnumerical_fields=numerical_fields, categorical_fields=categorical_fields\n)\nlabel = labels.get_model_labels(label_field=\"is_fraud\")\n```  \nNow that we have our setup ready, let's first see the performance of a base HoeffdingTreeClassfier model.  \n```htc_model = tb.HoeffdingTreeClassifier(n_classes=2)\n```  \n```deployed_model = htc_model.deploy(\"htc_classifier\", input=features, labels=label)\n```  \n```labels_df = labels.preview_df\n```  \n```outputs = deployed_model.get_outputs()\n```  \n```len(outputs)\n```  \n```output_df = pd.DataFrame(\n{labels.key_field: str(x[\"record\"].key), \"class\": x[\"record\"].predicted_class}\nfor x in outputs\n)\njoined_df = output_df.merge(labels_df, how=\"inner\", on=\"transactionID\")\n\ntrue_labels = joined_df[\"is_fraud\"]\nreal_outputs = joined_df[\"class\"]\njoined_df\n```  \n```roc_auc_score(true_labels, real_outputs)\n```  \nNot bad. But can we improve it further? We haven't yet used the fact that the dataset is highly skewed.  \n## Sampling for Imbalanced Learning [Permalink for this section](https://docs.turboml.com/pre_deployment_ml/performance_improvements/\\#sampling-for-imbalanced-learning)  \n```sampler_model = tb.RandomSampler(\nn_classes=2, desired_dist=[0.5, 0.5], sampling_method=\"under\", base_model=htc_model\n)\n```  \n```deployed_model = sampler_model.deploy(\n\"undersampler_model\", input=features, labels=label\n)\n```  \n```outputs = deployed_model.get_outputs()\n```  \n```len(outputs)\n```  \n```output_df = pd.DataFrame(\n{labels.key_field: str(x[\"record\"].key), \"class\": x[\"record\"].predicted_class}\nfor x in outputs\n)\njoined_df = output_df.merge(labels_df, how=\"inner\", on=\"transactionID\")\n\ntrue_labels = joined_df[\"is_fraud\"]\nreal_outputs = joined_df[\"class\"]\njoined_df\n```  \n```roc_auc_score(true_labels, real_outputs)\n```  \n## Bagging [Permalink for this section](https://docs.turboml.com/pre_deployment_ml/performance_improvements/\\#bagging)  \n```lbc_model = tb.LeveragingBaggingClassifier(n_classes=2, base_model=htc_model)\n```  \n```deployed_model = lbc_model.deploy(\"lbc_classifier\", input=features, labels=label)\n```  \n```outputs = deployed_model.get_outputs()\n```  \n```len(outputs)\n```  \n```output_df = pd.DataFrame(\n{labels.key_field: str(x[\"record\"].key), \"class\": x[\"record\"].predicted_class}\nfor x in outputs\n)\njoined_df = output_df.merge(labels_df, how=\"inner\", on=\"transactionID\")\n\ntrue_labels = joined_df[\"is_fraud\"]\nreal_outputs = joined_df[\"class\"]\njoined_df\n```  \n```roc_auc_score(true_labels, real_outputs)\n```  \n## Boosting [Permalink for this section](https://docs.turboml.com/pre_deployment_ml/performance_improvements/\\#boosting)  \n```abc_model = tb.AdaBoostClassifier(n_classes=2, base_model=htc_model)\n```  \n```deployed_model = abc_model.deploy(\"abc_classifier\", input=features, labels=label)\n```  \n```outputs = deployed_model.get_outputs()\n```  \n```len(outputs)\n```  \n```output_df = pd.DataFrame(\n{labels.key_field: str(x[\"record\"].key), \"class\": x[\"record\"].predicted_class}\nfor x in outputs\n)\njoined_df = output_df.merge(labels_df, how=\"inner\", on=\"transactionID\")\n\ntrue_labels = joined_df[\"is_fraud\"]\nreal_outputs = joined_df[\"class\"]\njoined_df\n```  \n```roc_auc_score(true_labels, real_outputs)\n```  \n## Continuous Model Selection with Bandits [Permalink for this section](https://docs.turboml.com/pre_deployment_ml/performance_improvements/\\#continuous-model-selection-with-bandits)  \n```bandit_model = tb.BanditModelSelection(base_models=[htc_model, lbc_model, abc_model])\ndeployed_model = bandit_model.deploy(\n\"demo_classifier_bandit\", input=features, labels=label\n)\n```  \n```outputs = deployed_model.get_outputs()\n```  \n```len(outputs)\n```  \n```output_df = pd.DataFrame(\n{labels.key_field: str(x[\"record\"].key), \"class\": x[\"record\"].predicted_class}\nfor x in outputs\n)\njoined_df = output_df.merge(labels_df, how=\"inner\", on=\"transactionID\")\n\ntrue_labels = joined_df[\"is_fraud\"]\nreal_outputs = joined_df[\"class\"]\njoined_df\n```  \n```roc_auc_score(true_labels, real_outputs)\n```  \nLast updated on January 24, 2025  \n[Hyperparameter Tuning](https://docs.turboml.com/pre_deployment_ml/hyperparameter_tuning/ \"Hyperparameter Tuning\") [Drift](https://docs.turboml.com/post_deployment_ml/drift/ \"Drift\")\n</page_content>"
    },
    {
        "section": "Online Neural Network",
        "content": "# Online Neural Network\n@ TurboML - page_link: https://docs.turboml.com/general_purpose/onlineneuralnetwork/\n<page_content>\nGeneral Purpose  \nOnline Neural Networks  \nNeural Network implementation using Hedge Backpropagation based on Online Deep Learning: Learning Deep Neural Networks on the Fly[1](https://docs.turboml.com/general_purpose/onlineneuralnetwork/#user-content-fn-1).  \n## Parameters [Permalink for this section](https://docs.turboml.com/general_purpose/onlineneuralnetwork/\\#parameters)  \n- **max\\_num\\_hidden\\_layers**(Default `10`) → The maximum number of hidden layers\n- **qtd\\_neuron\\_hidden\\_layer**(Default: `32`) → Hidden dimension of the intermediate neural network layers.\n- **n\\_classes**( `int`) → Number of classes.\n- **b**(Default: `0.99`) → Discounting parameter in the hedge backprop algorithm.\n- **n**(Default: `0.01`) → Learning rate parameter in the hedge backprop algorithm.\n- **s**(Default: `0.2`) → Smoothing parameter in the hedge backprop algorithm.  \n## Example Usage [Permalink for this section](https://docs.turboml.com/general_purpose/onlineneuralnetwork/\\#example-usage)  \nWe can create an instance and deploy ONN model like this.  \n```import turboml as tb\nmodel = tb.ONN(n_classes=2)\n```  \n## Footnotes [Permalink for this section](https://docs.turboml.com/general_purpose/onlineneuralnetwork/\\#footnote-label)  \n1. D. Sahoo, Q. Pham, J. Lu and S. Hoi. [Online Deep Learning: Learning Deep Neural Networks on the Fly (opens in a new tab)](https://arxiv.org/abs/1711.03705) [↩](https://docs.turboml.com/general_purpose/onlineneuralnetwork/#user-content-fnref-1)  \nLast updated on January 24, 2025  \n[Neural Networks](https://docs.turboml.com/general_purpose/neuralnetwork/ \"Neural Networks\") [ONNX](https://docs.turboml.com/general_purpose/onnx/ \"ONNX\")\n</page_content>"
    },
    {
        "section": "__init__.py",
        "content": "# __init__.py\n-## Location -> root_directory.common\nimport base64\nimport logging\nfrom typing import Optional, Callable, Tuple\nimport itertools  \nimport cloudpickle\nfrom cloudpickle import DEFAULT_PROTOCOL\nfrom pydantic import BaseModel  \nfrom turboml.common.datasets import LocalInputs, LocalLabels  \nfrom . import namespaces\nfrom . import llm\nfrom .types import PythonModel\nfrom .pytypes import InputData, OutputData\nfrom .datasets import OnlineDataset, LocalDataset\nfrom . import datasets\nfrom .feature_engineering import (\nIbisFeatureEngineering,\nget_timestamp_formats,\nretrieve_features,\nget_features,\n)\nfrom .models import (\nAddPythonClassRequest,\nServiceEndpoints,\nUser,\nInputSpec,\nVenvSpec,\nSupervisedAlgorithms,\nUnsupervisedAlgorithms,\nExternalUdafFunctionSpec,\nUdafFunctionSpec,\nCustomMetric,\n)\nfrom .dataloader import (\nupload_df,\nget_proto_msgs,\nget_protobuf_class,\ncreate_protobuf_from_row_tuple,\ncreate_protobuf_from_row_dict,\nPROTO_PREFIX_BYTE_LEN,\n)\nfrom .api import api\nfrom .concurrent import use_multiprocessing\nfrom .internal import TbPyArrow\nfrom .ml_algs import (\nevaluation_metrics,\nget_default_parameters,\nml_modelling,\n_resolve_duplicate_columns,\nget_score_for_model,\nRCF,\nHST,\nMStream,\nONNX,\nHoeffdingTreeClassifier,\nHoeffdingTreeRegressor,\nAMFClassifier,\nAMFRegressor,\nFFMClassifier,\nFFMRegressor,\nSGTClassifier,\nSGTRegressor,\nRandomSampler,\nNNLayer,\nNeuralNetwork,\nPython,\nONN,\nOVR,\nMultinomialNB,\nGaussianNB,\nAdaptiveXGBoost,\nAdaptiveLGBM,\nMinMaxPreProcessor,\nNormalPreProcessor,\nRobustPreProcessor,\nLlamaCppPreProcessor,\nLlamaTextPreprocess,\nClipEmbeddingPreprocessor,\nPreProcessor,\nLabelPreProcessor,\nOneHotPreProcessor,\nTargetPreProcessor,\nFrequencyPreProcessor,\nBinaryPreProcessor,\nImageToNumericPreProcessor,\nSNARIMAX,\nLeveragingBaggingClassifier,\nHeteroLeveragingBaggingClassifier,\nAdaBoostClassifier,\nHeteroAdaBoostClassifier,\nBanditModelSelection,\nContextualBanditModelSelection,\nRandomProjectionEmbedding,\nLLAMAEmbedding,\nLlamaText,\nClipEmbedding,\nRestAPIClient,\nEmbeddingModel,\nModel,\nDeployedModel,\nPythonEnsembleModel,\nGRPCClient,\nLocalModel,\n)\nfrom .model_comparison import compare_model_metrics\nfrom .sources import DataSource\nfrom .udf import ModelMetricAggregateFunction\nfrom .env import CONFIG  \nlogger = logging.getLogger(\"turboml.common\")  \nretrieve_model = Model.retrieve_model  \n__all__ = [\n\"init\",\n\"use_multiprocessing\",\n\"IbisFeatureEngineering\",\n\"get_timestamp_formats\",\n\"upload_df\",\n\"register_source\",\n\"register_custom_metric\",\n\"get_protobuf_class\",\n\"create_protobuf_from_row_tuple\",\n\"create_protobuf_from_row_dict\",\n\"retrieve_features\",\n\"get_features\",\n\"set_onnx_model\",\n\"ml_modelling\",\n\"setup_venv\",\n\"get_proto_msgs\",\n\"ml_algorithms\",\n\"evaluation_metrics\",\n\"get_default_parameters\",\n\"hyperparameter_tuning\",\n\"algorithm_tuning\",\n\"compare_model_metrics\",\n\"login\",\n\"get_user_info\",\n\"InputSpec\",\n\"RCF\",\n\"HST\",\n\"MStream\",\n\"ONNX\",\n\"HoeffdingTreeClassifier\",\n\"HoeffdingTreeRegressor\",\n\"AMFClassifier\",\n\"AMFRegressor\",\n\"FFMClassifier\",\n\"FFMRegressor\",\n\"SGTClassifier\",\n\"SGTRegressor\",\n\"RandomSampler\",\n\"NNLayer\",\n\"NeuralNetwork\",\n\"Python\",\n\"PythonEnsembleModel\",\n\"ONN\",\n\"OVR\",\n\"MultinomialNB\",\n\"GaussianNB\",\n\"AdaptiveXGBoost\",\n\"AdaptiveLGBM\",\n\"MinMaxPreProcessor\",\n\"NormalPreProcessor\",\n\"RobustPreProcessor\",\n\"LlamaCppPreProcessor\",\n\"LlamaTextPreprocess\",\n\"ClipEmbeddingPreprocessor\",\n\"PreProcessor\",\n\"LabelPreProcessor\",\n\"OneHotPreProcessor\",\n\"TargetPreProcessor\",\n\"FrequencyPreProcessor\",\n\"BinaryPreProcessor\",\n\"ImageToNumericPreProcessor\",\n\"SNARIMAX\",\n\"LeveragingBaggingClassifier\",\n\"HeteroLeveragingBaggingClassifier\",\n\"AdaBoostClassifier\",\n\"HeteroAdaBoostClassifier\",\n\"BanditModelSelection\",\n\"ContextualBanditModelSelection\",\n\"RandomProjectionEmbedding\",\n\"LLAMAEmbedding\",\n\"LlamaText\",\n\"ClipEmbedding\",\n\"RestAPIClient\",\n\"EmbeddingModel\",\n\"retrieve_model\",\n\"DeployedModel\",\n\"GRPCClient\",\n\"namespaces\",\n\"llm\",\n\"LocalModel\",\n\"PROTO_PREFIX_BYTE_LEN\",\n\"OnlineDataset\",\n\"LocalDataset\",\n\"datasets\",\n]  \ndef ml_algorithms(have_labels: bool) -> list[str]:\nif have_labels:\nalgs = [enum.value for enum in SupervisedAlgorithms]\nelse:\nalgs = [enum.value for enum in UnsupervisedAlgorithms]  \nfor alg in algs:\nif alg not in globals():\nraise Exception(f\"{alg} class doesn't exist\")\nelif alg not in __all__:\nraise Exception(f\"{alg} class hasn't been exposed\")  \nreturn algs  \ndef login(\napi_key: Optional[str] = None,\nusername: Optional[str] = None,\npassword: Optional[str] = None,\n):\n\"\"\"\nAuthenticate with the TurboML server.\nThe user should provide either the api_key or username, password.\nIf a username and password are provided, the api_key will be retrieved from the server.\nNote that instead of login, you can set the TURBOML_API_KEY env variable as well with your api_key.  \nArgs:\napi_key, username, password (str)\nRaises:\nException: Raises an exception if authentication fails.\n\"\"\"\napi.login(api_key, username, password)  \ndef init(backend_url: str, api_key: str):\n\"\"\"\nInitialize SDK and Authenticate with TurboML backend server.  \nArgs:\nbackend_url, api_key (str)\nRaises:\nException: Raises an exception if authentication fails.\n\"\"\"\nCONFIG.set_backend_server(backend_url)\nlogin(api_key=api_key)\nresponse = api.get(\"service/endpoints\").json()\nservice_endpoints = ServiceEndpoints(**response)\nCONFIG.set_feature_server(service_endpoints.feature_server)\nCONFIG.set_arrow_server(service_endpoints.arrow_server)  \ndef get_user_info() -> User:\nresp = api.get(\"user\").json()\nuser_info = User(**resp)\nreturn user_info  \ndef register_source(source: DataSource):\nif not isinstance(source, DataSource):\nraise TypeError(\"Expected a DataSource, found %s\" % type(source))\napi.post(\nendpoint=\"register_datasource\",\njson=source.model_dump(exclude_none=True),\n)  \ndef register_custom_metric(name: str, cls: type[ModelMetricAggregateFunction]):\n\"\"\"\nAdds a custom model metric to the system.  \nThis function registers a custom metric class that must extend\nModelMetricAggregateFunction. The metric class should implement the\nmethods 'create_state', 'accumulate', 'merge_states' and 'finish' to calculate the desired metric (e.g., accuracy, AUC, etc.).  \nArgs:\nname (str): The name used to register and identify the metric.\ncls (Callable[..., ModelMetricAggregateFunction]): The custom metric class\nthat should inherit from ModelMetricAggregateFunction.  \nRaises:\nTypeError: If `cls` does not inherit from `ModelMetricAggregateFunction`.\n\"\"\"\nif not issubclass(cls, ModelMetricAggregateFunction):\nraise TypeError(\nf\"{cls.__name__} must be a subclass of ModelMetricAggregateFunction.\"\n)\nspec = ExternalUdafFunctionSpec(obj=base64.b64encode(cloudpickle.dumps(cls)))  \npayload = UdafFunctionSpec(name=name, spec=spec, libraries=[])\nheaders = {\"Content-Type\": \"application/json\"}\napi.post(endpoint=\"register_udaf\", data=payload.model_dump_json(), headers=headers)\napi.post(\nendpoint=\"register_custom_metric\",\ndata=CustomMetric(metric_name=name, metric_spec={}).model_dump_json(),\nheaders=headers,\n)  \ndef set_onnx_model(input_model: str, onnx_bytes: bytes) -> None:\n\"\"\"\ninput_model: str\nThe model name(without the .onnx extension)\nonnx_bytes: bytes\nThe model bytes\n\"\"\"\napi.post(\nendpoint=\"onnx_model\",\ndata={\"input_model\": input_model},\nfiles={\n\"onnx_bytes\": (\nf\"{input_model}_bytes\",\nonnx_bytes,\n\"application/octet-stream\",\n)\n},\n)  \nclass Venv(BaseModel):\nname: str  \ndef add_python_file(self, filepath: str):\n\"\"\"Add a python source file to the system.  \nThis function registers input source file in the system.  \nArgs:\nfilepath (str): Path of the Python source file.  \nRaises:\nException: Raises an exception if registering the source with the system fails.\n\"\"\"\nwith open(filepath, \"rb\") as file:\nfiles = {\"python_file\": (filepath, file, \"text/x-python\")}\napi.post(f\"venv/{self.name}/python_source\", files=files)  \ndef add_python_class(\nself, cls: Callable[..., PythonModel], do_validate_as_model: bool = True\n):\n\"\"\"\nAdd a Python class to the system.\nBy default, validates the class as a model by instantiating and calling the init_imports,\nlearn_one, and predict_one methods. However this can be disabled with\ndo_validate_as_model=False, for instance when the required libraries are not\navailable or cannot be installed in the current environment.\n\"\"\"\nif not isinstance(cls, type):  # Should be a class\nraise ValueError(\"Input should be a class\")\nif do_validate_as_model:\ntry:\nVenv._validate_python_model_class(cls)\nexcept Exception as e:\nraise ValueError(\nf\"{e!r}. HINT: Set do_validate_as_model=False to skip validation if you believe the class is valid.\"\n) from e\nserialized_cls = base64.b64encode(\ncloudpickle.dumps(cls, protocol=DEFAULT_PROTOCOL)\n)\nreq = AddPythonClassRequest(obj=serialized_cls, name=cls.__name__)\nheaders = {\"Content-Type\": \"application/json\"}\napi.post(f\"venv/{self.name}/class\", data=req.model_dump_json(), headers=headers)  \n@staticmethod\ndef _validate_python_model_class(model_cls: Callable[..., PythonModel]):\ntry:\nmodel = model_cls()\nlogger.debug(\"Model class instantiated successfully\")\ninit_imports = getattr(model, \"init_imports\", None)\nif init_imports is None or not callable(init_imports):\nraise ValueError(\n\"Model class must have an init_imports method to import libraries\"\n)\ninit_imports()\nlogger.debug(\"Model class imports initialized successfully\")\nlearn_one = getattr(model, \"learn_one\", None)\npredict_one = getattr(model, \"predict_one\", None)\nif learn_one is None or not callable(learn_one):\nraise ValueError(\"Model class must have a learn_one method\")\nif predict_one is None or not callable(predict_one):\nraise ValueError(\"Model class must have a predict_one method\")\n## TODO: Once we have the Model.get_dimensions interface in place, use it to determine\n## appropriate input and output shape for the model before passing them to make this check\n## less brittle.\nmodel.learn_one(InputData.random())\nlogger.debug(\"Model class learn_one method validated successfully\")\nmodel.predict_one(InputData.random(), OutputData.random())\nlogger.debug(\"Model class predict_one method validated successfully\")\nexcept Exception as e:\n## NOTE: We have the\nraise ValueError(f\"Model class validation failed: {e!r}\") from e  \ndef setup_venv(venv_name: str, lib_list: list[str]) -> Venv:\n\"\"\"Executes `pip install \" \".join(lib_list)` in venv_name virtual environment.\nIf venv_name doesn't exist, it'll create one.  \nArgs:\nvenv_name (str): Name of virtual environment\nlib_list (list[str]): List of libraries to install. Will be executed as `pip install \" \".join(lib_list)`  \nRaises:\nException: Raises an exception if setting up the venv fails.\n\"\"\"\npayload = VenvSpec(venv_name=venv_name, lib_list=lib_list)\napi.post(\"venv\", json=payload.model_dump())\nreturn Venv(name=venv_name)  \ndef _check_hyperparameter_space(\nhyperparameter_space: list[dict[str, list[str]]], model: Model\n):\nmodel_config = model.get_model_config()  \nif len(hyperparameter_space) != len(model_config):\nraise Exception(\n\"The number of hyperparameter spaces should be equal to the number of entities in the model.\"\n)  \nfor idx in range(len(hyperparameter_space)):\nfor key, value in hyperparameter_space[idx].items():\nif key not in model_config[idx]:\nraise Exception(\nf\"Hyperparameter {key} is not a part of the model configuration.\"\n)\nif not value:\nraise Exception(f\"No values provided for hyperparameter {key}.\")  \nfor key, value in model_config[idx].items():\nif key not in hyperparameter_space[idx].keys():\nhyperparameter_space[idx][key] = [value]  \nSCORE_METRICS = [\n\"average_precision\",\n\"neg_brier_score\",\n\"neg_log_loss\",\n\"roc_auc\",\n\"roc_auc_ovo\",\n\"roc_auc_ovo_weighted\",\n\"roc_auc_ovr\",\n\"roc_auc_ovr_weighted\",\n]  \ndef hyperparameter_tuning(\nmetric_to_optimize: str,\nmodel: Model,\nhyperparameter_space: list[dict[str, list[str]]],\ninput: LocalInputs,\nlabels: LocalLabels,\n) -> list[Tuple[Model, float]]:\n\"\"\"\nPerform Hyperparameter Tuning on a model using Grid Search.  \nArgs:\nmetric_to_optimize: str\nThe performance metric to be used to find the best model.\nmodel: turboml.Model\nThe model object to be tuned.\nhyperparameter_space: list[dict[str, list[str]]]\nA list of dictionaries specifying the hyperparameters and the corresponding values to be tested for each entity which is a part of `model`.\ninput: Inputs\nThe input configuration for the models\nlabels: Labels\nThe label configuration for the models  \nReturns:\nlist[Tuple[Model, float]]: The list of all models with their corresponding scores sorted in descending order.  \n\"\"\"\n_check_hyperparameter_space(hyperparameter_space, model)  \nproduct_spaces = [\nlist(itertools.product(*space.values())) for space in hyperparameter_space\n]\ncombined_product = list(itertools.product(*product_spaces))  \nkeys = [list(space.keys()) for space in hyperparameter_space]  \nhyperparameter_combinations = []\nfor product_combination in combined_product:\ncombined_dicts = []\nfor key_set, value_set in zip(keys, product_combination, strict=False):\ncombined_dicts.append(dict(zip(key_set, value_set, strict=False)))\nhyperparameter_combinations.append(combined_dicts)  \nreturn algorithm_tuning(\n[\nModel._construct_model(config, index=0, is_flat=True)[0]\nfor config in hyperparameter_combinations\n],\nmetric_to_optimize,\ninput,\nlabels,\n)  \ndef algorithm_tuning(\nmodels_to_test: list[Model],\nmetric_to_optimize: str,\ninput: LocalInputs,\nlabels: LocalLabels,\n) -> list[Tuple[Model, float]]:\n\"\"\"\nTest a list of models to find the best model for the given metric.  \nArgs:\nmodels_to_test: List[turboml.Model]\nList of models to be tested.\nmetric_to_optimize: str\nThe performance metric to be used to find the best model.\ninput: Inputs\nThe input configuration for the models\nlabels: Labels\nThe label configuration for the models  \nReturns:\nlist[Tuple[Model, float]]: The list of all models with their corresponding scores sorted in descending order.\n\"\"\"\nfrom sklearn import metrics\nimport pandas as pd  \nif metric_to_optimize not in metrics.get_scorer_names():\nraise Exception(f\"{metric_to_optimize} is not yet supported.\")\nif not models_to_test:\nraise Exception(\"No models specified for testing.\")  \nprediction_column = (\n\"score\" if metric_to_optimize in SCORE_METRICS else \"predicted_class\"\n)  \nperf_metric = metrics.get_scorer(metric_to_optimize)\nassert isinstance(\nperf_metric, metrics._scorer._Scorer\n), f\"Invalid metric {metric_to_optimize}\"  \ninput_df, label_df = _resolve_duplicate_columns(\ninput.dataframe, labels.dataframe, input.key_field\n)\nmerged_df = pd.merge(input_df, label_df, on=input.key_field)  \ninput_spec = InputSpec(\nkey_field=input.key_field,\ntime_field=input.time_field or \"\",\nnumerical_fields=input.numerical_fields or [],\ncategorical_fields=input.categorical_fields or [],\ntextual_fields=input.textual_fields or [],\nimaginal_fields=input.imaginal_fields or [],\nlabel_field=labels.label_field,\n)  \ninput_table = TbPyArrow.df_to_table(merged_df, input_spec)\nresults = []\nfor model in models_to_test:\ntrained_model, score = get_score_for_model(\nmodel, input_table, input_spec, labels, perf_metric, prediction_column\n)\nshow_model_results(trained_model, score, metric_to_optimize)\nresults.append((trained_model, score))  \nreturn sorted(results, key=lambda x: x[1], reverse=True)  \ndef show_model_results(trained_model, score, metric_name):\n\"\"\"Displays formatted information for a trained model and its performance score.\"\"\"\nmodel_name = trained_model.__class__.__name__\nmodel_params = {\nk: v\nfor k, v in trained_model.__dict__.items()\nif not k.startswith(\"_\") and not callable(v)\n}  \nparams_display = \"\\n\".join(f\"  - {k}: {v}\" for k, v in model_params.items())  \nprint(f\"\\nModel: {model_name}\")\nprint(\"Parameters:\")\nprint(params_display)\nprint(f\"{metric_name.capitalize()} Score: {score:.5f}\\n\")"
    },
    {
        "section": "api.py",
        "content": "# api.py\n-## Location -> root_directory.common\n```python\nfrom .env import CONFIG\n\nimport os\nimport logging\nfrom typing import Optional\nimport time\nimport requests\n\nimport jwt\nimport tenacity\nimport uuid\n\n\n# Refresh access token if it is about to expire in 1 hour\nTOKEN_EXPIRY_THRESHOLD = 3600\n\nlogger = logging.getLogger(__name__)\n\n\nclass ApiException(Exception):\ndef __init__(self, message: str, status_code: int):\nself.message = message\nself.status_code = status_code\nsuper().__init__(message)\n\ndef __repr__(self):\nreturn f\"{self.__class__.__name__}(message={self.message}, status_code={self.status_code})\"\n\ndef __str__(self):\nreturn self.__repr__()\n\n\nclass NotFoundException(ApiException):\ndef __init__(self, message: str):\nsuper().__init__(message, 404)\n\n\nclass Api:\ndef __init__(self):\nself.session = requests.Session()\nself._api_key: Optional[str] = None\nself._access_token: Optional[str] = None\nself._namespace: Optional[str] = None\nif api_key := os.getenv(\"TURBOML_API_KEY\"):\nself._api_key = api_key\nif namespace := os.getenv(\"TURBOML_ACTIVE_NAMESPACE\"):\nself._namespace = namespace\nlogger.debug(\nf\"Namespace set to '{namespace}' from environment variable 'TURBOML_ACTIVE_NAMESPACE'\"\n)\nelse:\nlogger.debug(\n\"No namespace set; 'TURBOML_ACTIVE_NAMESPACE' environment variable not found.\"\n)\n\ndef clear_session(self):\nself._api_key = None\nself._access_token = None\n\ndef login(\nself,\napi_key: Optional[str] = None,\nusername: Optional[str] = None,\npassword: Optional[str] = None,\n):\nif api_key:\nself._api_key = api_key\nresp = self.session.get(\nurl=f\"{self.API_BASE_ADDRESS}/user\",\nheaders=self.headers,\n)\nif resp.status_code != 200:\nself._api_key = None\nraise ApiException(\"Invalid API key\", status_code=resp.status_code)\nreturn\nif username:\nassert password, \"Provide a password along with username\"\nresp = self.session.post(\nurl=f\"{self.API_BASE_ADDRESS}/login\",\ndata={\"username\": username, \"password\": password},\n)\nif resp.status_code != 200:\nraise ApiException(\n\"Invalid username/password\", status_code=resp.status_code\n)\nself._access_token = resp.json()[\"access_token\"]\nreturn\nraise ValueError(\"Provide either an API key or username/password\")\n\ndef _refresh_access_token_if_about_to_expire(self) -> None:\nassert self._access_token, \"No access token found\"\ndecoded_jwt = jwt.decode(\nself._access_token,\nalgorithms=[\"HS256\"],\noptions={\"verify_signature\": False},\n)\ntoken_expiry = decoded_jwt.get(\"exp\")\nif token_expiry - time.time() < TOKEN_EXPIRY_THRESHOLD:\nresp = self.session.post(\nurl=f\"{self.API_BASE_ADDRESS}/renew_token\",\nheaders={\"Authorization\": f\"Bearer {self._access_token}\"},\n)\nif resp.status_code != 200:\nraise ApiException(\n\"Failed to refresh access token try to login in again using login()\",\nstatus_code=resp.status_code,\n)\nself._access_token = resp.json()[\"access_token\"]\n\n@property\ndef API_BASE_ADDRESS(self) -> str:\nreturn CONFIG.TURBOML_BACKEND_SERVER_ADDRESS + \"/api\"\n\n@property\ndef headers(self) -> dict[str, str]:\nheaders = {}\nif self._namespace:\nheaders[\"X-Turboml-Namespace\"] = self._namespace\nif self._api_key:\nheaders[\"Authorization\"] = f\"apiKey {self._api_key}\"\nreturn headers\nif self._access_token:\nself._refresh_access_token_if_about_to_expire()\nheaders[\"Authorization\"] = f\"Bearer {self._access_token}\"\nreturn headers\nraise ValueError(\"No API key or access token found. Please login first\")\n\ndef set_active_namespace(self, namespace: str):\noriginal_namespace = self._namespace\nself._namespace = namespace\nresp = self.get(\"user/namespace\")\nif resp.status_code not in range(200, 300):\nself._namespace = original_namespace\nraise Exception(f\"Failed to set namespace: {resp.json()['detail']}\")\n\n@property\ndef arrow_headers(self) -> list[tuple[bytes, bytes]]:\nreturn [(k.lower().encode(), v.encode()) for k, v in self.headers.items()]\n\n@property\ndef namespace(self) -> str:\nreturn self.get(\"user/namespace\").json()\n\n@tenacity.retry(\nwait=tenacity.wait_exponential(multiplier=1, min=2, max=5),\nstop=tenacity.stop_after_attempt(3),\nreraise=True,\n)\ndef _request(self, method, url, headers, params, data, json, files):\nresp = requests.request(\nmethod=method,\nurl=url,\nheaders=headers,\nparams=params,\ndata=data,\njson=json,\nfiles=files,\n)\nif resp.status_code == 502:  # Catch and retry on Bad Gateway\nraise Exception(\"Bad Gateway\")\nreturn resp\n\ndef request(\nself,\nmethod,\nendpoint,\nhost=None,\ndata=None,\nparams=None,\njson=None,\nfiles=None,\nheaders=None,\nexclude_namespace=False,\n):\nif not host:\nhost = self.API_BASE_ADDRESS\ncombined_headers = self.headers.copy()\nif headers:\ncombined_headers.update(headers)\n# Exclude the namespace header if requested\nif exclude_namespace:\ncombined_headers.pop(\"X-Turboml-Namespace\", None)\n\nidempotency_key = uuid.uuid4().hex\ncombined_headers[\"Idempotency-Key\"] = idempotency_key\n\nresp = self._request(\nmethod=method.upper(),\nurl=f\"{host}/{endpoint}\",\nheaders=combined_headers,\nparams=params,\ndata=data,\njson=json,\nfiles=files,\n)\nif not (200 <= resp.status_code < 300):\ntry:\njson_resp = resp.json()\nerror_details = json_resp.get(\"detail\", json_resp)\nexcept ValueError:\nerror_details = resp.text\nif resp.status_code == 404:\nraise NotFoundException(error_details)\nraise ApiException(\nerror_details,\nstatus_code=resp.status_code,\n) from None\nreturn resp\n\ndef get(self, endpoint, **kwargs):\nreturn self.request(\"GET\", endpoint, **kwargs)\n\ndef options(self, endpoint, **kwargs):\nreturn self.request(\"OPTIONS\", endpoint, **kwargs)\n\ndef head(self, endpoint, **kwargs):\nreturn self.request(\"HEAD\", endpoint, **kwargs)\n\ndef post(self, endpoint, **kwargs):\nreturn self.request(\"POST\", endpoint, **kwargs)\n\ndef put(self, endpoint, **kwargs):\nreturn self.request(\"PUT\", endpoint, **kwargs)\n\ndef patch(self, endpoint, **kwargs):\nreturn self.request(\"PATCH\", endpoint, **kwargs)\n\ndef delete(self, endpoint, **kwargs):\nreturn self.request(\"DELETE\", endpoint, **kwargs)\n\n\napi = Api()\n\n```"
    },
    {
        "section": "concurrent.py",
        "content": "# concurrent.py\n-## Location -> root_directory.common\n```python\nfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n\n_tb_use_multiprocessing = True\n\n\ndef use_multiprocessing(enable: bool):\nglobal _tb_use_multiprocessing\n_tb_use_multiprocessing = enable\n\n\ndef multiprocessing_enabled() -> bool:\nglobal _tb_use_multiprocessing\nreturn _tb_use_multiprocessing\n\n\ndef get_executor_pool_class() -> type[ProcessPoolExecutor | ThreadPoolExecutor]:\nreturn ProcessPoolExecutor if multiprocessing_enabled() else ThreadPoolExecutor\n\n```"
    },
    {
        "section": "dataloader.py",
        "content": "# dataloader.py\n-## Location -> root_directory.common\n```python\nfrom __future__ import annotations\nfrom datetime import datetime\nfrom enum import StrEnum\nimport hashlib\nfrom pickle import PickleError\nimport sys\nimport struct\nfrom functools import partial\nfrom typing import (\nCallable,\nList,\nOptional,\nTypedDict,\nTYPE_CHECKING,\n)\nimport tempfile\nimport os\nimport importlib.util\nfrom concurrent.futures import Future, ProcessPoolExecutor, ThreadPoolExecutor\nfrom subprocess import run\nimport logging\n\nfrom tqdm import tqdm\nimport pandas as pd\nfrom google.protobuf.message import Message\nimport pyarrow\nimport pyarrow.flight\nfrom pyarrow.flight import FlightDescriptor\nfrom turboml.common.concurrent import multiprocessing_enabled, get_executor_pool_class\nfrom .api import api\nfrom .env import CONFIG\nfrom .internal import TbItertools, TbPyArrow\n\nif TYPE_CHECKING:\nfrom types import ModuleType\nfrom .models import (\nRegisteredSchema,\n)\nfrom google.protobuf import message\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass StreamType(StrEnum):\nINPUT_TOPIC = \"input_topic\"\nOUTPUT = \"output\"\nTARGET_DRIFT = \"target_drift\"\nUNIVARIATE_DRIFT = \"univariate_drift\"\nMULTIVARIATE_DRIFT = \"multivariate_drift\"\n\n\nclass Record(TypedDict):\noffset: int\nrecord: bytes\n\n\ndef _get_raw_msgs(dataset_type: StreamType, name: str, **kwargs):\n\"\"\"\nReturns a dataframe of type [offset: int, record: bytes] for the dataset\n\"\"\"\nif dataset_type == StreamType.UNIVARIATE_DRIFT:\nnumeric_feature = kwargs.get(\"numeric_feature\")\nif numeric_feature is None:\nraise ValueError(\"numeric_feature is required for univariate drift\")\nname = f\"{name}:{numeric_feature}\"\nif dataset_type == StreamType.MULTIVARIATE_DRIFT:\nlabel = kwargs.get(\"label\")\nif label is None:\nraise ValueError(\"label is required for multivariate drift\")\nname = f\"{name}:{label}\"\narrow_descriptor = pyarrow.flight.Ticket(f\"{dataset_type.value}:{name}\")\nclient = pyarrow.flight.connect(f\"{CONFIG.ARROW_SERVER_ADDRESS}\")\nTbPyArrow.wait_for_available(client)\nreader = client.do_get(\narrow_descriptor,\noptions=pyarrow.flight.FlightCallOptions(headers=api.arrow_headers),\n)\nLOG_FREQUENCY_SEC = 3\nlast_log_time = 0\nyielded_total = 0\nyielded_batches = 0\nstart_time = datetime.now().timestamp()\nwhile True:\ntable = reader.read_chunk().data\ndf = TbPyArrow.arrow_table_to_pandas(table)\nif df.empty:\nlogger.info(\nf\"Yielded {yielded_total} records ({yielded_batches} batches) in {datetime.now().timestamp() - start_time:.0f} seconds\"\n)\nbreak\nyielded_total += len(df)\nyielded_batches += 1\nif (now := datetime.now().timestamp()) - last_log_time > LOG_FREQUENCY_SEC:\nlogger.info(\nf\"Yielded {yielded_total} records ({yielded_batches} batches) in {now - start_time:.0f} seconds\"\n)\nlast_log_time = now\nassert isinstance(df, pd.DataFrame)\nyield df\n\n\nPROTO_PREFIX_BYTE_LEN = 6\n\n\ndef _records_to_proto_messages(\ndf: pd.DataFrame,\nproto_msg: Callable[[], message.Message],\n) -> tuple[list[int], list[message.Message]]:\noffsets = []\nproto_records = []\nfor _, offset_message in df.iterrows():\noffset, message = offset_message[\"offset\"], offset_message[\"record\"]\nassert isinstance(message, bytes)\nproto = proto_msg()\nproto.ParseFromString(message[PROTO_PREFIX_BYTE_LEN:])\noffsets.append(offset)\nproto_records.append(proto)\nreturn offsets, proto_records\n\n\nclass RecordList(TypedDict):\noffsets: list[int]\nrecords: list[message.Message]\n\n\n# HACK: Since it is observed that the ProcessPoolExecutor fails to pickle proto messages under\n# certain (not yet understood) conditions, we switch to the ThreadPoolExecutor upon encountering\n# such an error.\n# Ref: https://turboml.slack.com/archives/C07FM09V0MA/p1729082597265189\n\n\ndef get_proto_msgs(\ndataset_type: StreamType,\nname: str,\nproto_msg: Callable[[], message.Message],\n**kwargs,\n# limit: int = -1\n) -> list[Record]:\nexecutor_pool_class = get_executor_pool_class()\ntry:\nreturn _get_proto_msgs(\ndataset_type, name, proto_msg, executor_pool_class, **kwargs\n)\nexcept PickleError as e:\nif not multiprocessing_enabled():\nraise e\nlogger.warning(\nf\"Failed to pickle proto message class {proto_msg}: {e!r}. Retrying with ThreadPoolExecutor\"\n)\n\nreturn _get_proto_msgs(\ndataset_type, name, proto_msg, ThreadPoolExecutor, **kwargs\n)\n\n\ndef _get_proto_msgs(\ndataset_type: StreamType,\nname: str,\nproto_msg: Callable[[], message.Message],\nexecutor_cls: type[ProcessPoolExecutor | ThreadPoolExecutor],\n**kwargs,\n) -> list[Record]:\nmessages_generator = _get_raw_msgs(dataset_type, name, **kwargs)\noffsets = []\nrecords = []\nwith executor_cls(max_workers=os.cpu_count()) as executor:\nfutures: list[Future[tuple[list[int], list[message.Message]]]] = []\nfor df in messages_generator:\nfuture = executor.submit(\n_records_to_proto_messages,\ndf,\nproto_msg,\n)\nfutures.append(future)\nfor future in futures:\noffsets_chunk, records_chunk = future.result()\noffsets.extend(offsets_chunk)\nrecords.extend(records_chunk)\n\nret = []\nfor i, record in zip(offsets, records, strict=True):\nret.append({\"offset\": i, \"record\": record})\nreturn ret\n\n\ndef create_protobuf_from_row_tuple(\nrow: tuple,\nfields: List[str],\nproto_cls: Callable[[], message.Message],\nprefix: bytes,\n):\n\"\"\"Create a Protocol Buffers (protobuf) message by populating its fields with values from a tuple of row data.\n\nArgs:\nrow (Iterable): An iterable representing a row of data. Each element corresponds to a field in the protobuf message.\nfields (List[str]): A list of field names corresponding to the fields in the protobuf message class.\nproto_cls (type): The protobuf message class to instantiate.\nprefix (str): A string prefix to be concatenated with the serialized message.\n\nReturns:\nstr: A string representing the serialized protobuf message with the specified prefix.\n\"\"\"\nimport pandas as pd\n\nmy_msg = proto_cls()\nfor i, field in enumerate(fields):\nvalue = row[i]\n\nif pd.isna(value):\n# Leave the field unset if the value is NaN\ncontinue\n\ntry:\nsetattr(my_msg, field, value)\nexcept TypeError as e:\nlogger.error(\nf\"Error setting field '{field}' with value '{value}' in '{row}': {e!r}\"\n)\nraise\n\nreturn prefix + my_msg.SerializeToString()\n\n\ndef create_protobuf_from_row_dict(\nrow: dict,\nproto_cls: type[message.Message],\nprefix: bytes,\n):\n\"\"\"Create a Protocol Buffers (protobuf) message by populating its fields with values a from dictionary row of data.\n\nArgs:\nrow (Iterable): An iterable representing a row of data. Each element corresponds to a field in the protobuf message.\nproto_cls (type): The protobuf message class to instantiate.\nprefix (str): A string prefix to be concatenated with the serialized message.\n\nReturns:\nstr: A string representing the serialized protobuf message with the specified prefix.\n\"\"\"\nmy_msg = proto_cls()\nfor field, value in row.items():\nif value is None:\ncontinue  # skip None values -- protobuf isn't happy with them\ntry:\nsetattr(my_msg, field, value)\nexcept (TypeError, ValueError) as e:\nraise ValueError(f\"Error setting field '{field}'='{value}': {e!r}\") from e\n\nreturn prefix + my_msg.SerializeToString()\n\n\ndef _get_message_cls_from_pb_module(\nmodule: ModuleType, message_name: str | None\n) -> type[Message] | None:\nmessageClasses = [\nv\nfor v in vars(module).values()\nif isinstance(v, type) and issubclass(v, Message)\n]\nif len(messageClasses) == 0:\nlogger.error(\nf\"No message classes found in protobuf module composed of classes: {list(vars(module).keys())}\"\n)\nreturn None\n\nif message_name is None:\nreturn messageClasses[0] if len(messageClasses) > 0 else None\n\nmatching_class = [v for v in messageClasses if v.DESCRIPTOR.name == message_name]\nif len(matching_class) == 0:\nall_message_names = [v.DESCRIPTOR.name for v in messageClasses]\nlogger.error(\nf\"Could not find message class '{message_name}' in protobuf module composed of classes: {all_message_names}\"\n)\nreturn None\nreturn matching_class[0]\n\n\ndef _canonicalize_schema_body(schema_body: str) -> str:\n\"Schema registry formats does itd own canonicalization, but we need to do it for comparison\"\nreturn \"\\n\".join(\nline.strip()  # Remove leading/trailing whitespace\nfor line in schema_body.split(\"\\n\")\nif (\nnot line.strip().startswith(\"//\")  # Remove comments\nand not line.strip() == \"\"\n)  # Remove empty lines\n)\n\n\ndef get_protobuf_class(\nschema: str, message_name: str | None, retry: bool = True\n) -> type[Message] | None:\n\"\"\"\nGenerate a python class from a Protocol Buffers (protobuf) schema and message name.\nIf class_name is None, the first class in the schema is returned.\nIf a matching class is not found, None is returned.\n\"\"\"\nschema = _canonicalize_schema_body(schema)\nbasename = f\"p_{hashlib.md5(schema.encode()).hexdigest()[:8]}\"\nmodule_name = f\"{basename}_pb2\"\n\nif module_name in sys.modules:\nmodule = sys.modules[module_name]\nreturn _get_message_cls_from_pb_module(module, message_name)\n\nwith tempfile.TemporaryDirectory(prefix=\"turboml_\") as tempdir:\nfilename = os.path.join(tempdir, f\"{basename}.proto\")\nwith open(filename, \"w\") as f:\n_ = f.write(schema)\ndirname = os.path.dirname(filename)\nbasename = os.path.basename(filename)\nrun(\n[\n\"protoc\",\nf\"--python_out={dirname}\",\nf\"--proto_path={dirname}\",\nbasename,\n],\ncheck=True,\n)\nmodule_path = os.path.join(dirname, module_name + \".py\")\nmodule_spec = importlib.util.spec_from_file_location(module_name, module_path)\nassert module_spec is not None\nassert module_spec.loader is not None\nmodule = importlib.util.module_from_spec(module_spec)\nmodule_spec.loader.exec_module(module)\nif _get_message_cls_from_pb_module(module, None) is None:\n# Retry once if the module is empty\n# This is a rather bizarre behavior that seems to occur\n# in our CI, so we retry once to see if it resolves itself\nif not retry:\nreturn None\nlogger.error(\nf\"A seemingly empty protobuf module was generated from module_path='{module_path}', schema={schema}. Retrying once...\"\n)\nreturn get_protobuf_class(schema, message_name, retry=False)\n\nsys.modules[module_name] = module\nreturn _get_message_cls_from_pb_module(module, message_name)\n\n\ndef upload_df(\ndataset_id: str,\ndf: pd.DataFrame,\nschema: RegisteredSchema,\nprotoMessageClass: Optional[type[message.Message]] = None,\n) -> None:\n\"\"\"Upload data from a DataFrame to a dataset after preparing and serializing it as Protocol Buffers (protobuf) messages.\n\nArgs:\ndataset_id (str): The Kafka dataset_id to which the data will be sent.\ndf (pd.DataFrame): The DataFrame containing the data to be uploaded.\nschema (Schema): Dataset schema.\nprotoMessageClass (Optional(Message)): Protobuf Message Class to use. Generated if not provided.\n\"\"\"\n# dataset = api.get(f\"dataset?dataset_id={dataset_id}\").json()\n# dataset = Dataset(**dataset)\nif protoMessageClass is None:\nprotoMessageClass = get_protobuf_class(\nschema=schema.schema_body, message_name=schema.message_name\n)\nif protoMessageClass is None:\nraise ValueError(\nf\"Could not find protobuf message class message={schema.message_name} schema={schema.schema_body}\"\n)\n\nfields = df.columns.tolist()\nprefix = struct.pack(\"!xIx\", schema.id)\ndescriptor = FlightDescriptor.for_command(f\"produce:{dataset_id}\")\npa_schema = pyarrow.schema([(\"value\", pyarrow.binary())])\n\npartial_converter_func = partial(\ncreate_protobuf_from_row_tuple,\nfields=fields,\nproto_cls=protoMessageClass,\nprefix=prefix,\n)\n\nlogger.info(f\"Uploading {df.shape[0]} rows to dataset {dataset_id}\")\nexecutor_pool_class = get_executor_pool_class()\n\nclient = pyarrow.flight.connect(f\"{CONFIG.ARROW_SERVER_ADDRESS}\")\nTbPyArrow.wait_for_available(client)\nwriter, _ = client.do_put(\ndescriptor,\npa_schema,\noptions=pyarrow.flight.FlightCallOptions(headers=api.arrow_headers),\n)\ntry:\n_upload_df_batch(df, executor_pool_class, partial_converter_func, writer)\nexcept (PickleError, ModuleNotFoundError) as e:\nif not multiprocessing_enabled():\nraise e\nlogger.warning(\nf\"Dataframe batch update failed due to exception {e!r}. Retrying with ThreadPoolExecutor\"\n)\n_upload_df_batch(df, ThreadPoolExecutor, partial_converter_func, writer)\n\nlogger.info(\"Upload complete. Waiting for server to process messages.\")\nwriter.close()\n\n\ndef _upload_df_batch(\ndf: pd.DataFrame,\nexecutor_pool_class: type[ProcessPoolExecutor | ThreadPoolExecutor],\npartial_func,\nwriter,\n):\nwith executor_pool_class(max_workers=os.cpu_count()) as executor:\ndata_iterator = executor.map(\npartial_func,\ndf.itertuples(index=False, name=None),\nchunksize=1024,\n)\n\nCHUNK_SIZE = 1024\nrow_length = df.shape[0]\nwith tqdm(\ntotal=row_length, desc=\"Progress\", unit=\"rows\", unit_scale=True\n) as pbar:\nfor messages in TbItertools.chunked(data_iterator, CHUNK_SIZE):\nbatch = pyarrow.RecordBatch.from_arrays([messages], [\"value\"])\nwriter.write(batch)\npbar.update(len(messages))\n\n```"
    },
    {
        "section": "datasets.py",
        "content": "# datasets.py\n-## Location -> root_directory.common\n```python\nfrom __future__ import annotations\nfrom collections import Counter\nfrom dataclasses import dataclass\nimport inspect\nimport logging\nimport re\nimport time\nfrom typing import TYPE_CHECKING, Callable, Final, Generic, TypeVar\n\nimport ibis\nfrom pydantic import (\nBaseModel,\nConfigDict,\nmodel_validator,\n)\n\nfrom turboml.common import dataloader\nfrom turboml.common.api import ApiException, api, NotFoundException\nfrom turboml.common.feature_engineering import (\nFeatureEngineering,\nLocalFeatureEngineering,\nget_features,\n)\nfrom turboml.common.internal import TbPandas\nfrom turboml.common.protos import output_pb2\n\nfrom .models import (\nDataDrift,\nDataset,\nDatasetRegistrationRequest,\nDatasetRegistrationResponse,\nDatatype,\nDatasetField,\nRegisteredSchema,\nTurboMLResourceIdentifier,\nDatasetSchema,\n)  # noqa TCH0001\nimport pandas as pd\n\nif TYPE_CHECKING:\nfrom google.protobuf import message\n\nDATATYPE_NUMERICAL = Datatype.FLOAT\nDATATYPE_CATEGORICAL = Datatype.INT64\nDATATYPE_LABEL = Datatype.FLOAT\nDATATYPE_KEY = Datatype.STRING\nDATATYPE_IMAGE = Datatype.BYTES\nDATATYPE_TEXT = Datatype.STRING\nDATATYPE_TIMETICK = Datatype.INT64\n\nlogger = logging.getLogger(\"turboml.datasets\")\n\n\nclass LocalInputs(BaseModel):\nmodel_config = ConfigDict(arbitrary_types_allowed=True)\n\ndataframe: pd.DataFrame\nkey_field: DatasetField\ntime_field: DatasetField | None\nnumerical_fields: list[DatasetField]\ncategorical_fields: list[DatasetField]\ntextual_fields: list[DatasetField]\nimaginal_fields: list[DatasetField]\n\n@dataclass\nclass _FieldMeta:\nname: str\n_type: str\nwanted_dtype: Datatype\n\ndef all_fields_meta(self):\nreturn (\n[LocalInputs._FieldMeta(self.key_field, \"key\", DATATYPE_KEY)]\n+ [\nLocalInputs._FieldMeta(field, \"numerical\", DATATYPE_NUMERICAL)\nfor field in self.numerical_fields\n]\n+ [\nLocalInputs._FieldMeta(field, \"categorical\", DATATYPE_CATEGORICAL)\nfor field in self.categorical_fields\n]\n+ [\nLocalInputs._FieldMeta(field, \"textual\", DATATYPE_TEXT)\nfor field in self.textual_fields\n]\n+ [\nLocalInputs._FieldMeta(field, \"imaginal\", DATATYPE_IMAGE)\nfor field in self.imaginal_fields\n]\n+ (\n[LocalInputs._FieldMeta(self.time_field, \"time\", DATATYPE_TIMETICK)]\nif self.time_field\nelse []\n)\n)\n\n@model_validator(mode=\"after\")\ndef select_fields(self):\nall_fields_meta = self.all_fields_meta()\n\nall_field_names = [field.name for field in all_fields_meta]\n\n# if a field is used in more than one place, we'll raise an error\nif len(all_field_names) != len(set(all_field_names)):\n# figure out duplicates\nduplicates = [\nfield for field, count in Counter(all_field_names).items() if count > 1\n]\nraise ValueError(f\"Fields {duplicates} are specified more than once.\")\n\nabsent_fields = set(all_field_names) - set(self.dataframe.columns)\nif absent_fields:\nraise ValueError(\nf\"Fields {absent_fields} are not present in the dataframe.\"\n)\n\ndf = pd.DataFrame()\nfor field_meta in all_fields_meta:\nname, type_, wanted_dtype = (\nfield_meta.name,\nfield_meta._type,\nfield_meta.wanted_dtype,\n)\ntry:\ncolumn = self.dataframe[name]\nassert isinstance(column, pd.Series)\ncolumn = TbPandas.fill_nans_with_default(column)\ncolumn = column.astype(wanted_dtype.to_pandas_dtype())\ndf[name] = column\nexcept Exception as e:\nraise ValueError(\nf\"Failed to convert {type_} field '{name}' to {wanted_dtype}. \"\nf\"Error from pandas.astype(): {e!r}\"\n) from e\n\nself.dataframe = df\nreturn self\n\n@model_validator(mode=\"after\")\ndef _validate_time_field(self):\nif not self.time_field:\nreturn self\ntime_field_is_datetime64 = pd.api.types.is_datetime64_any_dtype(\nself.dataframe[self.time_field]\n)\nif not time_field_is_datetime64:\nraise ValueError(f\"Field '{self.time_field}' is not of a datetime type.\")\nreturn self\n\ndef validate_fields(self, dataframe: pd.DataFrame):\n## TODO: key field?\n\nfor field in self.numerical_fields:\nif not pd.api.types.is_numeric_dtype(dataframe[field]):\nraise ValueError(f\"Field '{field}' is not of a numeric type.\")\n\n# QUESTION: why is this commented out?\n# for field in self.categorical_fields:\n#    if not pd.api.types.is_categorical_dtype(dataframe[field]):\n#        raise ValueError(f\"Field '{field}' is not of categorical type.\")\n\nfor field in self.textual_fields:\nif not pd.api.types.is_string_dtype(dataframe[field]):\nraise ValueError(f\"Field '{field}' is not of a textual type.\")\n\n# QUESTION: why is this commented out?\n# for field in self.imaginal_fields:\n#     if not pd.api.types.is_string_dtype(dataframe[field]):\n#         raise ValueError(f\"Field '{field}' is not of a imaginal type.\")\n\n\n# NOTE: At most places where we were accepting `Inputs` previously, we should accept `LocalInputs | OnlineInputs`.\n# However for the moment I've kept it as `LocalInputs`, which includes `OnlineInputs` as well since we're\n# subclassing (for now) and basically load the entire dataset into memory by default.\n# At a later point we should change this so that its possible to pass streaming generators\n# from online datasets without loading everything into memory.\nclass OnlineInputs(LocalInputs):\ndataset_id: TurboMLResourceIdentifier\n\n\nclass LocalLabels(BaseModel):\nmodel_config = ConfigDict(arbitrary_types_allowed=True)\n\ndataframe: pd.DataFrame\nkey_field: DatasetField\nlabel_field: DatasetField\n\n@model_validator(mode=\"after\")\ndef validate_and_select_label_field(self):\nif self.label_field not in self.dataframe:\nraise ValueError(\nf\"Field '{self.label_field}' is not present in the dataframe.\"\n)\nlabel_field_is_numeric = pd.api.types.is_numeric_dtype(\nself.dataframe[self.label_field]\n)\nif not label_field_is_numeric:\nraise ValueError(f\"Field '{self.label_field}' is not of a numeric type.\")\ndf = pd.DataFrame()\ndf[self.label_field] = self.dataframe[self.label_field].astype(DATATYPE_LABEL)\ndf[self.key_field] = self.dataframe[self.key_field].astype(DATATYPE_KEY)\nself.dataframe = df\nreturn self\n\n\nclass OnlineLabels(LocalLabels):\ndataset_id: TurboMLResourceIdentifier\n\n\nFE = TypeVar(\"FE\", LocalFeatureEngineering, FeatureEngineering)\n\n\nclass _BaseInMemoryDataset(Generic[FE]):\n_init_key: Final[object] = object()\n\ndef __init__(\nself,\ninit_key: object,\nschema: DatasetSchema,\ndf: pd.DataFrame,\nkey_field: str,\nfeature_engineering: Callable[[pd.DataFrame], FE] = LocalFeatureEngineering,\n):\nif init_key not in [_BaseInMemoryDataset._init_key, OnlineDataset._init_key]:\nraise AssertionError(\nf\"Use from_* methods to instantiate {self.__class__.__name__}\"\n)\nself.schema = schema\nself.df = df  # The dataset, as it is\nself.key_field = key_field\nself.feature_engineering = feature_engineering(self.df.copy())\n\ndef __repr__(self):\nreturn f\"{self.__class__.__name__}({', '.join(f'{key}={value}' for key, value in vars(self).items())})\"\n\n@staticmethod\ndef from_schema(\nschema: DatasetSchema,\nkey_field: str,\nfeature_engineering: Callable[[pd.DataFrame], FE] = LocalFeatureEngineering,\n) -> _BaseInMemoryDataset[FE]:\nreturn _BaseInMemoryDataset(\n_BaseInMemoryDataset._init_key,\nschema,\npd.DataFrame(),\nkey_field,\nfeature_engineering,\n)\n\ndef __getitem__(self, item):\n\"\"\"\nReturns a new dataset that is a view of the original dataset.\n\"\"\"\nif not isinstance(item, slice):\nraise NotImplementedError(\"Only slicing is supported for now\")\n\ndf_view = self.df[item].copy()\nfe = self.feature_engineering\nassert isinstance(df_view, pd.DataFrame)\nassert isinstance(fe, LocalFeatureEngineering)\n\nreturn _BaseInMemoryDataset(\n_BaseInMemoryDataset._init_key,\nself.schema,\ndf_view,\nself.key_field,\nfeature_engineering=lambda df: fe.clone_with_df(df),\n)\n\ndef _is_pd_schema_compatible(self, df: pd.DataFrame) -> bool:\nif len(df) == 0:\nraise ValueError(\"Empty dataframe not allowed\")\nreturn DatasetSchema.from_pd(df) == self.schema\n\ndef add_pd(self, df: pd.DataFrame):\nif not self._is_pd_schema_compatible(df):\nraise ValueError(\n\"Schema mismatch: the dataframe does not match the dataset's input schema.\"\nf\" Expected: {self.schema}, got: {DatasetSchema.from_pd(df)}\"\n)\nself.df = pd.concat([self.df, df], ignore_index=True)\nself.feature_engineering._update_input_df(self.df.copy())\n\ndef get_model_inputs(\nself,\nnumerical_fields: list | None = None,\ncategorical_fields: list | None = None,\ntextual_fields: list | None = None,\nimaginal_fields: list | None = None,\ntime_field: str | None = None,\n):\n# Normalize\nif numerical_fields is None:\nnumerical_fields = []\nif categorical_fields is None:\ncategorical_fields = []\nif textual_fields is None:\ntextual_fields = []\nif imaginal_fields is None:\nimaginal_fields = []\n\nreturn LocalInputs(\ndataframe=self.feature_engineering.local_features_df,\nkey_field=self.key_field,\nnumerical_fields=numerical_fields,\ncategorical_fields=categorical_fields,\ntextual_fields=textual_fields,\nimaginal_fields=imaginal_fields,\ntime_field=time_field,\n)\n\ndef get_model_labels(self, label_field: str):\nreturn LocalLabels(\ndataframe=self.feature_engineering.local_features_df,\nkey_field=self.key_field,\nlabel_field=label_field,\n)\n\n\nclass LocalDataset(_BaseInMemoryDataset[LocalFeatureEngineering]):\n\"\"\"\nLocalDataset represents an in-memory dataset. In-memory datasets can\nbe used for local feature engineering experiments, and training local models.\nA LocalDataset can also be upgraded to an OnlineDataset for online feature\nengineering and serving models based on the same data.\n\"\"\"\n\ndef __getitem__(self, item):\ns = super().__getitem__(item)\nassert isinstance(s, _BaseInMemoryDataset)\nreturn LocalDataset(\nLocalDataset._init_key,\ns.schema,\ns.df,\ns.key_field,\nfeature_engineering=lambda _: s.feature_engineering,\n)\n\ndef __len__(self):\nreturn len(self.df)\n\n@staticmethod\ndef from_pd(\ndf: pd.DataFrame,\nkey_field: str,\n) -> LocalDataset:\nif len(df) == 0:\nraise ValueError(\"Empty dataframe\")\nschema = DatasetSchema.from_pd(df)\nreturn LocalDataset(LocalDataset._init_key, schema, df, key_field)\n\ndef to_online(self, id: str, load_if_exists: bool = False) -> OnlineDataset:\nreturn OnlineDataset.from_local_dataset(self, id, load_if_exists)\n\n\nclass _InMemoryDatasetOnlineFE(_BaseInMemoryDataset[FeatureEngineering]):\npass\n\n\nclass OnlineDataset:\n\"\"\"\nOnlineDataset represents a dataset managed and stored by the TurboML platform.\nIn addition to operations available on LocalDataset, an online dataset can be\nused to \"materialize\" engineered features, register and monitor drift, and\nserve models based on the data.\n\"\"\"\n\n_init_key = object()\n\ndef __init__(\nself,\ndataset_id: str,\ninit_key: object,\nkey_field: str,\nprotobuf_cls: type[message.Message],\nregistered_schema: RegisteredSchema,\nfe: LocalFeatureEngineering | None = None,\n):\nif init_key is not OnlineDataset._init_key:\nraise AssertionError(\nf\"Use load() or from_*() methods to instantiate {self.__class__.__name__}\"\n)\n\ndef feature_engineering(df: pd.DataFrame):\nif fe:\nreturn FeatureEngineering.inherit_from_local(fe, dataset_id)\nreturn FeatureEngineering(dataset_id, df)\n\nself.__local_dataset = _InMemoryDatasetOnlineFE.from_schema(\nregistered_schema.native_schema,\nkey_field=key_field,\nfeature_engineering=feature_engineering,\n)\n\nself.dataset_id = dataset_id\nself.protobuf_cls = protobuf_cls\nself.registered_schema = registered_schema\n\n@property\ndef schema(self):\nreturn self.__local_dataset.schema\n\n@property\ndef key_field(self):\nreturn self.__local_dataset.key_field\n\n@property\ndef feature_engineering(self):\nreturn self.__local_dataset.feature_engineering\n\n@property\ndef preview_df(self):\nreturn self.__local_dataset.df\n\ndef __repr__(self):\nreturn f\"OnlineDataset(id={self.dataset_id}, key_field={self.key_field}, schema={self.schema})\"\n\n@staticmethod\ndef load(dataset_id: str) -> OnlineDataset | None:\ntry:\ndataset = api.get(f\"dataset?dataset_id={dataset_id}\").json()\nexcept NotFoundException:\nreturn None\ndataset = Dataset(**dataset)\nschema = api.get(f\"dataset/{dataset_id}/schema\").json()\nschema = RegisteredSchema(**schema)\nprotobuf_cls = dataloader.get_protobuf_class(\nschema=schema.schema_body,\nmessage_name=dataset.meta.input_pb_message_name,\n)\nif protobuf_cls is None:\nraise ValueError(\nf\"Failed to load protobuf message class for message_name={dataset.message_name}, schema={schema.schema_body}\"\n)\nonline_dataset = OnlineDataset(\ndataset_id=dataset_id,\nkey_field=dataset.key,\ninit_key=OnlineDataset._init_key,\nprotobuf_cls=protobuf_cls,\nregistered_schema=schema,\n)\nonline_dataset.sync_features()\nreturn online_dataset\n\n@staticmethod\ndef _register_dataset(\ndataset_id: str, columns: dict[str, Datatype], key_field: str\n):\nregistration_request = DatasetRegistrationRequest(\ndataset_id=dataset_id,\ndata_schema=DatasetRegistrationRequest.ExplicitSchema(fields=columns),\nkey_field=key_field,\n)\ntry:\nresponse = api.post(\"dataset\", json=registration_request.model_dump())\nexcept ApiException as e:\nif \"already exists\" in str(e):\nraise ValueError(\nf\"Dataset with ID '{dataset_id}' already exists. Use OnlineDataset.load() to load it or specify a different ID.\"\n) from e\nraise\n\nreturn DatasetRegistrationResponse(**response.json())\n\n@staticmethod\ndef from_local_dataset(\ndataset: LocalDataset, dataset_id: str, load_if_exists: bool = False\n) -> OnlineDataset:\nif load_if_exists and (online_dataset := OnlineDataset.load(dataset_id)):\nif online_dataset.schema != dataset.schema:\nraise ValueError(\nf\"Dataset already exists with different schema: {online_dataset.schema} != {dataset.schema}\"\n)\nreturn online_dataset\ntry:\nresponse = OnlineDataset._register_dataset(\ndataset_id, dataset.schema.fields, dataset.key_field\n)\nexcept ApiException as e:\nraise Exception(f\"Failed to register dataset: {e!r}\") from e\n\nprotobuf_cls = dataloader.get_protobuf_class(\nschema=response.registered_schema.schema_body,\nmessage_name=response.registered_schema.message_name,\n)\nif protobuf_cls is None:\nraise ValueError(\nf\"Failed to load protobuf message class for message_name={response.registered_schema.message_name},\"\nf\" schema={response.registered_schema.schema_body}\"\n)\nonline_dataset = OnlineDataset(\ndataset_id=dataset_id,\nkey_field=dataset.key_field,\ninit_key=OnlineDataset._init_key,\nregistered_schema=response.registered_schema,\nprotobuf_cls=protobuf_cls,\n)\ntry:\nonline_dataset.add_pd(dataset.df)\nexcept Exception as e:\nraise ValueError(f\"Failed to push dataset: {e!r}\") from e\n## TODO: cleanup ops\nlogger.info(\nf\"Pushed dataset {online_dataset.dataset_id}. Note that any feature definitions will have to be materialized before they can be used with online models.\"\n)\nreturn online_dataset\n\ndef add_pd(self, df: pd.DataFrame):\nif not self.__local_dataset._is_pd_schema_compatible(df):\nraise ValueError(\n\"Schema mismatch: the dataframe does not match the dataset's input schema.\"\nf\" Expected: {self.schema}, got: {DatasetSchema.from_pd(df)}\"\n)\ntry:\ndataloader.upload_df(\nself.dataset_id, df, self.registered_schema, self.protobuf_cls\n)\nexcept Exception as e:\nraise ValueError(f\"Failed to upload data: {e!r}\") from e\n\n## TODO:\n# We really shouldn't maintain a local copy of the dataset\n# or its features. Instead we should support a way to iterate through the dataset\n# or derived featuresets in a streaming fashion, for example by using a generator\n# Still, we should make it so the preview_df is populated by the latest few thousand rows\nold_len = len(self.preview_df)\nwhile True:\nself.sync_features()\nif len(self.preview_df) > old_len:\nbreak\ntime.sleep(0.5)\n\ndef add_row_dict(self, row: dict):\nraise NotImplementedError  ## TODO: complete\n\n@staticmethod\ndef from_pd(\ndf: pd.DataFrame, id: str, key_field: str, load_if_exists: bool = False\n) -> OnlineDataset:\nif load_if_exists and (dataset := OnlineDataset.load(id)):\nreturn dataset\ndf_schema = DatasetSchema.from_pd(df)\nOnlineDataset._register_dataset(id, df_schema.fields, key_field)\ndataset = OnlineDataset.load(id)\nassert dataset is not None\ndataset.add_pd(df)\nreturn dataset\n\n# -- fn\n\ndef sync_features(self):\nfeatures_df = get_features(self.dataset_id)\ninput_df = features_df[list(self.schema.fields.keys())].copy()\nassert isinstance(input_df, pd.DataFrame)\nself.__local_dataset.df = input_df\nself.__local_dataset.feature_engineering._update_input_df(features_df)\n\ndef to_ibis(self):\n\"\"\"\nConverts the dataset into an Ibis table.\n\nReturns:\nibis.expr.types.Table: An Ibis in-memory table representing the features\nassociated with the given dataset_id.\n\nRaises:\nException: If any error occurs during the retrieval of the table name,\nfeatures, or conversion to Ibis table.\n\"\"\"\ntry:\ndf = get_features(self.dataset_id)\nreturn ibis.memtable(df, name=self.dataset_id)\nexcept Exception as e:\nraise e\n\ndef register_univariate_drift(self, numerical_field: str, label: str | None = None):\nif not numerical_field:\nraise Exception(\"Numerical field not specified\")\n\npayload = DataDrift(label=label, numerical_fields=[numerical_field])\napi.put(endpoint=f\"dataset/{self.dataset_id}/drift\", json=payload.model_dump())\n\ndef register_multivariate_drift(self, numerical_fields: list[str], label: str):\npayload = DataDrift(label=label, numerical_fields=numerical_fields)\napi.put(endpoint=f\"dataset/{self.dataset_id}/drift\", json=payload.model_dump())\n\ndef get_univariate_drift(\nself,\nlabel: str | None = None,\nnumerical_field: str | None = None,\nlimit: int = -1,\n):\nif numerical_field is None and label is None:\nraise Exception(\"Numerical field and label both cannot be None\")\n\nif numerical_field is not None and label is None:\nlabel = self._get_default_mv_drift_label([numerical_field])\n\nreturn dataloader.get_proto_msgs(\ndataloader.StreamType.UNIVARIATE_DRIFT,\nself.dataset_id,\noutput_pb2.Output,\nnumeric_feature=label,\n)\n\ndef get_multivariate_drift(\nself,\nlabel: str | None = None,\nnumerical_fields: list[str] | None = None,\nlimit: int = -1,\n):\nif numerical_fields is None and label is None:\nraise Exception(\"Numerical fields and label both cannot be None\")\n\nif numerical_fields is not None and label is None:\nlabel = self._get_default_mv_drift_label(numerical_fields)\n\nreturn dataloader.get_proto_msgs(\ndataloader.StreamType.MULTIVARIATE_DRIFT,\nself.dataset_id,\noutput_pb2.Output,\nlabel=label,\n)\n\ndef _get_default_mv_drift_label(self, numerical_fields: list[str]):\npayload = DataDrift(numerical_fields=numerical_fields, label=None)\n\ndrift_label = api.get(\nf\"dataset/{self.dataset_id}/drift_label\", json=payload.model_dump()\n).json()[\"label\"]\n\nreturn drift_label\n\ndef get_model_labels(self, label_field: str):\nlocal_labels = self.__local_dataset.get_model_labels(label_field)\nreturn OnlineLabels(\ndataset_id=self.dataset_id,\n**local_labels.model_dump(),\n)\n\ndef get_model_inputs(\nself,\nnumerical_fields: list[str] | None = None,\ncategorical_fields: list[str] | None = None,\ntextual_fields: list[str] | None = None,\nimaginal_fields: list[str] | None = None,\ntime_field: str | None = None,\n):\nlocal_inputs = self.__local_dataset.get_model_inputs(\nnumerical_fields=numerical_fields,\ncategorical_fields=categorical_fields,\ntextual_fields=textual_fields,\nimaginal_fields=imaginal_fields,\ntime_field=time_field,\n)\nreturn OnlineInputs(\ndataset_id=self.dataset_id,\n**local_inputs.model_dump(),\n)\n\n\nclass PandasHelpers:\n@staticmethod\ndef normalize_df(df: pd.DataFrame) -> pd.DataFrame:\n\"\"\"Normalize a dataframe by removing NaNs and replacing them with type-default values\"\"\"\nnorm_df = pd.DataFrame()\nfor cname in df.columns:\ncol = df[cname]\nassert isinstance(col, pd.Series)\nnorm_df[cname] = TbPandas.fill_nans_with_default(col)\nreturn norm_df\n\n\nDATA_BASE_URL = (\n\"https://raw.githubusercontent.com/TurboML-Inc/colab-notebooks/refs/heads/+data/\"\n)\n\n\nclass StandardDataset(LocalDataset):\n\"\"\"\nBase class for standard datasets used in our docs.\n\"\"\"\n\n@property\ndef description(self):\nassert self.__doc__ is not None, \"No docstring\"\ndesc = re.split(pattern=r\"\\w+\\n\\s{4}\\-{3,}\", string=self.__doc__, maxsplit=0)[0]\nreturn inspect.cleandoc(desc)\n\ndef __repr__(self):\nNEWLINE = \"\\n\"\nschema_k_v = (f\"{i[0]}: {i[1]}\" for i in self.schema.fields.items())\nreturn f\"\"\"{self.description}\n\n\nSamples  {len(self)}\nSchema   {(NEWLINE + 11 * \" \").join(schema_k_v)}\n\"\"\"\n\n\n## TODO: cache these datasets on disk (/tmp/turboml/datasets) to avoid downloading them\n# every time in CI etc\n\n\nclass FraudDetectionDatasetFeatures(StandardDataset):\n\"\"\"Fraud Detection - Features\n\nThe dataset contains a total of 200,000 fraudulent and non-fraudulent transactions\ndescribed by 22 features. The corresponding labels are available in the FraudDetectionDatasetLabels dataset.\n\"\"\"\n\ndef __init__(\nself,\n):\ntx_df = pd.read_csv(f\"{DATA_BASE_URL}/transactions.csv\")\nschema = DatasetSchema.from_pd(tx_df)\nsuper().__init__(self._init_key, schema, tx_df, \"transactionID\")\n\n\nclass FraudDetectionDatasetLabels(StandardDataset):\n\"\"\"Fraud Detection - Labels\n\nThe dataset contains a total of 200,000 fraudulent and non-fraudulent transactions.\nThe corresponding features are available in the FraudDetectionDatasetFeatures dataset.\n\"\"\"\n\ndef __init__(self):\nlabels_df = pd.read_csv(f\"{DATA_BASE_URL}/labels.csv\")\nschema = DatasetSchema.from_pd(labels_df)\nsuper().__init__(self._init_key, schema, labels_df, \"transactionID\")\n\n\n_credit_cards_df = None\n\n\ndef _load_credit_cards_dataset():\nglobal _credit_cards_df\nif _credit_cards_df is not None:\nreturn _credit_cards_df\n\ntry:\nfrom river import datasets\nexcept ImportError:\nraise ImportError(\n\"The river library is required to load the CreditCards dataset. \"\n\"Please install it using `pip install river`.\"\n) from None\ncc_feats = []\ncc_labels = []\nfor sample, score in datasets.CreditCard():\ncc_feats.append(sample)\ncc_labels.append({\"score\": score})\n\nfeats_df = pd.DataFrame(cc_feats).reset_index()\nlabels_df = pd.DataFrame(cc_labels).reset_index()\n_credit_cards_df = pd.merge(feats_df, labels_df, on=\"index\")\nreturn _credit_cards_df\n\n\nclass CreditCardsDatasetFeatures(StandardDataset):\n\"\"\"Credit card frauds - Features\n\nThe dataset contains labels for transactions made by credit cards in September 2013 by european\ncardholders. The dataset presents transactions that occurred in two days, where 492\nout of the 284,807 transactions are fraudulent. The dataset is highly unbalanced, the positive class\n(frauds) account for 0.172% of all transactions.\n\nThe corresponding labels are available in CreditCardsDatasetLabels.\n\nDataset source: River (https://riverml.xyz)\n\"\"\"\n\ndef __init__(self):\ntry:\ndf = _load_credit_cards_dataset()\nexcept ImportError as e:\nraise e\ndf = df.drop(columns=[\"score\"])\nschema = DatasetSchema.from_pd(df)\nsuper().__init__(self._init_key, schema, df, \"index\")\n\n\nclass CreditCardsDatasetLabels(StandardDataset):\n\"\"\"Credit card frauds - Labels\n\nThe dataset contains labels for transactions made by credit cards in September 2013 by european\ncardholders. The dataset presents transactions that occurred in two days, where 492\nout of the 284,807 transactions are fraudulent. The dataset is highly unbalanced, the positive class\n(frauds) account for 0.172% of all transactions.\n\nThe corresponding features are available in CreditCardsDatasetLabels.\n\nDataset source: River (https://riverml.xyz)\n\"\"\"\n\ndef __init__(self):\ntry:\ndf = _load_credit_cards_dataset()\nexcept ImportError as e:\nraise e\ndf = df[[\"index\", \"score\"]]\nassert isinstance(df, pd.DataFrame)\nschema = DatasetSchema.from_pd(df)\nsuper().__init__(self._init_key, schema, df, \"index\")\n\n```"
    },
    {
        "section": "default_model_configs.py",
        "content": "# default_model_configs.py\n-## Location -> root_directory.common\n```python\nfrom google.protobuf import json_format\nfrom frozendict import frozendict\nimport copy\n\nfrom turboml.common.protos import config_pb2\n\n\nclass _DefaultModelConfigs:\ndef __init__(self):\nself.default_configs = frozendict(\n{\n\"MStream\": config_pb2.MStreamConfig(\nnum_rows=2, num_buckets=1024, factor=0.8\n),\n\"RCF\": config_pb2.RCFConfig(\ntime_decay=0.000390625,\nnumber_of_trees=50,\noutput_after=64,\nsample_size=256,\n),\n\"HST\": config_pb2.HSTConfig(n_trees=20, height=12, window_size=50),\n\"HoeffdingTreeClassifier\": config_pb2.HoeffdingClassifierConfig(\ndelta=1e-7,\ntau=0.05,\ngrace_period=200,\nn_classes=2,\nleaf_pred_method=\"mc\",\nsplit_method=\"gini\",\n),\n\"HoeffdingTreeRegressor\": config_pb2.HoeffdingRegressorConfig(\ndelta=1e-7, tau=0.05, grace_period=200, leaf_pred_method=\"mean\"\n),\n\"AMFClassifier\": config_pb2.AMFClassifierConfig(\nn_classes=2,\nn_estimators=10,\nstep=1,\nuse_aggregation=True,\ndirichlet=0.5,\nsplit_pure=False,\n),\n\"AMFRegressor\": config_pb2.AMFRegressorConfig(\nn_estimators=10,\nstep=1,\nuse_aggregation=True,\ndirichlet=0.5,\n),\n\"FFMClassifier\": config_pb2.FFMClassifierConfig(\nn_factors=10,\nl1_weight=0,\nl2_weight=0,\nl1_latent=0,\nl2_latent=0,\nintercept=0,\nintercept_lr=0.01,\nclip_gradient=1e12,\n),\n\"FFMRegressor\": config_pb2.FFMRegressorConfig(\nn_factors=10,\nl1_weight=0,\nl2_weight=0,\nl1_latent=0,\nl2_latent=0,\nintercept=0,\nintercept_lr=0.01,\nclip_gradient=1e12,\n),\n\"SGTClassifier\": config_pb2.SGTClassifierConfig(\ndelta=1e-7,\ngamma=0.1,\ngrace_period=200,\n**{\"lambda\": 0.1},  # HACK: lambda is a reserved keyword in Python\n),\n\"SGTRegressor\": config_pb2.SGTRegressorConfig(\ndelta=1e-7,\ngamma=0.1,\ngrace_period=200,\n**{\"lambda\": 0.1},\n),\n\"SNARIMAX\": config_pb2.SNARIMAXConfig(\nhorizon=1, p=1, d=1, q=1, m=1, sp=0, sd=0, sq=0\n),\n\"ONNX\": config_pb2.ONNXConfig(model_save_name=\"\"),\n\"LeveragingBaggingClassifier\": config_pb2.LeveragingBaggingClassifierConfig(\nn_models=10,\nn_classes=2,\nw=6,\nbagging_method=\"bag\",\nseed=0,\n),\n\"HeteroLeveragingBaggingClassifier\": config_pb2.HeteroLeveragingBaggingClassifierConfig(\nn_classes=2,\nw=6,\nbagging_method=\"bag\",\nseed=0,\n),\n\"AdaBoostClassifier\": config_pb2.AdaBoostClassifierConfig(\nn_models=10,\nn_classes=2,\nseed=0,\n),\n\"HeteroAdaBoostClassifier\": config_pb2.HeteroAdaBoostClassifierConfig(\nn_classes=2,\nseed=0,\n),\n\"RandomSampler\": config_pb2.RandomSamplerConfig(\nn_classes=2,\ndesired_dist=[0.5, 0.5],\nsampling_method=\"mixed\",\nsampling_rate=1.0,\nseed=0,\n),\n\"Python\": config_pb2.PythonConfig(\nmodule_name=\"\", class_name=\"\", venv_name=\"\"\n),\n\"PythonEnsembleModel\": config_pb2.PythonEnsembleConfig(\nmodule_name=\"\",\nclass_name=\"\",\nvenv_name=\"\",\n),\n\"PreProcessor\": config_pb2.PreProcessorConfig(\npreprocessor_name=\"MinMax\",\n),\n\"NeuralNetwork\": config_pb2.NeuralNetworkConfig(\ndropout=0,\nlayers=[\nconfig_pb2.NeuralNetworkConfig.NeuralNetworkLayer(\noutput_size=64,\nactivation=\"relu\",\ndropout=0.3,\nresidual_connections=[],\nuse_bias=True,\n),\nconfig_pb2.NeuralNetworkConfig.NeuralNetworkLayer(\noutput_size=64,\nactivation=\"relu\",\ndropout=0.3,\nresidual_connections=[],\nuse_bias=True,\n),\nconfig_pb2.NeuralNetworkConfig.NeuralNetworkLayer(\noutput_size=1,\nactivation=\"sigmoid\",\ndropout=0.3,\nresidual_connections=[],\nuse_bias=True,\n),\n],\nloss_function=\"mse\",\nlearning_rate=1e-2,\noptimizer=\"sgd\",\nbatch_size=64,\n),\n\"ONN\": config_pb2.ONNConfig(\nmax_num_hidden_layers=10,\nqtd_neuron_hidden_layer=32,\nn_classes=2,\nb=0.99,\nn=0.01,\ns=0.2,\n),\n\"OVR\": config_pb2.OVRConfig(\nn_classes=2,\n),\n\"BanditModelSelection\": config_pb2.BanditModelSelectionConfig(\nbandit=\"EpsGreedy\",\nmetric_name=\"WindowedMAE\",\n),\n\"ContextualBanditModelSelection\": config_pb2.ContextualBanditModelSelectionConfig(\ncontextualbandit=\"LinTS\",\nmetric_name=\"WindowedMAE\",\n),\n\"RandomProjectionEmbedding\": config_pb2.RandomProjectionEmbeddingConfig(\nn_embeddings=2,\ntype_embedding=\"Gaussian\",\n),\n\"EmbeddingModel\": config_pb2.EmbeddingModelConfig(),\n\"MultinomialNB\": config_pb2.MultinomialConfig(n_classes=2, alpha=1.0),\n\"GaussianNB\": config_pb2.GaussianConfig(\nn_classes=2,\n),\n\"AdaptiveXGBoost\": config_pb2.AdaptiveXGBoostConfig(\nn_classes=2,\nlearning_rate=0.3,\nmax_depth=6,\nmax_window_size=1000,\nmin_window_size=0,\nmax_buffer=5,\npre_train=2,\ndetect_drift=True,\nuse_updater=True,\ntrees_per_train=1,\npercent_update_trees=1.0,\n),\n\"AdaptiveLGBM\": config_pb2.AdaptiveLGBMConfig(\nn_classes=2,\nlearning_rate=0.3,\nmax_depth=6,\nmax_window_size=1000,\nmin_window_size=0,\nmax_buffer=5,\npre_train=2,\ndetect_drift=True,\nuse_updater=True,\ntrees_per_train=1,\n),\n\"LLAMAEmbedding\": config_pb2.LLAMAEmbeddingModelConfig(),\n\"LlamaText\": config_pb2.LlamaTextConfig(),\n\"ClipEmbedding\": config_pb2.ClipEmbeddingConfig(),\n\"RestAPIClient\": config_pb2.RestAPIClientConfig(\nmax_retries=3,\nconnection_timeout=10,\nmax_request_time=30,\n),\n\"GRPCClient\": config_pb2.GRPCClientConfig(\nmax_retries=3,\nconnection_timeout=10000,\nmax_request_time=30000,\n),\n}\n)\nself.algo_config_mapping = frozendict(\n{\n\"MStream\": \"mstream_config\",\n\"RCF\": \"rcf_config\",\n\"HST\": \"hst_config\",\n\"HoeffdingTreeClassifier\": \"hoeffding_classifier_config\",\n\"HoeffdingTreeRegressor\": \"hoeffding_regressor_config\",\n\"AMFClassifier\": \"amf_classifier_config\",\n\"AMFRegressor\": \"amf_regressor_config\",\n\"FFMClassifier\": \"ffm_classifier_config\",\n\"SGTClassifier\": \"sgt_classifier_config\",\n\"SGTRegressor\": \"sgt_regressor_config\",\n\"FFMRegressor\": \"ffm_regressor_config\",\n\"SNARIMAX\": \"snarimax_config\",\n\"ONNX\": \"onnx_config\",\n\"LeveragingBaggingClassifier\": \"leveraging_bagging_classifier_config\",\n\"HeteroLeveragingBaggingClassifier\": \"hetero_leveraging_bagging_classifier_config\",\n\"AdaBoostClassifier\": \"adaboost_classifier_config\",\n\"HeteroAdaBoostClassifier\": \"hetero_adaboost_classifier_config\",\n\"RandomSampler\": \"random_sampler_config\",\n\"Python\": \"python_config\",\n\"PythonEnsembleModel\": \"python_ensemble_config\",\n\"PreProcessor\": \"preprocessor_config\",\n\"NeuralNetwork\": \"nn_config\",\n\"ONN\": \"onn_config\",\n\"OVR\": \"ovr_model_selection_config\",\n\"BanditModelSelection\": \"bandit_model_selection_config\",\n\"ContextualBanditModelSelection\": \"contextual_bandit_model_selection_config\",\n\"RandomProjectionEmbedding\": \"random_projection_config\",\n\"EmbeddingModel\": \"embedding_model_config\",\n\"MultinomialNB\": \"multinomial_config\",\n\"GaussianNB\": \"gaussian_config\",\n\"AdaptiveXGBoost\": \"adaptive_xgboost_config\",\n\"AdaptiveLGBM\": \"adaptive_lgbm_config\",\n\"LLAMAEmbedding\": \"llama_embedding_config\",\n\"LlamaText\": \"llama_text_config\",\n\"ClipEmbedding\": \"clip_embedding_config\",\n\"RestAPIClient\": \"rest_api_client_config\",\n\"GRPCClient\": \"grpc_client_config\",\n}\n)\n\ndef get_default_parameters(self):\nparameters = {}\nfor alg, config in self.default_configs.items():\nparameters[alg] = json_format.MessageToDict(config)\nreturn parameters\n\ndef fill_config(self, conf: config_pb2.ModelConfig, parameters):\nnew_config = json_format.ParseDict(\nparameters, copy.deepcopy(self.default_configs[conf.algorithm])\n)\ntry:\ngetattr(conf, self.algo_config_mapping[conf.algorithm]).CopyFrom(new_config)\nexcept Exception as e:\nraise Exception(f\"Failed to match config: {conf.algorithm}\") from e\nreturn conf\n\n\nDefaultModelConfigs = _DefaultModelConfigs()\n\n```"
    },
    {
        "section": "env.py",
        "content": "# env.py\n-## Location -> root_directory.common\n```python\nfrom __future__ import annotations\nfrom pydantic import Field  # noqa: TCH002\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\n\nclass _TurboMLConfig(BaseSettings):\nmodel_config = SettingsConfigDict(frozen=True)\n\nTURBOML_BACKEND_SERVER_ADDRESS: str = Field(default=\"http://localhost:8500\")\nFEATURE_SERVER_ADDRESS: str = Field(default=\"grpc+tcp://localhost:8552\")\nARROW_SERVER_ADDRESS: str = Field(default=\"grpc+tcp://localhost:8502\")\n\ndef set_backend_server(self, value: str):\nobject.__setattr__(self, \"TURBOML_BACKEND_SERVER_ADDRESS\", value)\n\ndef set_feature_server(self, value: str):\nobject.__setattr__(self, \"FEATURE_SERVER_ADDRESS\", value)\n\ndef set_arrow_server(self, value: str):\nobject.__setattr__(self, \"ARROW_SERVER_ADDRESS\", value)\n\n\n# Global config object\nCONFIG = _TurboMLConfig()  # type: ignore\n\n```"
    },
    {
        "section": "feature_engineering.py",
        "content": "# feature_engineering.py\n-## Location -> root_directory.common\n```python\nfrom __future__ import annotations\n\nimport inspect\nimport logging\nimport pickle\nfrom functools import reduce\nfrom textwrap import dedent\nimport time\nfrom typing import Any, Callable, Optional, TYPE_CHECKING, Union\nimport re\nimport sys\n\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport pyarrow as pa\nimport pandas as pd\nimport duckdb\nfrom pyarrow.flight import FlightClient, FlightCallOptions, FlightDescriptor, Ticket\nfrom ibis.backends.duckdb import Backend as DuckdbBackend\nfrom datafusion import udaf, Accumulator, SessionContext\nfrom tqdm import tqdm\nfrom turboml.common.internal import TbPyArrow\nimport cloudpickle\nimport base64\n\nif TYPE_CHECKING:\nimport ibis\nfrom ibis.expr.types.relations import Table as IbisTable\n\nfrom .util import risingwave_type_to_pyarrow, get_imports_used_in_function\nfrom .models import (\nAggregateFeatureSpec,\nBackEnd,\nDataset,\nIbisFeatureMaterializationRequest,\nFeatureMaterializationRequest,\nFetchFeatureRequest,\nSqlFeatureSpec,\nUdfFeatureSpec,\nUdfFunctionSpec,\nTimestampRealType,\nDuckDbVarcharType,\nRisingWaveVarcharType,\nTimestampQuery,\nUdafFunctionSpec,\nUdafFeatureSpec,\nIbisFeatureSpec,\nFeatureGroup,\nRwEmbeddedUdafFunctionSpec,\n)\nfrom .sources import (\nDataSource,\nFileSource,\nPostgresSource,\nFeatureGroupSource,\nS3Config,\n)\nfrom .api import api\nfrom .env import CONFIG\n\nlogger = logging.getLogger(\"turboml.feature_engineering\")\n\n\ndef get_timestamp_formats() -> list[str]:\n\"\"\"get the possible timestamp format strings\n\nReturns:\nlist[str]: list of format strings\n\"\"\"\nreturn [enum.value for enum in RisingWaveVarcharType] + [\nenum.name for enum in TimestampRealType\n]\n\n\ndef convert_timestamp(\ntimestamp_column_name: str, timestamp_type: str\n) -> tuple[str, str]:\n\"\"\"converts a timestamp string to a timestamp query for usage in db\n\nArgs:\ntimestamp_column_name (str): column name for the timestamp_query\ntimestamp_type (str): It must be one of real or varchar types\n\nRaises:\nException: If a valid timestamp type is not selected throws an exception\n\nReturns:\nstr: timestamp_query\n\"\"\"\nfor enum in RisingWaveVarcharType:\nif timestamp_type == enum.value:\nreturn (\nf\"to_timestamp({timestamp_column_name}, '{RisingWaveVarcharType[enum.name].value}')\",\nf\"try_strptime({timestamp_column_name}, '{DuckDbVarcharType[enum.name].value}')\",\n)\nif timestamp_type == \"epoch_seconds\":\nreturn (\nf\"to_timestamp({timestamp_column_name}::double)\",\nf\"to_timestamp({timestamp_column_name}::double)\",\n)\nif timestamp_type == \"epoch_milliseconds\":\nreturn (\nf\"to_timestamp({timestamp_column_name}::double/1000)\",\nf\"to_timestamp({timestamp_column_name}::double/1000)\",\n)\nraise Exception(\"Please select a valid option\")\n\n\ndef retrieve_features(dataset_id: str, df: pd.DataFrame) -> pd.DataFrame:\n\"\"\"\nRetrieve all materialized features for the given dataset and dataframe containing raw data as a dataframe.\n\nArgs:\ndataset_id (str): The dataset user wants to explore\ndf (pd.DataFrame): The dataframe of raw data\n\nReturns:\npandas.DataFrame: The dataframe of the dataset features\n\"\"\"\ntry:\narrow_table = pa.Table.from_pandas(df)\n\ndataset_name = dataset_id.encode(\"utf8\")\ndescriptor = FlightDescriptor.for_path(dataset_name)\noptions = FlightCallOptions(headers=api.arrow_headers, timeout=120)\nflight_client = FlightClient(CONFIG.FEATURE_SERVER_ADDRESS)\n\nfeatures_table = TbPyArrow._exchange_and_retry(\nflight_client, descriptor, options, arrow_table, max_chunksize=10000\n)\n\nreturn TbPyArrow.arrow_table_to_pandas(features_table)\n\nexcept Exception as e:\nraise Exception(\"An error occurred while fetching features\") from e\n\n\ndef get_features(\ndataset_id: str,\nlimit: int = -1,\nto_pandas_opts: dict | None = None,\n) -> pd.DataFrame:\n\"\"\"\nRetrieve all materialized features from the given dataset as a dataframe.\n\nArgs:\ndataset_id (str): The dataset_id user wants to explore\nlimit (int): Limit the number of rows returned. Default is -1 (no limit).\nto_pandas_opts (dict | None): Options to pass to the `to_pandas` method.\nRefer https://arrow.apache.org/docs/python/generated/pyarrow.Table.html#pyarrow.Table.to_pandas\nfor additional information.\n\nReturns:\npandas.DataFrame: The dataframe of the dataset features\n\"\"\"\ntry:\nif dataset_id == \"\":\nraise ValueError(\"'' is not a valid dataset_id\")\n\n# Small delay before fetching, in case this was called right after a push or feature materialization\ntime.sleep(0.2)\n\npayload = FetchFeatureRequest(dataset_id=dataset_id, limit=limit)\noptions = FlightCallOptions(headers=api.arrow_headers, timeout=120)\nticket = Ticket(payload.model_dump_json())\n\nflight_client = FlightClient(CONFIG.FEATURE_SERVER_ADDRESS)\nreader = flight_client.do_get(ticket, options)\n\nfeatures_table: pa.Table = reader.read_all()\n\nreturn TbPyArrow.arrow_table_to_pandas(features_table, to_pandas_opts)\nexcept Exception as e:\nlogger.error(f\"Feature server: {e!r}\")\nif \"Dataset does not exist\" in str(e):\nraise Exception(f\"Dataset `{dataset_id}` does not exist.\") from None\nraise Exception(f\"An error occurred while fetching features: {e!r}\") from e\n\n\ndef _register_udf(\ninput_types: list[str],\nresult_type: str,\nname: str,\nfunction_file_contents: str,\nlibraries: list[str],\nis_rich_function: bool,\ninitializer_arguments: list[str],\nclass_name: Optional[str],\nio_threads: Optional[int],\n):\n\"\"\"Add a User-Defined Function (UDF) to the system.\n\nThis function serializes the provided callable function and sends a series of\nrequests to register the UDF in the system.\n\nArgs:\ninput_types (list[str]): List of input types expected by the UDF.\nresult_type (str): The type of result produced by the UDF.\nname (str): Name of the UDF.\nfunction_file_contents (str): The contents of the python file that contains the UDF to be registered along with the imports used by it.\nlibraries (list[str]): List of libraries required by the UDF.\nis_rich_function (bool): Specifies whether the UDF is a rich function, i.e., a class-based function that uses state\ninitializer_arguments (list[str]): Arguments passed to the constructor of the rich function.\nclass_name (Optional[str]): Name of the class implementing the rich function, required if `is_rich_function` is True.\nio_threads (Optional[int]): Number of I/O threads allocated for the UDF, applicable for rich functions\nthat involve I/O operations like database or external service lookups.\n\nRaises:\nException: Raises an exception if the initial POST request to create the UDF fails.\nException: Raises an exception if registering the UDF with the system fails.\n\"\"\"\npayload = UdfFunctionSpec(\nname=name,\ninput_types=input_types,\noutput_type=result_type,\nlibraries=libraries,\nfunction_file_contents=function_file_contents,\nis_rich_function=is_rich_function,\ninitializer_arguments=initializer_arguments,\nclass_name=class_name,\nio_threads=io_threads,\n)\napi.post(endpoint=\"register_udf\", json=payload.model_dump())\n\n\ndef _register_udaf(\ninput_types: list[str],\nresult_type: str,\nname: str,\nfunction_file_contents: str,\n):\n\"\"\"Add a User-Defined Aggregation Function (UDAF) to the system.\nThis function serializes the provided callable function and sends a series of\nrequests to register the UDAF in the system.\nArgs:\ninput_types (list[str]): List of input types expected by the UDAF.\nresult_type (str): The type of result produced by the UDAF.\nname (str): Name of the UDAF.\nfunction_file_contents (str): The contents of the python file that contains the UDAF to be registered along with the imports used by it.\nRaises:\nException: Raises an exception if the initial POST request to create the UDAF fails.\nException: Raises an exception if registering the UDAF with the system fails.\n\"\"\"\n\nrw_embedded_udaf_spec = RwEmbeddedUdafFunctionSpec(\ninput_types=input_types,\noutput_type=result_type,\nfunction_file_contents=function_file_contents,\n)\npayload = UdafFunctionSpec(name=name, spec=rw_embedded_udaf_spec, libraries=[])\napi.post(endpoint=\"register_udaf\", json=payload.model_dump())\n\n\n@dataclass\nclass _UdafFeature:\nspec: UdafFeatureSpec\nfunction_file_contents: str\noutput_dtype: str\n\n\n@dataclass\nclass _UdfFeature:\nspec: UdfFeatureSpec\nfunction_file_contents: str\noutput_dtype: np.dtype\n\n\ndef _fetch_datasource(source_name: str):\ndatasource_json = api.get(endpoint=\"datasource\", json=source_name).json()\ndatasource = DataSource(**datasource_json)\nreturn datasource\n\n\ndef _get_udfs_from_ibis_table(table, backend_type):\n\"\"\"\nExtracts UDFs from an Ibis table and returns their details including name, source code,\noutput type, and input types.\n\"\"\"\nfrom ibis.backends.risingwave import Backend as RisingwaveBackend\nimport ibis.expr.operations as ops\n\nbackend = RisingwaveBackend()\ntype_mapper = backend.compiler.type_mapper\n\nudfs = []\n\nfor udf_node in table.op().find(ops.ScalarUDF):\nsource_lines = dedent(inspect.getsource(udf_node.__func__)).splitlines()\nsource_code = \"\\n\".join(\nline for line in source_lines if not line.startswith(\"@\")\n)\n\nresult_type = type_mapper.to_string(udf_node.dtype)\nif backend_type == BackEnd.Flink:\nsource_code = (\n\"from pyflink.table.udf import udf\\n\\n\"\n+ f\"@udf(result_type='{result_type}', func_type='general')\\n\"\n+ source_code\n)\n\nfn_imports = get_imports_used_in_function(udf_node.__func__)\nsource_code = f\"{fn_imports}\\n{source_code}\"\n\nudf_function_spec = UdfFunctionSpec(\nname=udf_node.__func_name__,\ninput_types=[type_mapper.to_string(arg.dtype) for arg in udf_node.args],\noutput_type=result_type,\nlibraries=[],\nfunction_file_contents=source_code,\nis_rich_function=False,\ninitializer_arguments=[],\nclass_name=None,\nio_threads=None,\n)\n\nudfs.append(udf_function_spec)\nreturn udfs\n\n\nclass IbisFeatureEngineering:\n\"\"\"\nA class for performing feature engineering using Ibis with various data sources.\n\nProvides methods to set up configurations and retrieve Ibis tables\nfor different types of data sources, such as S3, PostgreSQL.\n\"\"\"\n\ndef __init__(self) -> None:\nfrom ibis.backends.risingwave import Backend as RisingwaveBackend\nfrom ibis.backends.flink import Backend as FlinkBackend\n\nself._risingwave_backend = RisingwaveBackend()\nduckdb_backend = DuckdbBackend()\nduckdb_backend.do_connect()\nself._duckdb_backend = duckdb_backend\nself._flink_backend = FlinkBackend()\n\n@staticmethod\ndef _format_s3_endpoint(endpoint: str) -> str:\nreturn re.sub(r\"^https?://\", \"\", endpoint)\n\ndef _setup_s3_config(self, s3_config: S3Config) -> None:\n\"\"\"\nConfigure S3 settings for DuckDB, ensuring compatibility with MinIO and AWS S3.\n\"\"\"\nduckdb_con = self._duckdb_backend.con\n\nduckdb_con.sql(f\"SET s3_region='{s3_config.region}';\")\n\nif s3_config.access_key_id:\nduckdb_con.sql(f\"SET s3_access_key_id='{s3_config.access_key_id}';\")\nif s3_config.secret_access_key:\nduckdb_con.sql(f\"SET s3_secret_access_key='{s3_config.secret_access_key}';\")\n\nif s3_config.endpoint and not s3_config.endpoint.endswith(\"amazonaws.com\"):\nduckdb_con.sql(\nf\"SET s3_use_ssl={'true' if s3_config.endpoint.startswith('https') else 'false'};\"\n)\nendpoint = self._format_s3_endpoint(s3_config.endpoint)\n\nduckdb_con.sql(f\"SET s3_endpoint='{endpoint}';\")\nduckdb_con.sql(\"SET s3_url_style='path';\")\n\ndef _read_file_source(self, file_source: FileSource, name: str):\nif file_source.s3_config is None:\nraise ValueError(\"S3 configuration is required for reading from S3.\")\nself._setup_s3_config(file_source.s3_config)\npath = f\"s3://{file_source.s3_config.bucket}/{file_source.path}/*\"\n\nif file_source.format == FileSource.Format.CSV:\nreturn self._duckdb_backend.read_csv(file_source.path, name)\nelif file_source.format == FileSource.Format.PARQUET:\nreturn self._duckdb_backend.read_parquet(path, name)\nelse:\nraise ValueError(f\"Unimplemented file format: {file_source.format}\")\n\ndef _read_feature_group(self, feature_group_source: FeatureGroupSource):\ndf = get_features(feature_group_source.name, limit=100)\nreturn self._duckdb_backend.read_in_memory(df, feature_group_source.name)\n\ndef _read_postgres_source(self, postgres_source: PostgresSource):\nuri = (\nf\"postgres://{postgres_source.username}:{postgres_source.password}\"\nf\"@{postgres_source.host}:{postgres_source.port}/{postgres_source.database}\"\n)\nreturn self._duckdb_backend.read_postgres(uri, table_name=postgres_source.table)\n\ndef get_ibis_table(self, data_source: Union[str, DataSource]):\n\"\"\"\nRetrieve an Ibis table from a data source.\n\nArgs:\ndata_source (Union[str, DataSource]): The name of the data source as a string,\nor a `DataSource` object.\n\nReturns:\nTable: An Ibis table object corresponding to the provided data source.\n\nRaises:\nValueError: If the input type is invalid or the data source type is unsupported.\n\"\"\"\nif isinstance(data_source, str):\ndata_source = _fetch_datasource(data_source)\nif not isinstance(data_source, DataSource):\nraise TypeError(\nf\"Expected 'data_source' to be a DataSource instance, \"\nf\"but got {type(data_source).__name__}.\"\n)\nif data_source.file_source:\nreturn self._read_file_source(data_source.file_source, data_source.name)\nelif data_source.feature_group_source:\nreturn self._read_feature_group(data_source.feature_group_source)\nelif data_source.postgres_source:\nreturn self._read_postgres_source(data_source.postgres_source)\nelse:\nraise ValueError(\nf\"Unsupported data source type for {data_source.name}\"\n) from None\n\ndef materialize_features(\nself,\ntable: IbisTable,\nfeature_group_name: str,\nkey_field: str,\nbackend: BackEnd,\nprimary_source_name: str,\n):\n\"\"\"\nMaterialize features into a specified feature group using the selected backend.\n\nThis method registers the UDFs\nwith the backend, and triggers the feature materialization process for a specified\nfeature group. The backend can either be Risingwave or Flink.\n\nArgs:\ntable (IbisTable): The Ibis table representing the features to be materialized.\nfeature_group_name (str): The name of the feature group where the features will be stored.\nkey_field (str): The primary key field in the table used to uniquely identify records.\nbackend (BackEnd): The backend to use for materialization, either `Risingwave` or `Flink`.\nprimary_source_name (str): The name of the primary data source for the feature group.\n\nRaises:\nException: If an error occurs during the UDF registration or feature materialization process.\n\"\"\"\ntry:\nudfs_spec = _get_udfs_from_ibis_table(table, backend)\n\n[\napi.post(\nendpoint=\"register_udf\",\njson=udf.model_copy(\nupdate={\n\"function_file_contents\": re.sub(\nr\"from pyflink\\.table\\.udf import udf\\n|@udf\\(result_type=.*\\)\\n\",\n\"\",\nudf.function_file_contents,\n)\n}\n).model_dump(),\n)\nfor udf in udfs_spec\n]\n\nserialized_expr = cloudpickle.dumps(table)\nencoded_table = base64.b64encode(serialized_expr).decode(\"utf-8\")\n\npayload = IbisFeatureMaterializationRequest(\nfeature_group_name=feature_group_name,\nudfs_spec=udfs_spec,\nkey_field=key_field,\nbackend=backend,\nencoded_table=encoded_table,\nprimary_source_name=primary_source_name,\n)\napi.post(\nendpoint=\"ibis_materialize_features\",\njson=payload.model_dump(exclude_none=True),\n)\nexcept Exception as e:\nraise Exception(f\"Error while materializing features: {e!r}\") from None\n\ndef get_model_inputs(\nself,\nfeature_group_name: str,\nnumerical_fields: list | None = None,\ncategorical_fields: list | None = None,\ntextual_fields: list | None = None,\nimaginal_fields: list | None = None,\ntime_field: str | None = None,\n):\nfrom .datasets import OnlineInputs\n\nfeature_group = FeatureGroup(\n**api.get(endpoint=\"featuregroup\", json=feature_group_name).json()\n)\n# Normalize\nif numerical_fields is None:\nnumerical_fields = []\nif categorical_fields is None:\ncategorical_fields = []\nif textual_fields is None:\ntextual_fields = []\nif imaginal_fields is None:\nimaginal_fields = []\n\ndataframe = get_features(feature_group_name)\n\nfor field in (\nnumerical_fields + categorical_fields + textual_fields + imaginal_fields\n):\nif field not in dataframe.columns:\nraise ValueError(f\"Field '{field}' is not present in the dataset.\")\nif time_field is not None:\nif time_field not in dataframe.columns:\nraise ValueError(f\"Field '{time_field}' is not present in the dataset.\")\n\nreturn OnlineInputs(\ndataframe=dataframe,\nkey_field=feature_group.key_field,\nnumerical_fields=numerical_fields,\ncategorical_fields=categorical_fields,\ntextual_fields=textual_fields,\nimaginal_fields=imaginal_fields,\ntime_field=time_field,\ndataset_id=feature_group_name,\n)\n\n\nclass TurboMLScalarFunction:\ndef __init__(self, name=None, io_threads=None):\nself.name = name\nself.io_threads = io_threads\n\ndef func(self, *args):\nraise NotImplementedError(\"subclasses must implement func\")\n\n\nLocalFeatureTransformer = Callable[[pd.DataFrame], pd.DataFrame]\n\n\nclass LocalFeatureEngineering:\ndef __init__(self, features_df: pd.DataFrame):\nself.local_features_df = features_df\nself.pending_sql_features: dict[str, SqlFeatureSpec] = {}\nself.pending_ibis_feature: ibis.Table | None = None\nself.pending_aggregate_features: dict[str, AggregateFeatureSpec] = {}\nself.pending_udf_features: dict[str, _UdfFeature] = {}\nself.pending_udaf_features: dict[str, _UdafFeature] = {}\nself.timestamp_column_format: dict[str, str] = {}\nself.pending_feature_transformations: dict[str, LocalFeatureTransformer] = {}\n\ndef clone_with_df(self, feature_df: pd.DataFrame):\nclone = LocalFeatureEngineering(feature_df)\nclone.pending_sql_features = self.pending_sql_features.copy()\nclone.pending_ibis_feature = self.pending_ibis_feature\nclone.pending_aggregate_features = self.pending_aggregate_features.copy()\nclone.pending_udf_features = self.pending_udf_features.copy()\nclone.pending_udaf_features = self.pending_udaf_features.copy()\nclone.timestamp_column_format = self.timestamp_column_format.copy()\nclone.pending_feature_transformations = (\nself.pending_feature_transformations.copy()\n)\nclone._update_input_df(feature_df)\nreturn clone\n\ndef _update_input_df(self, df: pd.DataFrame) -> None:\nself.all_materialized_features_df = df\nself.local_features_df = df.copy()\nfor feature_name, transformer in self.pending_feature_transformations.items():\nif feature_name in df.columns:\n## TODO: A method to drop features could be nice to have\nraise ValueError(\nf\"Feature '{feature_name}' now exists in upstream materialized features\"\n)\nself.local_features_df = transformer(self.local_features_df)\n\ndef register_timestamp(self, column_name: str, format_type: str) -> None:\nif format_type not in get_timestamp_formats():\nraise ValueError(\nf\"Choose only the timestamp formats in {get_timestamp_formats()}\"\n)\nif column_name in self.timestamp_column_format and (\n(registered_format := self.timestamp_column_format[column_name])\n!= format_type\n):\nraise Exception(\nf\"The timestamp is already registered with a different format={registered_format}\"\n)\nself.timestamp_column_format[column_name] = format_type\n\ndef _get_timestamp_query(self, timestamp_column: str) -> tuple[str, str]:\ntry:\ntimestamp_format = self.timestamp_column_format[timestamp_column]\ntimestamp_query_rw, timestamp_query_ddb = convert_timestamp(\ntimestamp_column_name=timestamp_column,\ntimestamp_type=timestamp_format,\n)\nreturn timestamp_query_rw, timestamp_query_ddb\nexcept Exception as e:\nraise Exception(\nf\"Please register the timestamp column using `register_timestamp()` error caused by {e!r}\"\n) from None\n\ndef create_sql_features(self, sql_definition: str, new_feature_name: str) -> None:\n\"\"\"\nsql_definition: str\nThe SQL query you want to apply on the columns of the dataframe\nEg. \"transactionAmount + localHour\"\n\nnew_feature_name: str\nThe name of the new feature column\n\"\"\"\nif new_feature_name in self.local_features_df:\nraise ValueError(f\"Feature '{new_feature_name}' already exists\")\n\npattern = r'\"([^\"]+)\"'\nmatches = re.findall(pattern, sql_definition)\ncleaned_operands = [operand.strip('\"') for operand in matches]\nresult_string = re.sub(\npattern, lambda m: cleaned_operands.pop(0), sql_definition\n)\n\ndef feature_transformer(dataframe):\nquery = f\"SELECT {result_string} AS {new_feature_name} FROM dataframe\"\nresult_df = duckdb.sql(query).df()\nreturn dataframe.assign(**{new_feature_name: result_df[new_feature_name]})\n\nself.local_features_df = feature_transformer(self.local_features_df)\nself.pending_feature_transformations[new_feature_name] = feature_transformer\nself.pending_sql_features[new_feature_name] = SqlFeatureSpec(\nfeature_name=new_feature_name,\nsql_spec=sql_definition,\n)\n\ndef create_ibis_features(self, table: ibis.Table) -> None:\n\"\"\"\nProcesses an Ibis table and creates features by executing the table query.\n\nThis method verifies whether the provided Ibis table is derived from an in-memory\ntable that corresponds to the current dataset. It then connects to a DuckDB backend\nand executes the table query.\n\nParameters:\ntable (ibis.Table):\nThe Ibis table that contains the feature transformations to be executed.\n\nRaises:\nAssertionError:\nIf the provided Ibis table is not derived from an in-memory table associated\nwith the current dataset.\n\"\"\"\nif (local_feats := len(self.pending_feature_transformations)) > 0:\nraise Exception(\nf\"Can't create ibis features with other features: {local_feats} local features exist\"\n)\ntry:\ncon = DuckdbBackend()\ncon.do_connect()\nself.local_features_df = con.execute(table)\nself.pending_ibis_feature = table\nexcept Exception as e:\nraise Exception(\"An error occurred while creating ibis features\") from e\n\ndef create_aggregate_features(\nself,\ncolumn_to_operate: str,\ncolumn_to_group: str,\noperation: str,\nnew_feature_name: str,\ntimestamp_column: str,\nwindow_duration: float,\nwindow_unit: str,\n) -> None:\n\"\"\"\ncolumn_to_operate: str\nThe column to count\n\ncolumn_to_group: str\nThe column to group by\n\noperation: str\nThe operation to perform on the column, one of [\"SUM\", \"COUNT\", \"AVG\", \"MAX\", \"MIN\"]\n\nnew_feature_name: str\nThe name of the new feature\n\ntime_column: str\nThe column representing time or timestamp for windowing\n\nwindow_duration: float\nThe numeric duration of the window (e.g. 5, 1.1, 24 etc)\n\nwindow_unit: str\nThe unit of the window, one of [\"seconds\", \"minutes\", \"hours\", \"days\", \"weeks\", \"months\", \"years\"]\n\"\"\"\nif new_feature_name in self.local_features_df:\nraise ValueError(f\"Feature '{new_feature_name}' already exists\")\n\nif window_unit not in [\n\"seconds\",\n\"minutes\",\n\"hours\",\n\"days\",\n\"weeks\",\n\"months\",\n\"years\",\n]:\nraise Exception(\n\"\"\"Window unit should be one of [\"seconds\", \"minutes\", \"hours\", \"days\", \"weeks\", \"months\", \"years\"]\"\"\"\n)\nif window_unit == \"years\":\nwindow_unit = \"days\"\nwindow_duration = window_duration * 365\nif window_unit == \"months\":\nwindow_unit = \"days\"\nwindow_duration = window_duration * 30\nif window_unit == \"weeks\":\nwindow_unit = \"days\"\nwindow_duration = window_duration * 7\nif window_unit == \"days\":\nwindow_unit = \"hours\"\nwindow_duration = window_duration * 24\n\nwindow_duration_with_unit = str(window_duration) + \" \" + window_unit\n_, timestamp_query_ddb = self._get_timestamp_query(\ntimestamp_column=timestamp_column\n)\n\ndef feature_transformer(dataframe):\nreturn duckdb.sql(\nf\"\"\"\nSELECT *, {operation}({column_to_operate}) OVER win AS {new_feature_name}\nFROM dataframe\nWINDOW win AS (\nPARTITION BY {column_to_group}\nORDER BY {timestamp_query_ddb}\nRANGE BETWEEN INTERVAL {window_duration_with_unit} PRECEDING\nAND CURRENT ROW)\"\"\"\n).df()\n\nself.local_features_df = feature_transformer(self.local_features_df)\nself.pending_feature_transformations[new_feature_name] = feature_transformer\nself.pending_aggregate_features[new_feature_name] = AggregateFeatureSpec(\nfeature_name=new_feature_name,\ncolumn=column_to_operate,\naggregation_function=operation,\ngroup_by_columns=[column_to_group],\ninterval=window_duration_with_unit,\ntimestamp_column=timestamp_column,\n)\n\ndef create_rich_udf_features(\nself,\nnew_feature_name: str,\nargument_names: list[str],\nclass_name: str,\nfunction_name: str,\nclass_file_contents: str,\nlibraries: list[str],\ndev_initializer_arguments: list[str],\nprod_initializer_arguments: list[str],\nio_threads=None,\n) -> None:\nimport pandas as pd\n\nif new_feature_name in self.local_features_df:\nraise ValueError(f\"Feature '{new_feature_name}' already exists\")\n\ndef feature_transformer(dataframe):\nmain_globals = sys.modules[\"__main__\"].__dict__\nexec(class_file_contents, main_globals)\nobj = main_globals[class_name](*dev_initializer_arguments)\n\ntqdm.pandas(desc=\"Progress\")\n\nnew_col = dataframe.progress_apply(\nlambda row: obj.func(*[row[col] for col in argument_names]),\naxis=1,\n)\nreturn dataframe.assign(**{new_feature_name: new_col})\n\ntransformed_df = feature_transformer(self.local_features_df)\nout_col = transformed_df[new_feature_name]\nif not isinstance(out_col, pd.Series):\nraise ValueError(\nf\"UDF {function_name} must return a scalar value\"\n) from None\nout_type = out_col.dtype\nif not isinstance(out_type, np.dtype):\nraise ValueError(\nf\"UDF {function_name} must return a scalar value, instead got {out_type}\"\n) from None\n\nself.local_features_df = transformed_df\nself.pending_feature_transformations[new_feature_name] = feature_transformer\nself.pending_udf_features[new_feature_name] = _UdfFeature(\nspec=UdfFeatureSpec(\nfunction_name=function_name,\narguments=argument_names,\nlibraries=libraries,\nfeature_name=new_feature_name,\nis_rich_function=True,\nio_threads=io_threads,\nclass_name=class_name,\ninitializer_arguments=prod_initializer_arguments,\n),\nfunction_file_contents=class_file_contents,\noutput_dtype=out_type,\n)\n\ndef create_udf_features(\nself,\nnew_feature_name: str,\nargument_names: list[str],\nfunction_name: str,\nfunction_file_contents: str,\nlibraries: list[str],\n) -> None:\n\"\"\"\nnew_feature_name: str\nThe name of the new feature column\n\nargument_names: list[str]\nThe list of column names to pass as argument to the function\n\nfunction_name: str\nThe function name under which this UDF is registered\n\nfunction_file_contents: str\nThe contents of the python file that contains the function along with the imports used by it\n\nlibraries: list[str]\nThe list of libraries that need to be installed to run the function\n\n\"\"\"\nimport pandas as pd\n\nif new_feature_name in self.local_features_df:\nraise ValueError(f\"Feature '{new_feature_name}' already exists\")\n\ndef feature_transformer(dataframe):\nif len(dataframe) == 0:\ndataframe[new_feature_name] = pd.Series(dtype=object)\nreturn dataframe\nlocal_ctxt = {}\nexec(function_file_contents, local_ctxt)\nnew_col = dataframe.apply(\nlambda row: local_ctxt[function_name](\n## TODO: should we pass as kwargs instead of relying on order?\n*[row[col] for col in argument_names]\n),\naxis=1,\n)\nif not isinstance(new_col, pd.Series):\nprint(new_col.head())\nraise ValueError(f\"UDF {function_name} must return a scalar value\")\nreturn dataframe.assign(**{new_feature_name: new_col})\n\ntransformed_df = feature_transformer(self.local_features_df)\nout_col = transformed_df[new_feature_name]\nif not isinstance(out_col, pd.Series):\nraise ValueError(\nf\"UDF {function_name} must return a scalar value\"\n) from None\nout_type = out_col.dtype\nif not isinstance(out_type, np.dtype):\nraise ValueError(\nf\"UDF {function_name} must return a scalar value, instead got {out_type}\"\n) from None\n\nself.local_features_df = transformed_df\nself.pending_feature_transformations[new_feature_name] = feature_transformer\nself.pending_udf_features[new_feature_name] = _UdfFeature(\nspec=UdfFeatureSpec(\nfunction_name=function_name,\narguments=argument_names,\nlibraries=libraries,\nfeature_name=new_feature_name,\nis_rich_function=False,\nio_threads=None,\nclass_name=None,\ninitializer_arguments=[],\n),\nfunction_file_contents=function_file_contents,\noutput_dtype=out_type,\n)\n\ndef _create_dynamic_udaf_class(self, local_ctxt, return_type):\nclass DynamicUDAFClass(Accumulator):\ndef __init__(self):\nself._state = pickle.dumps(local_ctxt[\"create_state\"]())\n\ndef update(self, *values):  # Should values be pa.Array?\nfor row in zip(*values, strict=True):\nrow_values = [col.as_py() for col in row]\nstate = pickle.loads(self._state)\nself._state = pickle.dumps(\nlocal_ctxt[\"accumulate\"](state, *row_values)\n)\n\ndef merge(self, states: pa.Array):\ndeserialized_values = []\nfor list_array in states:\nfor pickled_value in list_array:\ndeserialized_values.append(pickle.loads(pickled_value.as_py()))\nmerged_value = reduce(local_ctxt[\"merge_states\"], deserialized_values)\nself._state = pickle.dumps(merged_value)\n\ndef state(self) -> pa.Array:\nreturn pa.array([[self._state]], type=pa.list_(pa.binary()))\n\ndef evaluate(self) -> pa.Scalar:\nreturn pa.scalar(\nlocal_ctxt[\"finish\"](pickle.loads(self._state)), type=return_type\n)\n\nreturn DynamicUDAFClass\n\ndef create_udaf_features(\nself,\nnew_feature_name: str,\ncolumn_to_operate: list[str],\nfunction_name: str,\nreturn_type: str,\nfunction_file_contents: str,\ncolumn_to_group: list[str],\ntimestamp_column: str,\nwindow_duration: float,\nwindow_unit: str,\n) -> None:\n\"\"\"\nnew_feature_name: str\nThe name of the new feature column\n\ncolumn_to_operate: list[str]\nThe list of column names to pass as argument to the function\n\nfunction_name: str\nThe function name under which this UDF is registered\n\nfunction_file_contents: str\nThe contents of the python file that contains the function along with the imports used by it\n\nreturn_type: list[str]\nThe return type the function\n\nlibraries: list[str]\nThe list of libraries that need to be installed to run the function\n\n\"\"\"\nif new_feature_name in self.local_features_df:\nraise ValueError(f\"Feature '{new_feature_name}' already exists\")\n\nlocal_ctxt = {}\nexec(function_file_contents, local_ctxt)\n\nrequired_functions = [\n\"create_state\",\n\"accumulate\",\n\"retract\",\n\"merge_states\",\n\"finish\",\n]\nmissing_functions = [f for f in required_functions if f not in local_ctxt]\n\nif missing_functions:\nraise AssertionError(\nf\"Missing functions in UDAF: {', '.join(missing_functions)}. Functions create_state, \"\nf\"accumulate, retract, merge_states, and finish must be defined.\"\n)\n\nif window_unit not in [\n\"seconds\",\n\"minutes\",\n\"hours\",\n\"days\",\n\"weeks\",\n\"months\",\n\"years\",\n]:\nraise Exception(\n\"\"\"Window unit should be one of [\"seconds\", \"minutes\", \"hours\", \"days\", \"weeks\", \"months\", \"years\"]\"\"\"\n)\nif window_unit == \"years\":\nwindow_unit = \"days\"\nwindow_duration = window_duration * 365\nif window_unit == \"months\":\nwindow_unit = \"days\"\nwindow_duration = window_duration * 30\nif window_unit == \"weeks\":\nwindow_unit = \"days\"\nwindow_duration = window_duration * 7\nif window_unit == \"days\":\nwindow_unit = \"hours\"\nwindow_duration = window_duration * 24\n\nwindow_duration_with_unit = str(window_duration) + \" \" + window_unit\n_, timestamp_query_ddb = self._get_timestamp_query(\ntimestamp_column=timestamp_column\n)\n\ndef feature_transformer(dataframe):\nctx = SessionContext()\npa_return_type = risingwave_type_to_pyarrow(return_type)\narrow_table = pa.Table.from_pandas(dataframe)\n\nctx.create_dataframe([arrow_table.to_batches()], name=\"my_table\")\n\nmy_udaf = udaf(\nself._create_dynamic_udaf_class(local_ctxt, pa_return_type),\n[arrow_table[col].type for col in column_to_operate],\npa_return_type,\n[pa.list_(pa.binary())],\n\"stable\",\nname=function_name,\n)\nctx.register_udaf(my_udaf)\n\ncolumn_args = \", \".join(f'\"{item}\"' for item in column_to_operate)\ngroup_by_cols = \", \".join(f'\"{item}\"' for item in column_to_group)\n\nsql = f\"\"\"\nSELECT *, {function_name}({column_args}) OVER win AS {new_feature_name}\nFROM my_table\nWINDOW win AS (\nPARTITION BY {group_by_cols}\nORDER BY {timestamp_query_ddb}\nRANGE BETWEEN UNBOUNDED PRECEDING\nAND CURRENT ROW)\"\"\"\n\ndataframe = ctx.sql(sql)\nreturn dataframe.to_pandas()\n\nself.local_features_df = feature_transformer(self.local_features_df)\nself.pending_feature_transformations[new_feature_name] = feature_transformer\nself.pending_udaf_features[new_feature_name] = _UdafFeature(\nspec=UdafFeatureSpec(\nfunction_name=function_name,\nfeature_name=new_feature_name,\narguments=column_to_operate,\ngroup_by_columns=column_to_group,\ninterval=window_duration_with_unit,\ntimestamp_column=timestamp_column,\n),\nfunction_file_contents=function_file_contents,\noutput_dtype=return_type,\n)\n\n\nclass FeatureEngineering:\ndef __init__(\nself,\ndataset_id: str,\nfeatures_df: pd.DataFrame,\nlocal_fe: LocalFeatureEngineering | None = None,\n):\nself.dataset_id = dataset_id\nself._sync_feature_information()\nself.all_materialized_features_df = features_df.copy()\n\nself.local_fe = (\nLocalFeatureEngineering(features_df) if local_fe is None else local_fe\n)\nself.local_fe.timestamp_column_format = {\n**self.timestamp_column_format,\n**self.local_fe.timestamp_column_format,\n}\n\ndef _sync_feature_information(self):\ndataset_json = api.get(endpoint=f\"dataset?dataset_id={self.dataset_id}\").json()\ndataset = Dataset(**dataset_json)\nself.sql_feats = dataset.sql_feats\nself.agg_feats = dataset.agg_feats\nself.udf_feats = dataset.udf_feats\nself.udaf_feats = dataset.udaf_feats\nself.ibis_feats = dataset.ibis_feats\nself.timestamp_column_format = dataset.timestamp_fields\n\n@staticmethod\ndef inherit_from_local(fe: LocalFeatureEngineering, dataset_id: str):\nreturn FeatureEngineering(dataset_id, fe.local_features_df, fe)\n\n@property\ndef local_features_df(self) -> pd.DataFrame:\nreturn self.local_fe.local_features_df\n\ndef _update_input_df(self, df: pd.DataFrame) -> None:\nself._sync_feature_information()\nself.local_fe._update_input_df(df.copy())\nself.all_materialized_features_df = df.copy()\n\ndef get_local_features(self) -> pd.DataFrame:\nreturn self.local_fe.local_features_df\n\ndef get_materialized_features(self) -> pd.DataFrame:\nreturn self.all_materialized_features_df\n\ndef register_timestamp(self, column_name: str, format_type: str) -> None:\nself.local_fe.register_timestamp(column_name, format_type)\n\ndef _get_timestamp_query(self, timestamp_column: str) -> tuple[str, str]:\nreturn self.local_fe._get_timestamp_query(timestamp_column)\n\ndef create_sql_features(self, sql_definition: str, new_feature_name: str) -> None:\n\"\"\"\nsql_definition: str\nThe SQL query you want to apply on the columns of the dataframe\nEg. \"transactionAmount + localHour\"\n\nnew_feature_name: str\nThe name of the new feature column\n\"\"\"\nself.local_fe.create_sql_features(sql_definition, new_feature_name)\n\ndef create_ibis_features(self, table: ibis.Table) -> None:\n\"\"\"\nProcesses an Ibis table and creates features by executing the table query.\n\nThis method verifies whether the provided Ibis table is derived from an in-memory\ntable that corresponds to the current dataset. It then connects to a DuckDB backend\nand executes the table query.\n\nParameters:\ntable (ibis.Table):\nThe Ibis table that contains the feature transformations to be executed.\n\nRaises:\nAssertionError:\nIf the provided Ibis table is not derived from an in-memory table associated\nwith the current dataset.\n\"\"\"\nother_feats = (\nself.sql_feats.keys() | self.agg_feats.keys() | self.udf_feats.keys()\n)\nif len(other_feats) > 0:\nraise Exception(\nf\"Can't create ibis features with other features: {other_feats} non-ibis features exist\"\n)\nself.local_fe.create_ibis_features(table)\n\ndef create_aggregate_features(\nself,\ncolumn_to_operate: str,\ncolumn_to_group: str,\noperation: str,\nnew_feature_name: str,\ntimestamp_column: str,\nwindow_duration: float,\nwindow_unit: str,\n) -> None:\n\"\"\"\ncolumn_to_operate: str\nThe column to count\n\ncolumn_to_group: str\nThe column to group by\n\noperation: str\nThe operation to perform on the column, one of [\"SUM\", \"COUNT\", \"AVG\", \"MAX\", \"MIN\"]\n\nnew_feature_name: str\nThe name of the new feature\n\ntime_column: str\nThe column representing time or timestamp for windowing\n\nwindow_duration: float\nThe numeric duration of the window (e.g. 5, 1.1, 24 etc)\n\nwindow_unit: str\nThe unit of the window, one of [\"seconds\", \"minutes\", \"hours\", \"days\", \"weeks\", \"months\", \"years\"]\n\"\"\"\nself.local_fe.create_aggregate_features(\ncolumn_to_operate,\ncolumn_to_group,\noperation,\nnew_feature_name,\ntimestamp_column,\nwindow_duration,\nwindow_unit,\n)\n\ndef create_rich_udf_features(\nself,\nnew_feature_name: str,\nargument_names: list[str],\nclass_name: str,\nfunction_name: str,\nclass_file_contents: str,\nlibraries: list[str],\ndev_initializer_arguments: list[str],\nprod_initializer_arguments: list[str],\nio_threads=None,\n) -> None:\nself.local_fe.create_rich_udf_features(\nnew_feature_name,\nargument_names,\nclass_name,\nfunction_name,\nclass_file_contents,\nlibraries,\ndev_initializer_arguments,\nprod_initializer_arguments,\nio_threads,\n)\n\ndef create_udf_features(\nself,\nnew_feature_name: str,\nargument_names: list[str],\nfunction_name: str,\nfunction_file_contents: str,\nlibraries: list[str],\n) -> None:\n\"\"\"\nnew_feature_name: str\nThe name of the new feature column\n\nargument_names: list[str]\nThe list of column names to pass as argument to the function\n\nfunction_name: str\nThe function name under which this UDF is registered\n\nfunction_file_contents: str\nThe contents of the python file that contains the function along with the imports used by it\n\nlibraries: list[str]\nThe list of libraries that need to be installed to run the function\n\n\"\"\"\nself.local_fe.create_udf_features(\nnew_feature_name,\nargument_names,\nfunction_name,\nfunction_file_contents,\nlibraries,\n)\n\ndef create_udaf_features(\nself,\nnew_feature_name: str,\ncolumn_to_operate: list[str],\nfunction_name: str,\nreturn_type: str,\nfunction_file_contents: str,\ncolumn_to_group: list[str],\ntimestamp_column: str,\nwindow_duration: float,\nwindow_unit: str,\n) -> None:\n\"\"\"\nnew_feature_name: str\nThe name of the new feature column\n\ncolumn_to_operate: list[str]\nThe list of column names to pass as argument to the function\n\nfunction_name: str\nThe function name under which this UDF is registered\n\nfunction_file_contents: str\nThe contents of the python file that contains the function along with the imports used by it\n\nreturn_type: list[str]\nThe return type the function\n\nlibraries: list[str]\nThe list of libraries that need to be installed to run the function\n\n\"\"\"\nself.local_fe.create_udaf_features(\nnew_feature_name,\ncolumn_to_operate,\nfunction_name,\nreturn_type,\nfunction_file_contents,\ncolumn_to_group,\ntimestamp_column,\nwindow_duration,\nwindow_unit,\n)\n\ndef _register_timestamps_for_aggregates(self, feature_names: list[str]):\n\"\"\"Register timestamps for the provided aggregate features. These timestamps can then be used\nfor windows in aggregate features.\n\"\"\"\ntry:\nspecs = [\nself.local_fe.pending_aggregate_features[feature_name]\nfor feature_name in feature_names\n]\nexcept KeyError as V:\nraise ValueError(f\"Aggregated feature {V} not found\") from V\n\nfor spec in specs:\ntimestamp_format = self.local_fe.timestamp_column_format[\nspec.timestamp_column\n]\npayload = TimestampQuery(\ncolumn_name=spec.timestamp_column,\ntimestamp_format=timestamp_format,\n)\napi.post(\nendpoint=f\"dataset/{self.dataset_id}/register_timestamp\",\njson=payload.model_dump(),\n)\n\ndef _register_udfs(self, feature_names: list[str]):\n\"\"\"\nRegister UDFs corresponding to the provided feature names. These UDFs are then used to create features.\n\"\"\"\nfor feature_name in feature_names:\ntry:\nudf = self.local_fe.pending_udf_features[feature_name]\nexcept KeyError as V:\nraise ValueError(f\"UDF feature {feature_name} not found\") from V\n\ndef pandas_type_to_risingwave_type(pd_type):\nmatch pd_type:\ncase np.int32:\nreturn \"INT\"\ncase np.int64:\nreturn \"BIGINT\"\ncase np.float32 | np.float64:\nreturn \"REAL\"\ncase np.object_:\nreturn \"VARCHAR\"\ncase _:\nreturn \"VARCHAR\"\n\ndb_dtype = pandas_type_to_risingwave_type(udf.output_dtype)\ndataset_json = api.get(\nendpoint=f\"dataset?dataset_id={self.dataset_id}\"\n).json()\ndataset = Dataset(**dataset_json)\ntable_columns = {col.name: col.dtype for col in dataset.table_columns}\n\n# Converts character varying to VARCHAR as that is only supported\n# by RisingWave as of (18.12.2023)\ninput_types = [\n\"VARCHAR\"\nif table_columns[col] == \"character varying\"\nelse table_columns[col]\nfor col in udf.spec.arguments\n]\n_register_udf(\nname=udf.spec.function_name,\ninput_types=input_types,\nlibraries=udf.spec.libraries,\nresult_type=db_dtype,\nfunction_file_contents=udf.function_file_contents,\nis_rich_function=udf.spec.is_rich_function,\ninitializer_arguments=udf.spec.initializer_arguments,\nclass_name=udf.spec.class_name,\nio_threads=udf.spec.io_threads,\n)\n\ndef _register_udafs(self, feature_names: list[str]):\n\"\"\"\nRegister UDAFs corresponding to the provided feature names. These UDAFs are then used to create features.\n\"\"\"\nfor feature_name in feature_names:\ntry:\nudaf = self.local_fe.pending_udaf_features[feature_name]\nexcept KeyError as V:\nraise ValueError(f\"UDAF feature {feature_name} not found\") from V\n\ndb_dtype = udaf.output_dtype\ndataset_json = api.get(\nendpoint=f\"dataset?dataset_id={self.dataset_id}\"\n).json()\ndataset = Dataset(**dataset_json)\ntable_columns = {col.name: col.dtype for col in dataset.table_columns}\n\n# Converts character varying to VARCHAR as that is only supported\n# by RisingWave as of (18.12.2023)\ninput_types = [\n\"VARCHAR\"\nif table_columns[col] == \"character varying\"\nelse table_columns[col]\nfor col in udaf.spec.arguments\n]\ntimestamp_format = self.local_fe.timestamp_column_format[\nudaf.spec.timestamp_column\n]\npayload = TimestampQuery(\ncolumn_name=udaf.spec.timestamp_column,\ntimestamp_format=timestamp_format,\n)\napi.post(\nendpoint=f\"dataset/{self.dataset_id}/register_timestamp\",\njson=payload.model_dump(),\n)\n_register_udaf(\nname=udaf.spec.function_name,\ninput_types=input_types,\nresult_type=db_dtype,\nfunction_file_contents=udaf.function_file_contents,\n)\n\ndef materialize_ibis_features(self):\n\"\"\"Send a POST request to the server to perform feature engineering based on the created ibis features.\n\nRaises:\nException: Raised if the server's response has a non-200 status code.\nThe exception message will contain details provided by the server.\n\"\"\"\nif self.local_fe.pending_ibis_feature is None:\nraise ValueError(\n\"No pending Ibis features found. Please create features using `create_ibis_features` first.\"\n)\n\ntable = self.local_fe.pending_ibis_feature\nudfs_spec = _get_udfs_from_ibis_table(table, BackEnd.Risingwave)\n\nfor udf in udfs_spec:\napi.post(endpoint=\"register_udf\", json=udf.model_dump())\n\nserialized_expr = cloudpickle.dumps(table)\nencoded_table = base64.b64encode(serialized_expr).decode(\"utf-8\")\nibis_feat_spec = IbisFeatureSpec(\ndataset_id=self.dataset_id,\nencoded_table=encoded_table,\nudfs_spec=udfs_spec,\n)\npayload = FeatureMaterializationRequest(\ndataset_id=self.dataset_id, ibis_feats=ibis_feat_spec\n)\napi.post(endpoint=\"materialize_features\", json=payload.model_dump())\n\nself.all_materialized_features_df = get_features(self.dataset_id)\n\n@property\ndef all_pending_features_names(self):\nall_pending_features: list[dict[str, Any]] = [\nself.local_fe.pending_sql_features,\nself.local_fe.pending_aggregate_features,\nself.local_fe.pending_udf_features,\nself.local_fe.pending_udaf_features,\n]\nreturn [n for features in all_pending_features for n in features.keys()]\n\ndef materialize_features(self, feature_names: list[str]):\n\"\"\"Send a POST request to the server to perform feature engineering based on the provided timestamp query.\n\nRaises:\nException: Raised if the server's response has a non-200 status code.\nThe exception message will contain details provided by the server.\n\"\"\"\nmissing_features = (\nset(feature_names)\n- set(self.local_fe.pending_feature_transformations.keys())\n- set(self.all_materialized_features_df.columns)\n)\nif missing_features:\nraise ValueError(\nf\"Feature names {missing_features} are not pending and also do not exist in the dataset\"\n)\n\naggregates_to_register = [\nk\nfor k, v in self.local_fe.pending_aggregate_features.items()\nif k in feature_names\n]\nudfs_to_register = [\nk\nfor k, v in self.local_fe.pending_udf_features.items()\nif k in feature_names\n]\nudafs_to_register = [\nk\nfor k, v in self.local_fe.pending_udaf_features.items()\nif k in feature_names\n]\nsql_to_register = [\nk\nfor k, v in self.local_fe.pending_sql_features.items()\nif k in feature_names\n]\n\nself._register_timestamps_for_aggregates(aggregates_to_register)\nself._register_udfs(udfs_to_register)\nself._register_udafs(udafs_to_register)\n\npayload = FeatureMaterializationRequest(\ndataset_id=self.dataset_id,\nsql_feats=[self.local_fe.pending_sql_features[k] for k in sql_to_register],\nagg_feats=[\nself.local_fe.pending_aggregate_features[k]\nfor k in aggregates_to_register\n],\nudf_feats=[\nself.local_fe.pending_udf_features[k].spec for k in udfs_to_register\n],\nudaf_feats=[\nself.local_fe.pending_udaf_features[k].spec for k in udafs_to_register\n],\n)\ntry:\nresponse = api.post(\nendpoint=\"materialize_features\", json=payload.model_dump()\n)\nif response.status_code != 200:\nraise Exception(f\"Error from server: {response.text}\")\nexcept Exception as e:\nraise Exception(f\"Failed to materialize features: {e!r}\") from e\nfinally:\nself.all_materialized_features_df = get_features(self.dataset_id)\n\n# clean up pending features\nfor feature_name in feature_names:\nself.local_fe.pending_feature_transformations.pop(feature_name)\nself.local_fe.pending_sql_features.pop(feature_name, None)\nself.local_fe.pending_aggregate_features.pop(feature_name, None)\nself.local_fe.pending_udf_features.pop(feature_name, None)\nself.local_fe.pending_udaf_features.pop(feature_name, None)\n\n```"
    },
    {
        "section": "internal.py",
        "content": "# internal.py\n-## Location -> root_directory.common\n```python\nimport itertools\nfrom typing import Generator, Iterable\nimport typing\nfrom pandas.core.base import DtypeObj\nimport pyarrow as pa\nimport pyarrow.flight\nimport pandas as pd\nimport logging\nimport threading\n\nfrom .api import api\nfrom tqdm import tqdm\n\nfrom .models import InputSpec\n\nlogger = logging.getLogger(__name__)\n\n\nclass TurboMLResourceException(Exception):\ndef __init__(self, message) -> None:\nsuper().__init__(message)\n\n\nclass TbPyArrow:\n\"\"\"\nUtility class containing some shared methods and data for our\nPyArrow based data exchange.\n\"\"\"\n\n@staticmethod\ndef _input_schema(has_labels: bool) -> pa.Schema:\nlabel_schema = [(\"label\", pa.float32())] if has_labels else []\nreturn pa.schema(\n[\n(\"numeric\", pa.list_(pa.float32())),\n(\"categ\", pa.list_(pa.int64())),\n(\"text\", pa.list_(pa.string())),\n(\"image\", pa.list_(pa.binary())),\n(\"time_tick\", pa.int32()),\n(\"key\", pa.string()),\n]\n+ label_schema\n)\n\n@staticmethod\ndef arrow_table_to_pandas(\ntable: pa.Table, to_pandas_opts: dict | None = None\n) -> pd.DataFrame:\ndefault_opts = {\"split_blocks\": False, \"date_as_object\": False}\nto_pandas_opts = {**default_opts, **(to_pandas_opts or {})}\nreturn table.to_pandas(**to_pandas_opts)\n\n@staticmethod\ndef df_to_table(df: pd.DataFrame, input_spec: InputSpec) -> pa.Table:\n# transform df to input form, where each column\n# is a list of values of the corresponding type\ninput_df = pd.DataFrame()\ninput_df[\"key\"] = df[input_spec.key_field].astype(\"str\").values\ninput_df[\"time_tick\"] = (\n0\nif input_spec.time_field in [\"\", None]\nelse df[input_spec.time_field].astype(\"int32\").values\n)\ninput_df[\"numeric\"] = (\ndf[input_spec.numerical_fields].astype(\"float32\").values.tolist()\n)\ninput_df[\"categ\"] = (\ndf[input_spec.categorical_fields].astype(\"int64\").values.tolist()\n)\ninput_df[\"text\"] = df[input_spec.textual_fields].astype(\"str\").values.tolist()\ninput_df[\"image\"] = (\ndf[input_spec.imaginal_fields].astype(\"bytes\").values.tolist()\n)\n\nhas_labels = input_spec.label_field is not None and input_spec.label_field in df\nif has_labels:\ninput_df[\"label\"] = df[input_spec.label_field].astype(\"float32\").values\n\nreturn pa.Table.from_pandas(input_df, TbPyArrow._input_schema(has_labels))\n\n@staticmethod\ndef wait_for_available(client: pyarrow.flight.FlightClient, timeout=10):\ntry:\nclient.wait_for_available(timeout=timeout)\nexcept pyarrow.flight.FlightUnauthenticatedError:\n# Server is up - wait_for_available() does not ignore auth errors\npass\n\n@staticmethod\ndef handle_flight_error(\ne: Exception, client: pyarrow.flight.FlightClient, allow_recovery=True\n):\nif isinstance(e, pyarrow.flight.FlightUnavailableError):\nif not allow_recovery:\nraise TurboMLResourceException(\n\"Failed to initialize TurboMLArrowServer: Check logs for more details\"\n) from e\n\n# If the server is not available, we can try to start it\napi.post(endpoint=\"start_arrow_server\")\nTbPyArrow.wait_for_available(client)\nreturn\nif isinstance(e, pyarrow.flight.FlightTimedOutError):\nif not allow_recovery:\nraise TurboMLResourceException(\n\"Flight server timed out: Check logs for more details\"\n) from e\nTbPyArrow.wait_for_available(client)\nreturn\nif isinstance(e, pyarrow.flight.FlightInternalError):\nraise TurboMLResourceException(\nf\"Internal flight error: {e!r}. Check logs for more details\"\n) from e\nif isinstance(e, pyarrow.flight.FlightError):\nraise TurboMLResourceException(\nf\"Flight server error: {e!r}. Check logs for more details\"\n) from e\nraise Exception(f\"Unknown error: {e!r}\") from e\n\n@staticmethod\ndef _put_and_retry(\nclient: pyarrow.flight.FlightClient,\nupload_descriptor: pyarrow.flight.FlightDescriptor,\noptions: pyarrow.flight.FlightCallOptions,\ninput_table: pa.Table,\ncan_retry: bool = True,\nmax_chunksize: int = 1024,\nepochs: int = 1,\n) -> None:\ntry:\nwriter, _ = client.do_put(\nupload_descriptor, input_table.schema, options=options\n)\nTbPyArrow._write_in_chunks(\nwriter, input_table, max_chunksize=max_chunksize, epochs=epochs\n)\nwriter.close()\nexcept Exception as e:\nTbPyArrow.handle_flight_error(e, client, can_retry)\nreturn TbPyArrow._put_and_retry(\nclient,\nupload_descriptor,\noptions,\ninput_table,\ncan_retry=False,\nmax_chunksize=max_chunksize,\nepochs=epochs,\n)\n\n@staticmethod\ndef _exchange_and_retry(\nclient: pyarrow.flight.FlightClient,\nupload_descriptor: pyarrow.flight.FlightDescriptor,\noptions: pyarrow.flight.FlightCallOptions,\ninput_table: pa.Table,\ncan_retry: bool = True,\nmax_chunksize: int = 1024,\n) -> pa.Table:\ntry:\nwriter, reader = client.do_exchange(upload_descriptor, options=options)\nwriter.begin(input_table.schema)\nwrite_event = threading.Event()\nwriter_thread = threading.Thread(\ntarget=TbPyArrow._write_in_chunks,\nargs=(writer, input_table),\nkwargs={\n\"max_chunksize\": max_chunksize,\n\"write_event\": write_event,\n},\n)\nwriter_thread.start()\nwrite_event.wait()\nread_result = TbPyArrow._read_in_chunks(reader)\nwriter_thread.join()\nwriter.close()\nreturn read_result\nexcept Exception as e:\nTbPyArrow.handle_flight_error(e, client, can_retry)\nreturn TbPyArrow._exchange_and_retry(\nclient,\nupload_descriptor,\noptions,\ninput_table,\ncan_retry=False,\nmax_chunksize=max_chunksize,\n)\n\n@staticmethod\ndef _write_in_chunks(\nwriter: pyarrow.flight.FlightStreamWriter,\ninput_table: pa.Table,\nwrite_event: threading.Event | None = None,\nmax_chunksize: int = 1024,\nepochs: int = 1,\n) -> None:\ntotal_rows = input_table.num_rows\nn_chunks = total_rows // max_chunksize + 1\nlogger.info(f\"Starting to upload data... Total rows: {total_rows}\")\n\nfor epoch in range(epochs):\nwith tqdm(\ntotal=n_chunks, desc=\"Progress\", unit=\"chunk\", unit_scale=True\n) as pbar:\nif epochs > 1:\npbar.set_postfix(epoch=epoch + 1)\n\nfor start in range(0, total_rows, max_chunksize):\nchunk_table = input_table.slice(\nstart, min(max_chunksize, total_rows - start)\n)\nwriter.write_table(chunk_table)\nif start == 0 and write_event:\nwrite_event.set()\npbar.update(1)\n\nwriter.done_writing()\nlogger.info(\"Completed data upload.\")\n\n@staticmethod\ndef _read_in_chunks(reader: pyarrow.flight.FlightStreamReader) -> pa.Table:\nbatches = []\nwhile True:\ntry:\nchunk = reader.read_chunk()\nbatches.append(chunk.data)\nexcept StopIteration:\nbreak\nreturn pa.Table.from_batches(batches) if batches else None\n\n\nclass TbPandas:\n@staticmethod\ndef fill_nans_with_default(series: pd.Series):\nif series.isna().any():\ndefault = TbPandas.default_for_type(series.dtype)\nreturn series.fillna(value=default)\nreturn series\n\n@staticmethod\ndef default_for_type(dtype: DtypeObj):\nif pd.api.types.is_numeric_dtype(dtype):\nreturn 0\nif pd.api.types.is_string_dtype(dtype):\nreturn \"\"\nif pd.api.types.is_bool_dtype(dtype):\nreturn False\nif pd.api.types.is_datetime64_dtype(dtype):\nreturn pd.Timestamp(\"1970-01-01\")\nraise ValueError(f\"Unsupported dtype: {dtype}\")\n\n\nT = typing.TypeVar(\"T\")\n\n\nclass TbItertools:\n@staticmethod\ndef chunked(iterable: Iterable[T], n: int) -> Generator[list[T], None, None]:\n\"\"\"Yield successive n-sized chunks from iterable.\"\"\"\niterator = iter(iterable)\nwhile True:\nchunk = list(itertools.islice(iterator, n))\nif not chunk:\nbreak\nyield list(chunk)\n\n```"
    },
    {
        "section": "llm.py",
        "content": "# llm.py\n-## Location -> root_directory.common\n```python\nimport logging\nimport time\n\nfrom turboml.common.types import GGUFModelId\nfrom .api import api\nfrom .models import (\nLlamaServerRequest,\nLlamaServerResponse,\nHFToGGUFRequest,\nModelAcquisitionJob,\n)\n\nlogger = logging.getLogger(\"turboml.llm\")\n\n\ndef acquire_hf_model_as_gguf(\nhf_repo_id: str,\nmodel_type: HFToGGUFRequest.GGUFType = HFToGGUFRequest.GGUFType.AUTO,\nselect_gguf_file: str | None = None,\n) -> GGUFModelId:\n\"\"\"\nAttempts to acquires a model from the Hugging Face repository and convert\nit to the GGUF format. The model is then stored in the TurboML system, and\nthe model key is returned.\n\"\"\"\nreq = HFToGGUFRequest(\nhf_repo_id=hf_repo_id,\nmodel_type=model_type,\nselect_gguf_file=select_gguf_file,\n)\nacq_resp = api.post(\"acquire_hf_model_as_gguf\", json=req.model_dump()).json()\nstatus_endpoint = acq_resp[\"status_endpoint\"]\nlast_status = None\nlast_progress = None\n\nwhile True:\njob_info = api.get(status_endpoint.lstrip(\"/\")).json()\njob_info = ModelAcquisitionJob(**job_info)\n\nstatus = job_info.status\nprogress = job_info.progress_message or \"No progress info\"\n\nif status != last_status or progress != last_progress:\nlogger.info(f\"[hf-acquisition] Status: {status}, Progress: {progress}\")\nlast_status = status\nlast_progress = progress\n\nif status == \"completed\":\ngguf_id = job_info.gguf_id\nif not gguf_id:\nraise AssertionError(\"GGUF ID not found in job_info\")\nlogger.info(f\"[hf-acquisition] Acquisition Done, gguf_id = {gguf_id}\")\nreturn GGUFModelId(gguf_id)\nelif status == \"failed\":\nerror_msg = job_info.error_message or \"Unknown error\"\nraise RuntimeError(f\"HF->GGUF acquisition failed: {error_msg}\")\n\ntime.sleep(5)\n\n\ndef spawn_llm_server(req: LlamaServerRequest) -> LlamaServerResponse:\n\"\"\"\nIf source_type=HUGGINGFACE, we do the async acquisition under the hood,\nbut we poll until it’s done. Then we do the normal /model/openai call.\n\"\"\"\nif req.source_type == LlamaServerRequest.SourceType.HUGGINGFACE:\nif not req.hf_spec:\nraise ValueError(\"hf_spec is required for source_type=HUGGINGFACE\")\ngguf_id = acquire_hf_model_as_gguf(\nhf_repo_id=req.hf_spec.hf_repo_id,\nmodel_type=req.hf_spec.model_type,\nselect_gguf_file=req.hf_spec.select_gguf_file,\n)\nreq.source_type = LlamaServerRequest.SourceType.GGUF_ID\nreq.gguf_id = gguf_id\n\nresp = api.post(\"model/openai\", json=req.model_dump())\nreturn LlamaServerResponse(**resp.json())\n\n\ndef stop_llm_server(server_id: str):\n\"\"\"\nTo DE_acquire(iLETE /model/openai/{server_id}\n\"\"\"\napi.delete(f\"model/openai/{server_id}\")\n\n```"
    },
    {
        "section": "ml_algs.py",
        "content": "# ml_algs.py\n-## Location -> root_directory.common\n```python\nfrom __future__ import annotations\nimport logging\nfrom abc import ABC\nfrom typing import Optional, TYPE_CHECKING, List\nimport random\nimport string\nimport urllib.parse as urlparse\nimport re\nimport json\nimport os\nimport time\nimport base64\nimport datetime\n\nfrom google.protobuf import json_format\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.flight\nfrom pydantic import (\nBaseModel,\nField,\nValidationError,\nvalidator,\nConfigDict,\nmodel_validator,\nfield_serializer,\nfield_validator,\n)\n\n\nfrom .types import GGUFModelId  # noqa: TCH001\n\nif TYPE_CHECKING:\nfrom sklearn import metrics\n\nfrom .default_model_configs import DefaultModelConfigs\nfrom .internal import TbPyArrow\nfrom .api import api\nfrom .models import (\nInputSpec,\nModelConfigStorageRequest,\nModelInfo,\nMetricRegistrationRequest,\nEvaluationMetrics,\nMLModellingRequest,\nLabelAssociation,\nLearnerConfig,\nModelParams,\nModelPatchRequest,\nModelDeleteRequest,\nEvaluations,\nProcessOutput,\n)\nfrom .feature_engineering import retrieve_features\nfrom .env import CONFIG\nfrom .protos import output_pb2, metrics_pb2\nfrom .dataloader import StreamType, get_proto_msgs\n\nfrom turboml.common.pytypes import InputData, OutputData\nfrom turboml.common.pymodel import create_model_from_config, Model as CppModel\nfrom turboml.common.datasets import (\nLocalInputs,\nLocalLabels,\nOnlineInputs,\nOnlineLabels,\nPandasHelpers,\n)\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndefault_configs = DefaultModelConfigs\nMAX_RETRY_FOR_TEST = 5\n\n\ndef _istest():\nreturn os.environ.get(\"TURBOML_AUTO_RETRY\", \"false\") == \"true\"\n\n\ndef retry_operation(operation, attempts=MAX_RETRY_FOR_TEST, base_delay=4):\n\"\"\"Retry operation with exponential backoff\"\"\"\noriginal_level = logger.level\nlogger.setLevel(logging.DEBUG)\nlast_exception = None\nfor attempt in range(attempts):\ntry:\nreturn operation()\nexcept Exception as e:\nlogger.debug(f\"Attempt {attempt+1} failed with error: {str(e)}\")\nlast_exception = e\ndelay = base_delay * (2**attempt)\nlogger.debug(f\"Retrying in {delay} second.\")\ntime.sleep(delay)\nfinally:\nlogger.setLevel(original_level)\n\nlogger.setLevel(original_level)\nraise Exception(f\"Failed after {attempts} attempts: {str(last_exception)}\")\n\n\ndef validate_non_empty(output):\nif not output or len(output) == 0:\nraise Exception(\"output cannot be empty\")\nreturn output\n\n\n# converts camelcase string to underscore seperated\ndef _identity(name):\nreturn name\n\n\ndef _camel_to_underscore(name):\nif name[0].isupper():\nname = name[0].lower() + name[1:]\n\nname = re.sub(\"([A-Z])\", lambda match: \"_\" + match.group(1).lower(), name)\nreturn name\n\n\ndef _to_camel(string):\ncomponents = string.split(\"_\")\nreturn components[0] + \"\".join(x.title() for x in components[1:])\n\n\ndef evaluation_metrics() -> list[str]:\nreturn [enum.value for enum in EvaluationMetrics]\n\n\ndef get_default_parameters(algorithm):\nparameters = json_format.MessageToDict(default_configs.default_configs[algorithm])\nreturn parameters\n\n\ndef ml_modelling(\nmodel_name: str,\nmodel_configs: list[dict],\nlabel_dataset: str,\nlabel_field: str,\ndataset_id: str,\nnumerical_fields: list[str] | None = None,\ncategorical_fields: list[str] | None = None,\ntextual_fields: list[str] | None = None,\nimaginal_fields: list[str] | None = None,\ntime_field: str | None = None,\npredict_workers: int = 1,\nupdate_batch_size: int = 64,\nsynchronization_method: str = \"\",\npredict_only=False,\ninitial_model_id=\"\",\n):\n\"\"\"Perform machine learning modeling based on specified configurations.\n\nThis function sends a POST request to the server to initiate machine\nlearning modeling using the provided parameters.\n\nArgs:\nmodel_configs (list[dict]): List of model configs (model parameters)\nmodel_name (str): Name of the machine learning model.\nlabel_dataset (str): dataset_id related to the label data.\nlabel_field (str): Name of the column containing label data.\ndataset_id (str): Dataset related to the input data.\nnumerical_fields (list[str], optional): List of numeric fields used in the model.\ncategorical_fields (list[str], optional): List of categorical fields used in the model.\ntextual_fields (list[str], optional): List of textual fields used in the model.\nimaginal_fields (list[str], optional): List of imaginal fields used in the model.\ntime_field (str, optional): The time field used in the model configuration.\npredict_workers (int, optional): The number of threads for prediction.\nupdate_batch_size (int, optional): The update frequency for models.\nsynchronization_method (str, optional): Synchronization method to use. One of \"\" or \"_lr\".\npredict_only (bool, optional): Should this model only be used for prediction.\ninitial_model_id (str, optional): Model id for deploying a batch trained model\n\nRaises:\nException: Raises an exception if the POST request fails, providing details\nfrom the response JSON.\n\"\"\"\nif categorical_fields is None:\ncategorical_fields = []\nif numerical_fields is None:\nnumerical_fields = []\nif textual_fields is None:\ntextual_fields = []\nif imaginal_fields is None:\nimaginal_fields = []\n\nif label_dataset != \"\" and label_field != \"\":\nlabel = LabelAssociation(\ndataset_id=label_dataset,\nfield=label_field,\n)\nelse:\nraise Exception(\"Both label_dataset and label_field must be provided\")\n\npayload = MLModellingRequest(\nid=model_name,\ndataset_id=dataset_id,\nmodel_configs=model_configs,\nnumerical_fields=numerical_fields,\ncategorical_fields=categorical_fields,\ntextual_fields=textual_fields,\nimaginal_fields=imaginal_fields,\ntime_field=time_field,\nlabel=label,\nlearner_config=LearnerConfig(\npredict_workers=predict_workers,\nupdate_batch_size=update_batch_size,\nsynchronization_method=synchronization_method,\n),\npredict_only=predict_only,\ninitial_model_id=initial_model_id,\n)\n\napi.get(endpoint=\"model_validation\", json=payload.model_dump())\napi.post(endpoint=\"ml_modelling\", json=payload.model_dump())\n\n\ndef _resolve_duplicate_columns(\ninput_df: pd.DataFrame, label_df: pd.DataFrame, key_field: str\n):\n# Drop any common columns between the two from inputs\n# In the absence of this pandas will rename the conflicting columns to <col>_x <col>_y instead\n# Note that we drop from inputs instead of labels since the label dataframe is only supposed to have\n# the label and key fields, so a conflict would indicate that the label field made its way into the input.\nfor col in label_df.columns:\nif col == key_field:\ncontinue\nif col in input_df.columns:\nlogger.warn(\nf\"Duplicate column '{col}' in input and label df. Dropping column from inputs\"\n)\ninput_df = input_df.drop(columns=[col])\nreturn input_df, label_df\n\n\ndef _prepare_merged_df(input: LocalInputs, labels: LocalLabels):\n\"\"\"\nIt resolves duplicate columns, and merges the input and label dataframes on the key field.\n\"\"\"\ninput_df, label_df = _resolve_duplicate_columns(\ninput.dataframe, labels.dataframe, input.key_field\n)\nmerged_df = pd.merge(input_df, label_df, on=input.key_field)\nreturn merged_df\n\n\ndef model_learn(\nmodel_name: str,\nmerged_df: pd.DataFrame,\nkey_field: str,\nlabel_field: str,\nnumerical_fields: Optional[list[str]] = None,\ncategorical_fields: Optional[list[str]] = None,\ntextual_fields: Optional[list[str]] = None,\nimaginal_fields: Optional[list[str]] = None,\ntime_field: Optional[str] = None,\ninitial_model_key: str | None = None,\nmodel_configs: Optional[list[dict[str, str]]] = None,\nepochs: int = 1,\n):\nif initial_model_key == \"\" and model_configs is None:\nraise Exception(\"initial_model_key and model_configs both can't be empty.\")\n\n# Normalize\nif categorical_fields is None:\ncategorical_fields = []\nif numerical_fields is None:\nnumerical_fields = []\nif textual_fields is None:\ntextual_fields = []\nif imaginal_fields is None:\nimaginal_fields = []\nif time_field is None:\ntime_field = \"\"\nif model_configs is None:\nmodel_configs = []\nif initial_model_key == \"\":\ninitial_model_key = None\n\ninput_spec = InputSpec(\nkey_field=key_field,\ntime_field=time_field,\nnumerical_fields=numerical_fields,\ncategorical_fields=categorical_fields,\ntextual_fields=textual_fields,\nimaginal_fields=imaginal_fields,\nlabel_field=label_field,\n)\n\nif initial_model_key is None:\nmodel_params = ModelParams(\nmodel_configs=model_configs,\nnumerical_fields=numerical_fields,\ncategorical_fields=categorical_fields,\ntextual_fields=textual_fields,\nimaginal_fields=imaginal_fields,\ntime_field=time_field,\nlabel=LabelAssociation(field=label_field, dataset_id=key_field),\n)\nelse:\nmodel_params = None\n\nversion_name = _save_model_configs_with_random_version(\nmodel_name, initial_model_key, model_params\n)\n\n# Send our training data to the server\nclient = pyarrow.flight.connect(f\"{CONFIG.ARROW_SERVER_ADDRESS}\")\n\ninput_table = TbPyArrow.df_to_table(merged_df, input_spec)\n\nupload_descriptor = pyarrow.flight.FlightDescriptor.for_command(\nf\"learn:{model_name}:{version_name}\"\n)\noptions = pyarrow.flight.FlightCallOptions(headers=api.arrow_headers)\n\nTbPyArrow._put_and_retry(\nclient, upload_descriptor, options, input_table, epochs=epochs\n)\nreturn version_name\n\n\ndef _save_model_configs_with_random_version(\nmodel_name: str,\ninitial_model_key: str | None,\nmodel_params: ModelParams | None,\n):\nversion_name = \"\".join(random.choices(string.ascii_lowercase, k=10))\npayload = ModelConfigStorageRequest(\nid=model_name,\nversion=version_name,\ninitial_model_key=initial_model_key,\nparams=model_params,\n)\nres = api.post(endpoint=\"train_config\", json=payload.model_dump())\nif res.status_code != 201:\nraise Exception(f\"Failed to save train config: {res.json()['detail']}\")\nreturn version_name\n\n\ndef model_predict(\nmodel_name: str,\ninitial_model_key: str,\ninput_df: pd.DataFrame,\nkey_field: str,\nnumerical_fields: Optional[list[str]] = None,\ncategorical_fields: Optional[list[str]] = None,\ntextual_fields: Optional[list[str]] = None,\nimaginal_fields: Optional[list[str]] = None,\ntime_field: Optional[str] = None,\n):\nif model_name == \"\" or initial_model_key == \"\":\nraise ValueError(\"model_name and initial_model_key cannot be empty\")\n\n# Normalize\nif categorical_fields is None:\ncategorical_fields = []\nif numerical_fields is None:\nnumerical_fields = []\nif time_field is None:\ntime_field = \"\"\nif textual_fields is None:\ntextual_fields = []\nif imaginal_fields is None:\nimaginal_fields = []\n\n# Send our inputs to the server, get back the predictions\nclient = pyarrow.flight.connect(f\"{CONFIG.ARROW_SERVER_ADDRESS}\")\n\ninput_spec = InputSpec(\nkey_field=key_field,\ntime_field=time_field,\nnumerical_fields=numerical_fields,\ncategorical_fields=categorical_fields,\ntextual_fields=textual_fields,\nimaginal_fields=imaginal_fields,\nlabel_field=\"\",  # will be ignored by df_to_table below\n)\ninput_table = TbPyArrow.df_to_table(input_df, input_spec)\n\nrequest_id = \"\".join(random.choices(string.ascii_lowercase, k=10))\nupload_descriptor = pyarrow.flight.FlightDescriptor.for_command(\nf\"predict:{request_id}:{model_name}:{initial_model_key}\"\n)\noptions = pyarrow.flight.FlightCallOptions(headers=api.arrow_headers)\nread_table = TbPyArrow._exchange_and_retry(\nclient, upload_descriptor, options, input_table\n)\nreturn TbPyArrow.arrow_table_to_pandas(read_table)\n\n\ndef get_score_for_model(\ntmp_model: Model,\ninput_table: pa.Table,\ninput_spec: InputSpec,\nlabels: LocalLabels,\nperf_metric: metrics._scorer._Scorer,\nprediction_column: str,\n):\nif not tmp_model.model_id:\ntmp_model.model_id = \"\".join(random.choices(string.ascii_lowercase, k=10))\ninitial_model_key = tmp_model.version\nmodel_configs = tmp_model.get_model_config()\n\nif initial_model_key == \"\" and model_configs is None:\nraise Exception(\"initial_model_key and model_configs both can't be empty.\")\n\nif model_configs is None:\nmodel_configs = []\nif initial_model_key == \"\":\ninitial_model_key = None\n\nlabel = LabelAssociation(field=labels.label_field, dataset_id=input_spec.key_field)\nif initial_model_key is None:\nmodel_params = ModelParams(\nmodel_configs=model_configs,\nnumerical_fields=input_spec.numerical_fields,\ncategorical_fields=input_spec.categorical_fields,\ntextual_fields=input_spec.textual_fields,\nimaginal_fields=input_spec.imaginal_fields,\ntime_field=input_spec.time_field,\nlabel=label,\n)\nelse:\nmodel_params = None\ntmp_model.version = _save_model_configs_with_random_version(\ntmp_model.model_id, initial_model_key, model_params\n)\n# Send our training data to the server\nclient = pyarrow.flight.connect(f\"{CONFIG.ARROW_SERVER_ADDRESS}\")\n\nupload_descriptor = pyarrow.flight.FlightDescriptor.for_command(\nf\"learn:{tmp_model.model_id}:{tmp_model.version}\"\n)\noptions = pyarrow.flight.FlightCallOptions(headers=api.arrow_headers)\nTbPyArrow._put_and_retry(client, upload_descriptor, options, input_table)\nrequest_id = \"\".join(random.choices(string.ascii_lowercase, k=10))\nupload_descriptor = pyarrow.flight.FlightDescriptor.for_command(\nf\"predict:{request_id}:{tmp_model.model_id}:{tmp_model.version}\"\n)\nread_table = TbPyArrow._exchange_and_retry(\nclient, upload_descriptor, options, input_table\n)\ntemp_outputs = TbPyArrow.arrow_table_to_pandas(read_table)\nscore = perf_metric._score_func(\nlabels.dataframe[labels.label_field], temp_outputs[prediction_column]\n)\nreturn tmp_model, score\n\n\ndef validate_model_configs(model_configs: list[dict], input_spec: InputSpec):\npayload = ModelParams(\nmodel_configs=model_configs,\nlabel=LabelAssociation(\nfield=input_spec.label_field,\ndataset_id=input_spec.key_field,\n),\nnumerical_fields=input_spec.numerical_fields,\ncategorical_fields=input_spec.categorical_fields,\ntextual_fields=input_spec.textual_fields,\nimaginal_fields=input_spec.imaginal_fields,\ntime_field=input_spec.time_field,\n)\n\nresp = api.get(endpoint=\"model_validation\", json=payload.model_dump())\nreturn resp.json()[\"message\"]\n\n\nclass DeployedModel(BaseModel):\nmodel_name: str\nmodel_instance: Model\nalgorithm: str\nmodel_configs: list[dict]\n\nclass Config:\narbitrary_types_allowed = True\nprotected_namespaces = ()\n\ndef __init__(self, **data):\nsuper().__init__(**data)\napi.get(f\"model/{self.model_name}/info\")\n\ndef pause(self) -> None:\n\"\"\"Pauses a running model.\"\"\"\napi.patch(\nendpoint=f\"model/{self.model_name}\",\njson=ModelPatchRequest(action=\"pause\").model_dump(mode=\"json\"),\n)\n\ndef resume(self) -> None:\n\"\"\"Resumes a paused model or does nothing if model is already running.\"\"\"\napi.patch(\nendpoint=f\"model/{self.model_name}\",\njson=ModelPatchRequest(action=\"resume\").model_dump(mode=\"json\"),\n)\n\ndef delete(self, delete_output_topic: bool = True) -> None:\n\"\"\"Delete the model.\n\nArgs:\ndelete_output_topic (bool, optional): Delete output dataset. Defaults to True.\n\"\"\"\napi.delete(\nendpoint=f\"model/{self.model_name}\",\njson=ModelDeleteRequest(delete_output_topic=delete_output_topic).model_dump(\nmode=\"json\"\n),\n)\n\ndef add_metric(self, metric_name) -> None:\npayload = MetricRegistrationRequest(\nmetric=metric_name,\n)\napi.post(\nendpoint=f\"model/{self.model_name}/metric\",\njson=payload.model_dump(),\n)\n\ndef add_drift(self) -> None:\napi.put(endpoint=f\"model/{self.model_name}/target_drift\")\n\ndef get_drifts(self, limit: int = -1) -> list:\nreturn get_proto_msgs(\nStreamType.TARGET_DRIFT,\nself.model_name,\noutput_pb2.Output,\n# limit\n)\n\ndef get_outputs(self, limit: int = -1) -> list:\nif _istest():\nreturn retry_operation(\nlambda: validate_non_empty(\nget_proto_msgs(\nStreamType.OUTPUT,\nself.model_name,\noutput_pb2.Output,\n# limit\n)\n),\n)\nreturn get_proto_msgs(\nStreamType.OUTPUT,\nself.model_name,\noutput_pb2.Output,\n# limit\n)\n\ndef get_evaluation(\nself,\nmetric_name: str,\nfilter_expression: str = \"\",\nwindow_size: int = 1000,\nlimit: int = 100000,\noutput_type: Evaluations.ModelOutputType = Evaluations.ModelOutputType.SCORE,\n) -> list:\n\"\"\"Fetch model evaluation data for the given metric.\n\nThis function sends a POST request to the server to get model evaluation data\nusing the provided parameters.\n\nArgs:\nmetric_name (str): Evaluation metric to use.\nfilter_expression (str): Filter expression for metric calculation, should be a valid SQL expression.\nFields can be `processing_time` or any of the model `input_data` or `output_data` columns used as\n`input_data.input_column1`,\n`output_data.score`,\n`output_data.predicted_class`,\n`output_data.class_probabilities[1]`,\n`output_data.feature_score[2]` etc...\neg: `input_data.input1 > 100 AND (output_data.score > 0.5 OR output_data.feature_score[1] > 0.3)`,\n`processing_time between '2024-12-31 15:42:38.425000' AND '2024-12-31 15:42:44.603000'`\nwindow_size (int): Window size to use for metric calculation.\nlimit (int): Limit value for evaluation data response.\noutput_type (`Evaluations.ModelOutputType`): Output type to use for response.\n\nRaises:\nException: Raises an exception if the POST request fails, providing details\nfrom the response JSON.\n\"\"\"\npayload = Evaluations(\nmodel_names=[self.model_name],\nmetric=metric_name,\nfilter_expression=filter_expression,\nwindow_size=window_size,\nlimit=limit,\noutput_type=output_type,\n)\n\nif _istest():\nresponse = retry_operation(\nlambda: validate_non_empty(\napi.post(\nendpoint=\"model/evaluations\",\njson=payload.model_dump(),\n).json()\n),\n)\nelse:\nresponse = api.post(\nendpoint=\"model/evaluations\",\njson=payload.model_dump(),\n).json()\n\nif len(response) == 0:\nreturn []\n\nfirst_element = response[0]\n\nindex_value_pairs = list(\nzip(first_element[\"index\"], first_element[\"values\"], strict=True)\n)\nreturn [\nmetrics_pb2.Metrics(index=index, metric=metric)\nfor index, metric in index_value_pairs\n]\n\ndef get_endpoints(self):\nresp = api.get(f\"model/{self.model_name}/info\").json()\ninfo = ModelInfo(**resp)\n\nbase_url = CONFIG.TURBOML_BACKEND_SERVER_ADDRESS\nreturn [\nurlparse.urljoin(base_url, endpoint) for endpoint in info.endpoint_paths\n]\n\ndef get_logs(self):\nreturn ProcessOutput(**api.get(f\"model/{self.model_name}/logs\").json())\n\ndef get_inference(self, df: pd.DataFrame) -> pd.DataFrame:\nmodel_info = api.get(f\"model/{self.model_name}/info\").json()\nmodel_info = ModelInfo(**model_info)\ndf = PandasHelpers.normalize_df(df)\ninput_spec = model_info.metadata.get_input_spec()\ndf_with_engineered_features = retrieve_features(\nmodel_info.metadata.input_db_source, df\n)\ntable = TbPyArrow.df_to_table(df_with_engineered_features, input_spec)\n\nclient = pyarrow.flight.connect(f\"{CONFIG.ARROW_SERVER_ADDRESS}\")\n\nrequest_id = \"\".join(random.choices(string.ascii_lowercase, k=10))\nmodel_port = model_info.metadata.process_config[\"arrowPort\"]\ncommand_str = f\"relay:{self.model_name}:{request_id}:{model_port}\"\nupload_descriptor = pyarrow.flight.FlightDescriptor.for_command(command_str)\noptions = pyarrow.flight.FlightCallOptions(headers=api.arrow_headers)\nread_table = TbPyArrow._exchange_and_retry(\nclient, upload_descriptor, options, table\n)\n\nreturn TbPyArrow.arrow_table_to_pandas(read_table)\n\ndef __getattr__(self, name):\nreturn getattr(self.model_instance, name)\n\n\nclass Model(ABC, BaseModel):\nmodel_id: str = Field(default=None, exclude=True)\nversion: str = Field(default=\"\", exclude=True)\n\nclass Config:\nextra = \"forbid\"\nprotected_namespaces = ()\nvalidate_assignment = True\n\ndef __init__(self, **data):\ntry:\nsuper().__init__(**data)\nexcept ValidationError as e:\nfor error in e.errors():\nif error[\"type\"] == \"extra_forbidden\":\nextra_field = error[\"loc\"][0]\nraise Exception(\nf\"{extra_field} is not a field in {self.__class__.__name__}\"\n) from e\nraise e\n\ndef get_model_config(self):\nparams = self.model_dump(by_alias=True)\nparams[\"algorithm\"] = self.__class__.__name__\nreturn [params]\n\ndef learn(self, input: LocalInputs, labels: LocalLabels, epochs: int = 1):\n\"\"\"\nTrains the model on provided input data and labels for the specified number of epochs.\n\nParameters:\ninput (Inputs): Contains input data.\nlabels (Labels): Contains target labels.\nepochs (int, optional): No. of times to iterate over the dataset during training. Defaults to 1.\n- Note: Currently, data is processed in sequential order for each epoch.\nUsers who need shuffling or sampling should modify the input data before calling learn method.\nThese features may be added in the future.\n\nReturns:\nModel: A new model instance trained on the provided data.\n\"\"\"\nif not self.model_id:\nself.model_id = \"\".join(random.choices(string.ascii_lowercase, k=10))\n\nmerged_df = _prepare_merged_df(input, labels)\n\nversion_name = model_learn(\nmodel_name=self.model_id,\nmerged_df=merged_df,\nkey_field=input.key_field,\nlabel_field=labels.label_field,\nnumerical_fields=input.numerical_fields,\ncategorical_fields=input.categorical_fields,\ntextual_fields=input.textual_fields,\nimaginal_fields=input.imaginal_fields,\ntime_field=input.time_field,\ninitial_model_key=self.version,\nmodel_configs=self.get_model_config(),\nepochs=epochs,\n)\n\ntrained_model = self.model_copy()\ntrained_model.version = version_name\n\nreturn trained_model\n\ndef predict(self, input: LocalInputs):\nif self.model_id is None:\nraise Exception(\"The model is untrained.\")\nreturn model_predict(\nmodel_name=self.model_id,\ninitial_model_key=self.version,\ninput_df=input.dataframe,\nkey_field=input.key_field,\nnumerical_fields=input.numerical_fields,\ncategorical_fields=input.categorical_fields,\ntextual_fields=input.textual_fields,\nimaginal_fields=input.imaginal_fields,\ntime_field=input.time_field,\n)\n\ndef deploy(\nself, name: str, input: OnlineInputs, labels: OnlineLabels, predict_only=False\n) -> DeployedModel:\nif self.model_id:\ninitial_model_id = f\"{api.namespace}.{self.model_id}:{self.version}\"\nelse:\ninitial_model_id = \"\"\n\nif not isinstance(input, OnlineInputs) or not isinstance(labels, OnlineLabels):\nexplaination = \"\"\nif isinstance(input, LocalInputs) or isinstance(labels, LocalLabels):\nexplaination = (\n\" It looks like you are trying to deploy a model based on a local dataset.\"\n\" Please use OnlineDataset.from_local() to register your dataset with the\"\n\" platform before deploying the model.\"\n)\nraise ValueError(\n\"Inputs/labels must be an OnlineInputs/OnlineLabels object obtained from Online datasets.\"\nf\"{explaination}\"\n)\n\nmodel_configs = self.get_model_config()\n\nml_modelling(\nmodel_name=name,\nmodel_configs=model_configs,\nlabel_dataset=labels.dataset_id if labels else \"\",\nlabel_field=labels.label_field if labels else \"\",\n# QUESTION: here input.dataset_id can be None. Are we\n# allowing deployment without input dataset_ids or should\n# we complain?\ndataset_id=input.dataset_id,\nnumerical_fields=input.numerical_fields,\ncategorical_fields=input.categorical_fields,\ntextual_fields=input.textual_fields,\nimaginal_fields=input.imaginal_fields,\ntime_field=input.time_field,\npredict_only=predict_only,\ninitial_model_id=initial_model_id,\n)\n\nreturn DeployedModel(\nmodel_name=name,\nmodel_instance=self,\nalgorithm=self.__class__.__name__,\nmodel_configs=model_configs,\n)\n\ndef set_params(self, model_configs: list[dict]) -> None:\nmodel_config = model_configs[0]\ndel model_config[\"algorithm\"]\nfor key, value in model_config.items():\nsetattr(self, _camel_to_underscore(key), value)\n\n@staticmethod\ndef _construct_model(\nconfigs: list, index: int = 0, is_flat: bool = False\n) -> tuple[Model | None, int]:\n\"\"\"\nReturn (model_instance, next_config_index)\n\"\"\"\nif index >= len(configs):\nreturn None, index\nconfig = configs[index]\nalgorithm = config[\"algorithm\"]\nmodel_class = globals()[algorithm]\nmodel_instance = model_class.construct()\nspecific_config_dict = {k: v for k, v in config.items() if k != \"algorithm\"}\nconvert_func = _identity\nnum_children_key = \"num_children\"\nif not is_flat:\nconvert_func = _camel_to_underscore\nnum_children_key = \"numChildren\"\nspecific_config_dict = list(specific_config_dict.values())[0]\n\nfor key, value in specific_config_dict.items():\nsetattr(model_instance, convert_func(key), value)\n\nnum_children = specific_config_dict.get(num_children_key, 0)\nif num_children > 0:\nnext_index = index + 1\n\nif \"base_model\" in model_class.__fields__:\n# For models with a single base_model\nbase_model, next_index = Model._construct_model(\nconfigs, next_index, is_flat\n)\nif base_model:\nmodel_instance.base_model = base_model\nmodel_instance.num_children = 1\nelif \"base_models\" in model_class.__fields__:\n# For models with multiple base_models\nbase_models = []\nfor _ in range(num_children):\nchild_model, next_index = Model._construct_model(\nconfigs, next_index, is_flat\n)\nif child_model:\nbase_models.append(child_model)\nmodel_instance.base_models = base_models\nmodel_instance.num_children = len(base_models)\nelse:\nnext_index = index + 1\n\nreturn model_instance, next_index\n\n@staticmethod\ndef _flatten_model_config(model):\n\"\"\"\nRecreate flattened model configs\n\"\"\"\nconfig = model.model_dump(by_alias=True)\nconfig[\"algorithm\"] = model.__class__.__name__\nflattened = [config]\n\nif hasattr(model, \"base_models\"):\nfor base_model in model.base_models:\nflattened.extend(Model._flatten_model_config(base_model))\nelif hasattr(model, \"base_model\") and model.base_model:\nflattened.extend(Model._flatten_model_config(model.base_model))\n\nreturn flattened\n\n@staticmethod\ndef retrieve_model(model_name: str) -> DeployedModel:\ntry:\nresp = api.get(f\"model/{model_name}/info\")\nexcept Exception as e:\nlogger.error(f\"Error fetching model: {e!r}\")\nraise\n\nmodel_meta = ModelInfo(**resp.json()).metadata\nprocess_config = model_meta.process_config\nmodel_configs = process_config.get(\"modelConfigs\", [])\nif not model_configs:\nraise ValueError(\"No model configurations found in the API response\")\n\nroot_model, _ = Model._construct_model(model_configs)\n\nflattened_configs = Model._flatten_model_config(root_model)\ndeployed_model = DeployedModel(\nmodel_name=model_name,\nmodel_instance=root_model,\nalgorithm=root_model.__class__.__name__,\nmodel_configs=flattened_configs,\n)\n\nreturn deployed_model\n\ndef to_local_model(self, input_spec: InputSpec) -> LocalModel:\n\"\"\"\nConverts the current Model instance into a LocalModel instance.\n\"\"\"\n## TODO: Shouldn't we be retrieving the latest model snapshot from the server?\nparams = self.model_dump(by_alias=True)\nconfig_key = default_configs.algo_config_mapping.get(self.__class__.__name__)\n\nif config_key:\nparams = {\n_to_camel(config_key): params,\n\"algorithm\": self.__class__.__name__,\n}\n\nreturn LocalModel(model_configs=[params], input_spec=input_spec)\n\n\nclass LocalModel(BaseModel):\n\"\"\"\nLocalModel allows for local training and prediction using Python bindings\nto the underlying C++ code.\n\"\"\"\n\nmodel_configs: List[dict]\ninput_spec: InputSpec\ncpp_model: CppModel = Field(default=None)\n\nmodel_config = ConfigDict(arbitrary_types_allowed=True)\n\n@field_serializer(\"cpp_model\")\ndef serialize_cpp_model(self, cpp_model: CppModel) -> str:\nif cpp_model is None:\nreturn None\nmodel_bytes = cpp_model.serialize()\nmodel_base64 = base64.b64encode(model_bytes).decode(\"utf-8\")\nreturn model_base64\n\n@field_validator(\"cpp_model\", mode=\"before\")\n@classmethod\ndef deserialize_cpp_model(cls, value):\nif value is None:\nreturn None\nif isinstance(value, CppModel):\nreturn value\nif isinstance(value, str):\nmodel_bytes = base64.b64decode(value.encode(\"utf-8\"))\ncpp_model = CppModel.deserialize(model_bytes)\nreturn cpp_model\nraise ValueError(\"Invalid type for cpp_model\")\n\ndef __init__(self, **data):\nsuper().__init__(**data)\n\nif self.cpp_model is None:\n# Serialize the model_configs to JSON\nconfig_json = json.dumps({\"model_configs\": self.model_configs})\n\n# Prepare the input configuration\ninput_config = {\n\"keyField\": self.input_spec.key_field,\n\"time_tick\": self.input_spec.time_field or \"\",\n\"numerical\": self.input_spec.numerical_fields or [],\n\"categorical\": self.input_spec.categorical_fields or [],\n\"textual\": self.input_spec.textual_fields or [],\n\"imaginal\": self.input_spec.imaginal_fields or [],\n}\ninput_config_json = json.dumps(input_config)\n\n# Create the cpp_model using create_model_from_config\nself.cpp_model = create_model_from_config(config_json, input_config_json)\n\ndef learn_one(self, input: dict, label: int):\n\"\"\"\nLearn from a single data point.\n\"\"\"\ninput_data = self._dict_to_input_data(input)\ninput_data.label = label\nself.cpp_model.learn_one(input_data)\n\ndef predict_one(self, input: dict) -> OutputData:\n\"\"\"\nPredict for a single data point.\n\"\"\"\ninput_data = self._dict_to_input_data(input)\noutput = self.cpp_model.predict_one(input_data)\nreturn output\n\ndef _dict_to_input_data(self, input: dict) -> InputData:\n\"\"\"\nConverts a dictionary of input data to an InputData object.\n\"\"\"\ninput_data = InputData()\nif self.input_spec.key_field in input:\ninput_data.key = str(input[self.input_spec.key_field])\nelse:\ninput_data.key = \"\"\nif self.input_spec.time_field and self.input_spec.time_field in input:\ntime_value = input[self.input_spec.time_field]\nif isinstance(time_value, pd.Timestamp):\ninput_data.time_tick = int(time_value.timestamp())\nelif isinstance(time_value, datetime.datetime):\ninput_data.time_tick = int(time_value.timestamp())\nelse:\ninput_data.time_tick = int(time_value)\nelse:\ninput_data.time_tick = 0\ninput_data.numeric = [\nfloat(input[col])\nfor col in self.input_spec.numerical_fields\nif col in input\n]\ninput_data.categ = [\nint(input[col])\nfor col in self.input_spec.categorical_fields\nif col in input\n]\ninput_data.text = [\nstr(input[col]) for col in self.input_spec.textual_fields if col in input\n]\ninput_data.images = [\nstr(input[col]) for col in self.input_spec.imaginal_fields if col in input\n]\nreturn input_data\n\ndef learn(self, inputs: LocalInputs, labels: LocalLabels):\n\"\"\"\nTrains the model on provided input data and labels.\n\"\"\"\nmerged_df = _prepare_merged_df(inputs, labels)\nfor _, row in merged_df.iterrows():\ninput_dict = row.to_dict()\nlabel = int(row[labels.label_field])\nself.learn_one(input_dict, label)\n\ndef predict(self, inputs: LocalInputs) -> pd.DataFrame:\n\"\"\"\nMakes predictions on provided input data.\n\"\"\"\noutputs = []\nfor _, row in inputs.dataframe.iterrows():\ninput_dict = row.to_dict()\noutput = self.predict_one(input_dict)\noutputs.append(output)\n# Convert outputs to DataFrame\noutput_dicts = []\nfor output in outputs:\noutput_dict = {\n\"score\": output.score(),\n\"predicted_class\": output.predicted_class(),\n\"feature_scores\": output.feature_scores,\n\"class_probabilities\": output.class_probabilities,\n\"text_output\": output.text_output(),\n\"embeddings\": output.embeddings,\n}\noutput_dicts.append(output_dict)\noutput_df = pd.DataFrame(output_dicts)\nreturn output_df\n\ndef serialize(self) -> bytes:\nreturn self.cpp_model.serialize()\n\ndef __eq__(self, other):\nreturn self.cpp_model == other.cpp_model\n\n\ndef is_regressor(model: Model):\nREGRESSOR_CLASSES = [\nHoeffdingTreeRegressor,\nAMFRegressor,\nFFMRegressor,\nSGTRegressor,\nSNARIMAX,\n]\nPREPROCESSOR_CLASSES = [\nMinMaxPreProcessor,\nNormalPreProcessor,\nRobustPreProcessor,\nLlamaCppPreProcessor,\nClipEmbeddingPreprocessor,\nLLAMAEmbedding,\nLabelPreProcessor,\nOneHotPreProcessor,\nTargetPreProcessor,\nFrequencyPreProcessor,\nBinaryPreProcessor,\nImageToNumericPreProcessor,\nRandomSampler,\n]\nif any(isinstance(model, cls) for cls in REGRESSOR_CLASSES):\nreturn True\n\nif isinstance(model, NeuralNetwork):\nreturn True\n\nif isinstance(model, ONN):\nreturn model.n_classes == 1\n\nif any(isinstance(model, cls) for cls in PREPROCESSOR_CLASSES):\n## TODO: Add this assertion for type narrowing. Currently it fails because preprocessors don't inherit from PreProcessor.\n# Also PreProcessor appears mis-named.\n# assert isinstance(model, PreProcessor)\nreturn is_regressor(model.base_model)\n\nreturn False\n\n\ndef is_classifier(model: Model):\nCLASSIFIER_CLASSES = [\nHoeffdingTreeClassifier,\nAMFClassifier,\nFFMClassifier,\nSGTClassifier,\nLeveragingBaggingClassifier,\nHeteroLeveragingBaggingClassifier,\nAdaBoostClassifier,\nHeteroAdaBoostClassifier,\nAdaptiveXGBoost,\nAdaptiveLGBM,\n]\nPREPROCESSOR_CLASSES = [\nMinMaxPreProcessor,\nNormalPreProcessor,\nRobustPreProcessor,\nClipEmbeddingPreprocessor,\nLlamaCppPreProcessor,\nLLAMAEmbedding,\nPreProcessor,\nLabelPreProcessor,\nOneHotPreProcessor,\nTargetPreProcessor,\nFrequencyPreProcessor,\nBinaryPreProcessor,\nImageToNumericPreProcessor,\nRandomSampler,\n]\nif any(isinstance(model, cls) for cls in CLASSIFIER_CLASSES):\nreturn True\n\nif isinstance(model, NeuralNetwork):\nreturn True\n\nif isinstance(model, ONN):\nreturn model.n_classes > 1\n\nif any(isinstance(model, cls) for cls in PREPROCESSOR_CLASSES):\nreturn is_classifier(model.base_model)\n\nreturn False\n\n\nclass RCF(Model):\ntime_decay: float = Field(default=0.000390625)\nnumber_of_trees: int = Field(default=50)\noutput_after: int = Field(default=64)\nsample_size: int = Field(default=256)\n\n\nclass HST(Model):\nn_trees: int = Field(default=20)\nheight: int = Field(default=12)\nwindow_size: int = Field(default=50)\n\n\nclass MStream(Model):\nnum_rows: int = Field(default=2)\nnum_buckets: int = Field(default=1024)\nfactor: float = Field(default=0.8)\n\n\nclass ONNX(Model):\nmodel_save_name: str = Field(default=\"\")\nmodel_config = ConfigDict(protected_namespaces=())\n\n\nclass HoeffdingTreeClassifier(Model):\ndelta: float = Field(default=1e-7)\ntau: float = Field(default=0.05)\ngrace_period: int = Field(default=200)\nn_classes: int\nleaf_pred_method: str = Field(default=\"mc\")\nsplit_method: str = Field(default=\"gini\")\n\n\nclass HoeffdingTreeRegressor(Model):\ndelta: float = Field(default=1e-7)\ntau: float = Field(default=0.05)\ngrace_period: int = Field(default=200)\nleaf_pred_method: str = Field(default=\"mean\")\n\n\nclass AMFClassifier(Model):\nn_classes: int\nn_estimators: int = Field(default=10)\nstep: float = Field(default=1)\nuse_aggregation: bool = Field(default=True)\ndirichlet: float = Field(default=0.5)\nsplit_pure: bool = Field(default=False)\n\n\nclass AMFRegressor(Model):\nn_estimators: int = Field(default=10)\nstep: float = Field(default=1)\nuse_aggregation: bool = Field(default=True)\ndirichlet: float = Field(default=0.5)\n\n\nclass FFMClassifier(Model):\nn_factors: int = Field(default=10)\nl1_weight: float = Field(default=0)\nl2_weight: float = Field(default=0)\nl1_latent: float = Field(default=0)\nl2_latent: float = Field(default=0)\nintercept: float = Field(default=0)\nintercept_lr: float = Field(default=0.01)\nclip_gradient: float = Field(default=1e12)\n\n\nclass FFMRegressor(Model):\nn_factors: int = Field(default=10)\nl1_weight: float = Field(default=0)\nl2_weight: float = Field(default=0)\nl1_latent: float = Field(default=0)\nl2_latent: float = Field(default=0)\nintercept: float = Field(default=0)\nintercept_lr: float = Field(default=0.01)\nclip_gradient: float = Field(default=1e12)\n\n\nclass SGTClassifier(Model):\ndelta: float = Field(default=1e-7)\ngamma: float = Field(default=0.1)\ngrace_period: int = Field(default=200)\nlambda_: float = Field(default=0.1, alias=\"lambda\")\n\n\nclass SGTRegressor(Model):\ndelta: float = Field(default=1e-7)\ngamma: float = Field(default=0.1)\ngrace_period: int = Field(default=200)\nlambda_: float = Field(default=0.1, alias=\"lambda\")\n\n\nclass RandomSampler(Model):\nn_classes: int\ndesired_dist: list = Field(default=[0.5, 0.5])\nsampling_method: str = Field(default=\"mixed\")\nsampling_rate: float = Field(default=1.0)\nseed: int = Field(default=0)\nbase_model: Model = Field(..., exclude=True)\nnum_children: int = Field(default=1)\n\ndef get_model_config(self):\nparams = super().get_model_config()\nparams += self.base_model.get_model_config()\nreturn params\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.base_model = globals()[model_configs[0][\"algorithm\"]].construct()\nself.base_model.set_params(model_configs)\nself.num_children = 1\n\n\nclass NNLayer(BaseModel):\noutput_size: int = 64\nactivation: str = \"relu\"\ndropout: float = 0.3\nresidual_connections: list = []\nuse_bias: bool = True\n\n\nclass NeuralNetwork(Model):\ndropout: int = Field(default=0)\nlayers: list[NNLayer] = Field(\ndefault_factory=lambda: [\nNNLayer(),\nNNLayer(),\nNNLayer(output_size=1, activation=\"sigmoid\"),\n]\n)\nloss_function: str = Field(default=\"mse\")\nlearning_rate: float = 1e-2\noptimizer: str = Field(default=\"sgd\")\nbatch_size: int = 64\n\n@validator(\"layers\")\ndef validate_layers(cls, layers):\nif len(layers) == 0:\nraise Exception(\"layers must be non empty\")\n\n## TODO other layer checks\nreturn layers\n\n\nclass Python(Model):\nmodule_name: str = \"\"\nclass_name: str = \"\"\nvenv_name: str = \"\"\n\n\nclass ONN(Model):\nmax_num_hidden_layers: int = Field(default=10)\nqtd_neuron_hidden_layer: int = Field(default=32)\nn_classes: int\nb: float = Field(default=0.99)\nn: float = Field(default=0.01)\ns: float = Field(default=0.2)\n\n\nclass OVR(Model):\nn_classes: int\nbase_model: Model = Field(..., exclude=True)\nnum_children: int = Field(default=1)\n\ndef get_model_config(self):\nparams = super().get_model_config()\nparams += self.base_model.get_model_config()\nreturn params\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.base_model = globals()[model_configs[0][\"algorithm\"]].construct()\nself.base_model.set_params(model_configs)\nself.num_children = 1\n\n\nclass MultinomialNB(Model):\nn_classes: int\nalpha: float = Field(default=1.0)\n\n\nclass GaussianNB(Model):\nn_classes: int\n\n\nclass AdaptiveXGBoost(Model):\nn_classes: int\nlearning_rate: float = Field(default=0.3)\nmax_depth: int = Field(default=6)\nmax_window_size: int = Field(default=1000)\nmin_window_size: int = Field(default=0)\nmax_buffer: int = Field(default=5)\npre_train: int = Field(default=2)\ndetect_drift: bool = Field(default=True)\nuse_updater: bool = Field(default=True)\ntrees_per_train: int = Field(default=1)\npercent_update_trees: float = Field(default=1.0)\n\n\nclass AdaptiveLGBM(Model):\nn_classes: int\nlearning_rate: float = Field(default=0.3)\nmax_depth: int = Field(default=6)\nmax_window_size: int = Field(default=1000)\nmin_window_size: int = Field(default=0)\nmax_buffer: int = Field(default=5)\npre_train: int = Field(default=2)\ndetect_drift: bool = Field(default=True)\nuse_updater: bool = Field(default=True)\ntrees_per_train: int = Field(default=1)\n\n\nclass PreProcessor(Model):\npreprocessor_name: str\nbase_model: Model = Field(..., exclude=True)\nnum_children: int = Field(default=1)\ntext_categories: list[int] = Field(default=[])\nimage_sizes: list[int] = Field(default=[64, 64, 1])\nchannel_first: bool = Field(default=False)\ngguf_model_id: GGUFModelId = Field(default=None)\nmax_tokens_per_input: int = Field(default=512)\n\ndef get_model_config(self):\nparams = self.model_dump(by_alias=True)\nparams[\"algorithm\"] = \"PreProcessor\"\nparams[\"preprocessor_name\"] = self.preprocessor_name\nmodel_cfgs = [params] + self.base_model.get_model_config()\nreturn model_cfgs\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.base_model.set_params(model_configs)\nself.num_children = 1\n\n\nclass MinMaxPreProcessor(Model):\nbase_model: Model = Field(..., exclude=True)\nnum_children: int = Field(default=1)\n\ndef get_model_config(self):\nparams = self.model_dump(by_alias=True)\nparams[\"algorithm\"] = \"PreProcessor\"\nparams[\"preprocessor_name\"] = \"MinMax\"\nmodel_cfgs = [params] + self.base_model.get_model_config()\nreturn model_cfgs\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.base_model.set_params(model_configs)\nself.num_children = 1\n\n\nclass NormalPreProcessor(Model):\nbase_model: Model = Field(..., exclude=True)\nnum_children: int = Field(default=1)\n\ndef get_model_config(self):\nparams = self.model_dump(by_alias=True)\nparams[\"algorithm\"] = \"PreProcessor\"\nparams[\"preprocessor_name\"] = \"Normal\"\nmodel_cfgs = [params] + self.base_model.get_model_config()\nreturn model_cfgs\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.base_model.set_params(model_configs)\nself.num_children = 1\n\n\nclass RobustPreProcessor(Model):\nbase_model: Model = Field(..., exclude=True)\nnum_children: int = Field(default=1)\n\ndef get_model_config(self):\nparams = self.model_dump(by_alias=True)\nparams[\"algorithm\"] = \"PreProcessor\"\nparams[\"preprocessor_name\"] = \"Robust\"\nmodel_cfgs = [params] + self.base_model.get_model_config()\nreturn model_cfgs\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.base_model.set_params(model_configs)\nself.num_children = 1\n\n\nclass LabelPreProcessor(Model):\ntext_categories: list[int] = Field(default=[])\nbase_model: Model = Field(..., exclude=True)\nnum_children: int = Field(default=1)\n\ndef get_model_config(self):\nparams = self.model_dump(by_alias=True)\nparams[\"algorithm\"] = \"PreProcessor\"\nparams[\"preprocessor_name\"] = \"Label\"\nmodel_cfgs = [params] + self.base_model.get_model_config()\nreturn model_cfgs\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.base_model.set_params(model_configs)\nself.num_children = 1\n\n\nclass OneHotPreProcessor(Model):\ntext_categories: list[int] = Field(default=[])\nbase_model: Model = Field(..., exclude=True)\nnum_children: int = Field(default=1)\n\ndef get_model_config(self):\nparams = self.model_dump(by_alias=True)\nparams[\"algorithm\"] = \"PreProcessor\"\nparams[\"preprocessor_name\"] = \"OneHot\"\nmodel_cfgs = [params] + self.base_model.get_model_config()\nreturn model_cfgs\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.base_model.set_params(model_configs)\nself.num_children = 1\n\n\nclass TargetPreProcessor(Model):\ntext_categories: list[int] = Field(default=[])\nbase_model: Model = Field(..., exclude=True)\nnum_children: int = Field(default=1)\n\ndef get_model_config(self):\nparams = self.model_dump(by_alias=True)\nparams[\"algorithm\"] = \"PreProcessor\"\nparams[\"preprocessor_name\"] = \"Target\"\nmodel_cfgs = [params] + self.base_model.get_model_config()\nreturn model_cfgs\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.base_model.set_params(model_configs)\nself.num_children = 1\n\n\nclass FrequencyPreProcessor(Model):\ntext_categories: list[int] = Field(default=[])\nbase_model: Model = Field(..., exclude=True)\nnum_children: int = Field(default=1)\n\ndef get_model_config(self):\nparams = self.model_dump(by_alias=True)\nparams[\"algorithm\"] = \"PreProcessor\"\nparams[\"preprocessor_name\"] = \"Frequency\"\nmodel_cfgs = [params] + self.base_model.get_model_config()\nreturn model_cfgs\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.base_model.set_params(model_configs)\nself.num_children = 1\n\n\nclass BinaryPreProcessor(Model):\ntext_categories: list[int] = Field(default=[])\nbase_model: Model = Field(..., exclude=True)\nnum_children: int = Field(default=1)\n\ndef get_model_config(self):\nparams = self.model_dump(by_alias=True)\nparams[\"algorithm\"] = \"PreProcessor\"\nparams[\"preprocessor_name\"] = \"Binary\"\nmodel_cfgs = [params] + self.base_model.get_model_config()\nreturn model_cfgs\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.base_model.set_params(model_configs)\nself.num_children = 1\n\n\nclass ImageToNumericPreProcessor(Model):\nimage_sizes: list[int] = Field(default=[64, 64, 1])\nchannel_first: bool = Field(default=False)\nbase_model: Model = Field(..., exclude=True)\nnum_children: int = Field(default=1)\n\ndef get_model_config(self):\nparams = self.model_dump(by_alias=True)\nparams[\"algorithm\"] = \"PreProcessor\"\nparams[\"preprocessor_name\"] = \"ImageToNumeric\"\nmodel_cfgs = [params] + self.base_model.get_model_config()\nreturn model_cfgs\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.base_model.set_params(model_configs)\nself.num_children = 1\n\n\nclass SNARIMAX(Model):\nhorizon: int = Field(default=1)\np: int = Field(default=1)\nd: int = Field(default=1)\nq: int = Field(default=1)\nm: int = Field(default=1)\nsp: int = Field(default=0)\nsd: int = Field(default=0)\nsq: int = Field(default=0)\nbase_model: Model = Field(..., exclude=True)\nnum_children: int = Field(default=1)\n\n@validator(\"base_model\")\ndef validate_base_model(cls, base_model):\nif not is_regressor(base_model):\nraise Exception(\"base_model must be a regressor model\")\nreturn base_model\n\ndef get_model_config(self):\nparams = super().get_model_config()\nparams += self.base_model.get_model_config()\nreturn params\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.base_model = globals()[model_configs[0][\"algorithm\"]].construct()\nself.base_model.set_params(model_configs)\nself.num_children = 1\n\n\nclass HeteroLeveragingBaggingClassifier(Model):\nn_classes: int\nw: float = Field(default=6)\nbagging_method: str = Field(default=\"bag\")\nseed: int = Field(default=0)\nbase_models: list[Model] = Field(..., exclude=True)\nnum_children: int = Field(default=0)\n\n@validator(\"base_models\")\ndef validate_base_models(cls, base_models):\nfor base_model in base_models:\nif not is_classifier(base_model):\nraise Exception(\"all base_models must be classifier models\")\nreturn base_models\n\ndef get_model_config(self):\nparams = super().get_model_config()\nfor base_model in self.base_models:\nparams += base_model.get_model_config()\nreturn params\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.base_models = []\nwhile len(model_configs):\nbase_model = globals()[model_configs[0][\"algorithm\"]].construct()\nbase_model.set_params(model_configs)\nself.base_models.append(base_model)\nself.update_num_children()\n\ndef update_num_children(self):\nself.num_children = len(self.base_models)\n\n@model_validator(mode=\"before\")\n@classmethod\ndef set_num_children(cls, values):\nbase_models = values.get(\"base_models\", [])\nvalues[\"num_children\"] = len(base_models)\nreturn values\n\n\nclass AdaBoostClassifier(Model):\nn_models: int = Field(default=10)\nn_classes: int\nseed: int = Field(default=0)\nbase_model: Model = Field(..., exclude=True)\nnum_children: int = Field(default=0)\n\n@validator(\"base_model\")\ndef validate_base_model(cls, base_model):\nif not is_classifier(base_model):\nraise Exception(\"base_model must be a classifier model\")\nreturn base_model\n\ndef get_model_config(self):\nparams = super().get_model_config()\nparams += self.base_model.get_model_config()\nreturn params\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.base_model = globals()[model_configs[0][\"algorithm\"]].construct()\nself.base_model.set_params(model_configs)\nself.update_num_children()\n\ndef update_num_children(self):\nself.num_children = 1 if self.base_model else 0\n\n@model_validator(mode=\"before\")\n@classmethod\ndef set_num_children(cls, values):\nvalues[\"num_children\"] = 1 if values.get(\"base_model\") else 0\nreturn values\n\n\nclass LeveragingBaggingClassifier(Model):\nn_models: int = Field(default=10)\nn_classes: int\nw: float = Field(default=6)\nbagging_method: str = Field(default=\"bag\")\nseed: int = Field(default=0)\nbase_model: Model = Field(..., exclude=True)\nnum_children: int = Field(default=0)\n\n@validator(\"base_model\")\ndef validate_base_model(cls, base_model):\nif not is_classifier(base_model):\nraise Exception(\"base_model must be a classifier model\")\nreturn base_model\n\ndef get_model_config(self):\nparams = super().get_model_config()\nparams += self.base_model.get_model_config()\nreturn params\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.base_model = globals()[model_configs[0][\"algorithm\"]].construct()\nself.base_model.set_params(model_configs)\nself.update_num_children()\n\ndef update_num_children(self):\nself.num_children = 1 if self.base_model else 0\n\n@model_validator(mode=\"before\")\n@classmethod\ndef set_num_children(cls, values):\nvalues[\"num_children\"] = 1 if values.get(\"base_model\") else 0\nreturn values\n\n\nclass HeteroAdaBoostClassifier(Model):\nn_classes: int\nseed: int = Field(default=0)\nbase_models: list[Model] = Field(..., exclude=True)\nnum_children: int = Field(default=0)\n\n@validator(\"base_models\")\ndef validate_base_models(cls, base_models):\nfor base_model in base_models:\nif not is_classifier(base_model):\nraise Exception(\"all base_models must be classifier models\")\nreturn base_models\n\ndef __init__(self, **data):\nsuper().__init__(**data)\nself.update_num_children()\n\ndef update_num_children(self):\nself.num_children = len(self.base_models)\n\ndef get_model_config(self):\nparams = super().get_model_config()\nfor base_model in self.base_models:\nparams += base_model.get_model_config()\nreturn params\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.base_models = []\nwhile len(model_configs):\nbase_model = globals()[model_configs[0][\"algorithm\"]].construct()\nbase_model.set_params(model_configs)\nself.base_models.append(base_model)\nself.update_num_children()\n\n@model_validator(mode=\"before\")\n@classmethod\ndef set_num_children(cls, values):\nbase_models = values.get(\"base_models\", [])\nvalues[\"num_children\"] = len(base_models)\nreturn values\n\n\nclass BanditModelSelection(Model):\nbandit: str = Field(default=\"EpsGreedy\")\nmetric_name: EvaluationMetrics = Field(default=\"WindowedMAE\")\nbase_models: list[Model] = Field(..., exclude=True)\n\ndef get_model_config(self):\nparams = super().get_model_config()\nfor base_model in self.base_models:\nparams += base_model.get_model_config()\n\nreturn params\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.base_models = []\nwhile len(model_configs):\nbase_model = globals()[model_configs[0][\"algorithm\"]].construct()\nbase_model.set_params(model_configs)\nself.base_models.append(base_model)\n\n\nclass ContextualBanditModelSelection(Model):\ncontextualbandit: str = Field(default=\"LinTS\")\nmetric_name: EvaluationMetrics = Field(default=\"WindowedMAE\")\nbase_models: list[Model] = Field(..., exclude=True)\n\ndef get_model_config(self):\nparams = super().get_model_config()\nfor base_model in self.base_models:\nparams += base_model.get_model_config()\n\nreturn params\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.base_models = []\nwhile len(model_configs):\nbase_model = globals()[model_configs[0][\"algorithm\"]].construct()\nbase_model.set_params(model_configs)\nself.base_models.append(base_model)\n\n\nclass RandomProjectionEmbedding(Model):\nn_embeddings: int = Field(default=2)\ntype_embedding: str = Field(default=\"Gaussian\")\n\n\nclass ClipEmbeddingPreprocessor(Model):\nbase_model: Model = Field(..., exclude=True)\nnum_children: int = Field(default=1)\ngguf_model_id: GGUFModelId = Field(default=None)\n\ndef get_model_config(self):\nparams = self.model_dump(by_alias=True)\nparams[\"algorithm\"] = \"PreProcessor\"\nparams[\"preprocessor_name\"] = \"ClipEmbeddingPreprocessor\"\nmodel_cfgs = [params] + self.base_model.get_model_config()\nreturn model_cfgs\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.base_model.set_params(model_configs)\nself.num_children = 1\n\n\nclass LlamaCppPreProcessor(Model):\n\"\"\"\nLlamaCppPreProcessor is a preprocessor model that uses the LlamaCpp library to\npreprocess text fields into embeddings, passing them to the base model\nas numerical features.\n\"\"\"\n\nbase_model: Model = Field(..., exclude=True)\nnum_children: int = Field(default=1)\ngguf_model_id: GGUFModelId = Field(default=None)\n\"\"\"\nA model id issued by `tb.llm.acquire_hf_model_as_gguf`.\nIf this is not provided, our default BERT model will be used.\n\"\"\"\nmax_tokens_per_input: int = Field(default=512)\n\"\"\"\nThe maximum number of tokens to consider in the input text.\nTokens beyond this limit will be truncated.\n\"\"\"\n\ndef get_model_config(self):\nparams = self.model_dump(by_alias=True)\nparams[\"algorithm\"] = \"PreProcessor\"\nparams[\"preprocessor_name\"] = \"LlamaCpp\"\nmodel_cfgs = [params] + self.base_model.get_model_config()\nreturn model_cfgs\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.base_model.set_params(model_configs)\nself.num_children = 1\n\n\nclass LlamaTextPreprocess(Model):\nbase_model: Model = Field(..., exclude=True)\nnum_children: int = Field(default=1)\ngguf_model_id: GGUFModelId = Field(default=None)\n\ndef get_model_config(self):\nparams = self.model_dump(by_alias=True)\nparams[\"algorithm\"] = \"PreProcessor\"\nparams[\"preprocessor_name\"] = \"LlamaTextPreprocess\"\nmodel_cfgs = [params] + self.base_model.get_model_config()\nreturn model_cfgs\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.base_model.set_params(model_configs)\nself.num_children = 1\n\n\nclass LLAMAEmbedding(Model):\n\"\"\"\nLLAMAEmbedding is a model that uses the LlamaCpp library to preprocess text fields\ninto embeddings, filling them into the `embeddings` field of the output.\n\"\"\"\n\ngguf_model_id: GGUFModelId = Field(default=None)\n\"\"\"\nA model id issued by `tb.llm.acquire_hf_model_as_gguf`.\nIf this is not provided, our default BERT model will be used.\n\"\"\"\nmax_tokens_per_input: int = Field(default=512)\n\"\"\"\nThe maximum number of tokens to consider in the input text.\nTokens beyond this limit will be truncated.\n\"\"\"\n\n\nclass ClipEmbedding(Model):\ngguf_model_id: GGUFModelId = Field(default=None)\n\n\nclass LlamaText(Model):\ngguf_model_id: GGUFModelId = Field(default=None)\n\n\nclass EmbeddingModel(Model):\nembedding_model: Model = Field(..., exclude=True)\nbase_model: Model = Field(..., exclude=True)\nnum_children: int = Field(default=2)\n\ndef get_model_config(self):\nparams = super().get_model_config()\nparams += self.embedding_model.get_model_config()\nparams += self.base_model.get_model_config()\nreturn params\n\ndef set_params(self, model_configs: list[dict]) -> None:\nsuper().set_params(model_configs)\nself.embedding_model = globals()[model_configs[0][\"algorithm\"]].construct()\nself.embedding_model.set_params(model_configs)\nself.base_model = globals()[model_configs[1][\"algorithm\"]].construct()\nself.base_model.set_params(model_configs[1:])\nself.num_children = 2\n\n\nclass RestAPIClient(Model):\nserver_url: str = Field()\nmax_retries: int = Field(default=3)\nconnection_timeout: int = Field(default=10)\nmax_request_time: int = Field(default=30)\n\n\nclass PythonEnsembleModel(Model):\n\"\"\"\nPythonEnsembleModel manages an ensemble of Python-based models.\n\"\"\"\n\nbase_models: list[Model] = Field(..., exclude=True)\nmodule_name: Optional[str] = Field(default=None)\nclass_name: Optional[str] = Field(default=None)\nvenv_name: Optional[str] = Field(default=None)\n\ndef get_model_config(self):\nensemble_params = {\n\"algorithm\": \"PythonEnsembleModel\",\n\"module_name\": self.module_name,\n\"class_name\": self.class_name,\n\"venv_name\": self.venv_name or \"\",\n}\nconfigs = [ensemble_params]\nfor base_model in self.base_models:\nconfigs.extend(base_model.get_model_config())\nreturn configs\n\ndef set_params(self, model_configs: list[dict]) -> None:\nif not model_configs:\nraise ValueError(\"No configuration provided for PythonEnsembleModel.\")\n\n# Extract ensemble-specific configuration\nensemble_config = model_configs[0]\nif ensemble_config.get(\"algorithm\") != \"PythonEnsembleModel\":\nraise ValueError(\"The first configuration must be for PythonEnsembleModel.\")\n\nself.module_name = ensemble_config.get(\"module_name\", \"\")\nself.class_name = ensemble_config.get(\"class_name\", \"\")\nself.venv_name = ensemble_config.get(\"venv_name\", \"\")\n\n# Initialize base models\nbase_model_configs = model_configs[1:]  # Remaining configs are for base models\nif not base_model_configs:\nraise ValueError(\n\"PythonEnsembleModel requires at least one base model configuration.\"\n)\n\nself.base_models = []\nfor config in base_model_configs:\nalgorithm = config.get(\"algorithm\")\nif not algorithm:\nraise ValueError(\n\"Each base model configuration must include an 'algorithm' field.\"\n)\nmodel_class = globals().get(algorithm)\nif not model_class:\nraise ValueError(f\"Unknown algorithm '{algorithm}' for base model.\")\nbase_model = model_class.construct()\nbase_model.set_params([config])\nself.base_models.append(base_model)\n\n\nclass GRPCClient(Model):\nserver_url: str = Field()\nmax_retries: int = Field(default=3)\nconnection_timeout: int = Field(default=10000)\nmax_request_time: int = Field(default=30000)\n\n```"
    },
    {
        "section": "models.py",
        "content": "# models.py\n-## Location -> root_directory.common\n```python\nfrom __future__ import annotations\nfrom copy import deepcopy\nimport re\nfrom typing import (\nAny,\nOptional,\nTuple,\nLiteral,\nType,\nList,\nUnion,\nAnnotated,\nTYPE_CHECKING,\n)\nfrom enum import StrEnum, Enum, auto\nfrom datetime import datetime, timezone\n\nfrom google.protobuf.descriptor import FieldDescriptor\nfrom pydantic import (\nBase64Bytes,\nBaseModel,\nConfigDict,\nField,\nPositiveInt,\nStringConstraints,\ncreate_model,\nfield_serializer,\nfield_validator,\nvalidator,\nmodel_validator,\nStrictBool,\n)\nfrom pandas._libs.tslibs.timestamps import Timestamp\nimport pandas as pd\nimport numpy as np\n\nfrom turboml.common import dataloader\n\n\nfrom .sources import PostgresSource, FileSource  # noqa: TCH001\n\nif TYPE_CHECKING:\nfrom pydantic.fields import FieldInfo\n\n\nTurboMLResourceIdentifier = Annotated[\nstr,\nStringConstraints(\npattern=r\"^[a-zA-Z0-9_-]+$\",\n## TODO: We need to ensure we're using this type everywhere identifiers are accepted (url/query params!)\n# Otherwise this would break APIs.\n# to_lower=True,\n),\n]\nDatasetId = Annotated[str, StringConstraints(pattern=r\"^[a-zA-Z][a-zA-Z0-9_]*$\")]\n\n# Since our dataset fields are used as protobuf message fields, we need to ensure they're valid\n# protobuf field names. This means they must start with an underscore ('_') or a letter (a-z, A-Z),\n# followed by alphanumeric characters or underscores.\nDatasetField = Annotated[str, StringConstraints(pattern=r\"^[a-zA-Z_][a-zA-Z0-9_]*$\")]\n\nSQLIden = Annotated[str, StringConstraints(pattern=r\"^[a-zA-Z][a-zA-Z0-9_]*$\")]\n\n\nclass SchemaType(StrEnum):\nPROTOBUF = \"PROTOBUF\"\n\n\nclass KafkaConnectDatasetRegistrationRequest(BaseModel):\ndataset_id: DatasetId\nsource: Union[FileSource, PostgresSource]\nkey_field: str\n\n@field_validator(\"dataset_id\")\ndef check_dataset_id(cls, v: str):\n# We use `.` to partition dataset_ids into namespaces, so we don't allow it in dataset names\n# `-` is used as another internal delimiter, so we don't allow it either.\nif not re.match(r\"^[a-zA-Z0-9_]+$\", v):\nraise ValueError(\"Invalid dataset name\")\nreturn v\n\n\nclass Datatype(StrEnum):\n\"\"\"\nData types supported by the TurboML platform, corresponding to protobuf types.\n\"\"\"\n\nINT32 = auto()\nINT64 = auto()\nFLOAT = auto()\nDOUBLE = auto()\nSTRING = auto()\nBOOL = auto()\nBYTES = auto()\n\n## TODO: we support some more types for floats and datetimes...\n\ndef to_protobuf_type(self) -> str:\nreturn str(self).lower()\n\n@staticmethod\ndef from_proto_field_descriptor_type(type_: int) -> Datatype:\nmatch type_:\ncase FieldDescriptor.TYPE_INT32:\nreturn Datatype.INT32\ncase FieldDescriptor.TYPE_INT64:\nreturn Datatype.INT64\ncase FieldDescriptor.TYPE_FLOAT:\nreturn Datatype.FLOAT\ncase FieldDescriptor.TYPE_DOUBLE:\nreturn Datatype.DOUBLE\ncase FieldDescriptor.TYPE_STRING:\nreturn Datatype.STRING\ncase FieldDescriptor.TYPE_BOOL:\nreturn Datatype.BOOL\ncase FieldDescriptor.TYPE_BYTES:\nreturn Datatype.BYTES\ncase _:\nraise ValueError(f\"Unsupported protobuf type: {type_}\")\n\ndef to_pandas_dtype(self) -> str:\n\"\"\"Convert TurboML datatype to pandas dtype that works with astype()\"\"\"\nmatch self:\ncase Datatype.INT32:\nreturn \"int32\"\ncase Datatype.INT64:\nreturn \"int64\"\ncase Datatype.FLOAT:\nreturn \"float32\"\ncase Datatype.DOUBLE:\nreturn \"float64\"\ncase Datatype.STRING:\nreturn \"string\"\ncase Datatype.BOOL:\nreturn \"bool\"\ncase Datatype.BYTES:\nreturn \"bytes\"\ncase _:\nraise ValueError(f\"Unsupported datatype for pandas conversion: {self}\")\n\n@staticmethod\ndef from_pandas_column(column: pd.Series) -> Datatype:\nmatch column.dtype:\ncase np.int32:\nreturn Datatype.INT32\ncase np.int64:\nreturn Datatype.INT64\ncase np.float32:\nreturn Datatype.FLOAT\ncase np.float64:\nreturn Datatype.DOUBLE\ncase np.bool_:\nreturn Datatype.BOOL\ncase np.bytes_:\nreturn Datatype.BYTES\ncase \"string\":\nreturn Datatype.STRING\ncase np.object_:\n# At this point we're not sure of the type: pandas by default\n# interprets both `bytes` and `str` into `object_` columns\nproto_dtype = Datatype._infer_pd_object_col_type(column)\nif proto_dtype is None:\nraise ValueError(f\"Unsupported dtype: {column.dtype}\")\nreturn proto_dtype\ncase _:\nraise ValueError(f\"Unsupported dtype: {column.dtype}\")\n\n@staticmethod\ndef _infer_pd_object_col_type(column: pd.Series) -> Optional[Datatype]:\nfirst_non_na_idx = column.first_valid_index()\nif first_non_na_idx is None:\nreturn None\ntry:\nif (\nisinstance(column.loc[first_non_na_idx], str)\nand column.astype(str) is not None\n):\nreturn Datatype.STRING\nexcept UnicodeDecodeError:\npass\n\ntry:\nif (\nisinstance(column.loc[first_non_na_idx], bytes)\nand column.astype(bytes) is not None\n):\nreturn Datatype.BYTES\nexcept TypeError:\npass\n\nreturn None\n\n\nclass DatasetSchema(BaseModel):\nfields: dict[TurboMLResourceIdentifier, Datatype]\n\ndef __str__(self) -> str:\nreturn f\"{self.__class__.__name__}({', '.join(f'{k}: {v}' for k, v in self.fields.items())})\"\n\n@staticmethod\ndef from_pd(df: pd.DataFrame) -> DatasetSchema:\nfields = {}\nfor column_name in df.columns:\ncolumn = df[column_name]\nassert isinstance(column, pd.Series)\nfields[column_name] = Datatype.from_pandas_column(column)\nreturn DatasetSchema(fields=fields)\n\n@staticmethod\ndef from_protobuf_schema(schema: str, message_name: str | None) -> DatasetSchema:\ncls = dataloader.get_protobuf_class(\nschema=schema,\nmessage_name=message_name,\n)\nif cls is None:\nraise ValueError(\nf\"No matching protobuf message found for message={message_name}, schema={schema}\"\n)\ncolumns = {}\nfor field in cls.DESCRIPTOR.fields:\nname = field.name\nproto_type = Datatype.from_proto_field_descriptor_type(field.type)\ncolumns[name] = proto_type\nreturn DatasetSchema(fields=columns)\n\ndef to_protobuf_schema(self, message_name: str) -> str:\nNEWLINE = \"\\n\"\n\ndef column_to_field_decl(cname: str, ctype: Datatype, idx: int) -> str:\nreturn f\"optional {ctype.to_protobuf_type()} {cname} = {idx};\"\n\nfield_decls = map(\ncolumn_to_field_decl,\nself.fields.keys(),\nself.fields.values(),\nrange(1, len(self.fields) + 1),\n)\nreturn f\"\"\"\nsyntax = \"proto2\";\nmessage {message_name} {{\n{NEWLINE.join(field_decls)}\n}}\n\"\"\"\n\n\nclass DatasetRegistrationRequest(BaseModel):\nclass SchemaFromRegistry(BaseModel):\ntype_: Literal[\"registry\"] = \"registry\"\nkind: Literal[\"protobuf\"] = \"protobuf\"\nmessage_name: str\n\nclass ExplicitSchema(DatasetSchema):\ntype_: Literal[\"explicit\"] = \"explicit\"\n\n@staticmethod\ndef from_pd(df: pd.DataFrame) -> DatasetRegistrationRequest.ExplicitSchema:\nds = DatasetSchema.from_pd(df)\nreturn DatasetRegistrationRequest.ExplicitSchema(**ds.model_dump())\n\ndataset_id: DatasetId\ndata_schema: Union[SchemaFromRegistry, ExplicitSchema] = Field(\ndiscriminator=\"type_\"\n)\nkey_field: str\n\n\nclass RegisteredSchema(BaseModel):\nid: int\nschema_type: SchemaType\nschema_body: str\nmessage_name: str\nnative_schema: DatasetSchema\n\n\nclass DatasetRegistrationResponse(BaseModel):\nregistered_schema: RegisteredSchema\n\n\nclass DatasetSpec(BaseModel):\ndataset_id: DatasetId\nkey: str\n\n\nclass DbColumn(BaseModel):\nname: str\ndtype: str\n\n\nclass Dataset(BaseModel):\nclass JoinInformation(BaseModel):\nsources: tuple[DatasetId, DatasetId]\njoined_on_column_pairs: list[tuple[str, str]]\nprefixes: tuple[SQLIden, SQLIden]\n\nclass Metadata(BaseModel):\nkafka_topic: str\ninput_pb_message_name: str\nrisingwave_source: str\nrisingwave_view: str\njoin_information: Optional[Dataset.JoinInformation] = None\n\nfeature_version: int = 0\nsink_version: int = 0\ntable_columns: list[DbColumn]\nkey: str\nsource_type: str\nmessage_name: str\nfile_proto: str\nsql_feats: dict[str, dict] = Field(default_factory=dict)  ## TODO: type\nagg_feats: dict[str, dict] = Field(default_factory=dict)  ## TODO: type\nudf_feats: dict[str, dict] = Field(default_factory=dict)  ## TODO: type\nudaf_feats: dict[str, dict] = Field(default_factory=dict)  ## TODO: type\nagg_cols_indexes: list[str] = Field(default_factory=list)\nmeta: Metadata = Field()\ntimestamp_fields: dict[str, str] = Field(default_factory=dict)\ndrifts: list[DataDrift] = Field(default_factory=list)\nibis_feats: list[dict] = Field(default_factory=list)\n\n\nclass LabelAssociation(BaseModel):\ndataset_id: DatasetId\nfield: str\n\n\nclass LearnerConfig(BaseModel):\npredict_workers: int\nupdate_batch_size: int\nsynchronization_method: str\n\n\nclass ModelParams(BaseModel):\n# Silence pydantic warning about protected namespace\nmodel_config = ConfigDict(protected_namespaces=())\n\nmodel_configs: list[dict]\n\n## TODO: We should replace these with InputSpec\nlabel: LabelAssociation\nnumerical_fields: list[str]\ncategorical_fields: list[str]\ntextual_fields: list[str]\nimaginal_fields: list[str]\ntime_field: Optional[str]\n\n\nclass MLModellingRequest(ModelParams):\nid: TurboMLResourceIdentifier\ndataset_id: DatasetId\n\n# Use a pretrained model as the initial state\ninitial_model_id: Optional[str] = None\n\nlearner_config: Optional[LearnerConfig] = None\npredict_only: bool = False\n\n\nclass ModelConfigStorageRequest(BaseModel):\nid: TurboMLResourceIdentifier\nversion: TurboMLResourceIdentifier\ninitial_model_key: str | None\nparams: ModelParams | None\n\n@model_validator(mode=\"after\")\ndef validate_model_params(self):\nif not self.initial_model_key and not self.params:\nraise ValueError(\n\"Either initial_model_key or model_params must be provided\"\n)\nreturn self\n\n\nclass DataDriftType(StrEnum):\nUNIVARIATE = \"univariate\"\nMULTIVARIATE = \"multivariate\"\n\n\nclass DataDrift(BaseModel):\nlabel: Optional[TurboMLResourceIdentifier]\nnumerical_fields: list[str]\n\n## TODO: we could do some validations on fields for improved UX\n# (verify that they're present and are numeric)\n\n\nclass DriftQuery(BaseModel):\nlimit: int = 100\nstart_timestamp: datetime\nend_timestamp: datetime\n\n\nclass DataDriftQuery(DataDrift, DriftQuery):\npass\n\n\nclass TargetDriftQuery(DriftQuery):\npass\n\n\nclass DriftScores(BaseModel):\nscores: List[float]\ntimestamps: List[int]\n\n@validator(\"timestamps\", pre=True, each_item=True)\ndef convert_timestamp_to_epoch_microseconds(\ncls, value: Union[Timestamp, Any]\n) -> int:\nif isinstance(value, Timestamp):\nreturn int(value.timestamp() * 1_000_000)\nreturn int(float(value) * 1_000_000)\n\n\nclass VenvSpec(BaseModel):\nvenv_name: str\nlib_list: list[str]\n\n@field_validator(\"venv_name\")\ndef ensure_venv_name_is_not_funny(cls, v):\n# Restrict venv names to alphanumeric\nsafe_name_regex = r\"^[a-zA-Z0-9_]+$\"\nif not re.match(safe_name_regex, v):\nraise ValueError(\"Venv name must be alphanumeric\")\nreturn v\n\n\nclass AddPythonClassRequest(BaseModel):\nclass PythonClassValidationType(StrEnum):\nNONE = auto()\nMODEL_CLASS = auto()\nMODULE = auto()\n\nobj: Base64Bytes\nname: str\n# NOTE: No validations on the backend for now\nvalidation_type: Optional[PythonClassValidationType] = (\nPythonClassValidationType.NONE\n)\n\n\nclass HFToGGUFRequest(BaseModel):\nclass GGUFType(StrEnum):\nF32 = \"f32\"\nF16 = \"f16\"\nBF16 = \"bf16\"\nQUANTIZED_8_0 = \"q8_0\"\nAUTO = \"auto\"\n\nhf_repo_id: str\nmodel_type: GGUFType = Field(default=GGUFType.AUTO)\nselect_gguf_file: Optional[str] = None\n\n@field_validator(\"hf_repo_id\")\ndef validate_hf_repo_id(cls, v):\nif not re.match(r\"^[a-zA-Z0-9_-]+\\/[a-zA-Z0-9_\\.-]+$\", v):\nraise ValueError(\"Invalid HF repo id\")\nreturn v\n\nclass Config:\nprotected_namespaces = ()\n\n\nclass ModelAcquisitionJob(BaseModel):\nclass AcquisitionStatus(StrEnum):\nPENDING = \"pending\"\nIN_PROGRESS = \"in_progress\"\nCOMPLETED = \"completed\"\nFAILED = \"failed\"\n\njob_id: str\nstatus: AcquisitionStatus = AcquisitionStatus.PENDING\nhf_repo_id: str\nmodel_type: str\nselect_gguf_file: Optional[str] = None\ngguf_id: Optional[str] = None\nerror_message: Optional[str] = None\nprogress_message: Optional[str] = None\n\n\nclass LlamaServerRequest(BaseModel):\nclass SourceType(StrEnum):\nHUGGINGFACE = \"huggingface\"\nGGUF_ID = \"gguf_id\"\n\nclass HuggingFaceSpec(HFToGGUFRequest):\npass\n\nclass ServerParams(BaseModel):\nthreads: int = -1\nseed: int = -1\ncontext_size: int = 0\nflash_attention: bool = False\n\nsource_type: SourceType\ngguf_id: Optional[str] = None\nhf_spec: Optional[HuggingFaceSpec] = None\nserver_params: ServerParams = Field(default_factory=ServerParams)\n\n@field_validator(\"source_type\", mode=\"before\")\ndef accept_string_for_enum(cls, v):\nif isinstance(v, str):\nreturn cls.SourceType(v)\nreturn v\n\n@model_validator(mode=\"after\")\ndef validate_model_source(self):\nif self.source_type == self.SourceType.HUGGINGFACE and not self.hf_spec:\nraise ValueError(\"Huggingface model source requires hf_spec\")\nif self.source_type == self.SourceType.GGUF_ID and not self.gguf_id:\nraise ValueError(\"GGUF model source requires gguf_id\")\nreturn self\n\n\nclass LlamaServerResponse(BaseModel):\nserver_id: str\nserver_relative_url: str\n\n\nclass HFToGGUFResponse(BaseModel):\ngguf_id: str\n\n\nclass MetricRegistrationRequest(BaseModel):\n## TODO: metric types should be enum (incl custom metrics)\nmetric: str\n\n\nclass FeatureMetadata(BaseModel):\nauthor: int\nintroduced_in_version: int\ncreated_at: str  # As datetime is not json serializable\ndatatype: str  # Pandas type as SDK is python\n\n\nclass SqlFeatureSpec(BaseModel):\nfeature_name: str\nsql_spec: str\n\n\nclass AggregateFeatureSpec(BaseModel):\nfeature_name: str\ncolumn: str\naggregation_function: str\ngroup_by_columns: list[str]\ninterval: str\ntimestamp_column: str\n\n\nclass UdafFeatureSpec(BaseModel):\nfeature_name: str\narguments: list[str]\nfunction_name: str\ngroup_by_columns: list[str]\ntimestamp_column: str\ninterval: str\n\n\nclass CustomMetric(BaseModel):\nmetric_name: str\nmetric_spec: dict\n\n\nclass RwEmbeddedUdafFunctionSpec(BaseModel):\ninput_types: list[str]\noutput_type: str\nfunction_file_contents: str\n\n\nclass ExternalUdafFunctionSpec(BaseModel):\nobj: Base64Bytes\n\n\nclass UdafFunctionSpec(BaseModel):\nname: Annotated[\nstr, StringConstraints(min_length=1, pattern=r\"^[a-zA-Z][a-zA-Z0-9_]*$\")\n]\nlibraries: list[str]\nspec: RwEmbeddedUdafFunctionSpec | ExternalUdafFunctionSpec\n\n\nclass UdfFeatureSpec(BaseModel):\nfeature_name: str\narguments: list[str]\nfunction_name: str\nlibraries: list[str]\nis_rich_function: bool = False\nio_threads: Optional[int] = None\nclass_name: Optional[str] = None\ninitializer_arguments: list[str]\n\n\nclass UdfFunctionSpec(BaseModel):\nname: Annotated[\nstr, StringConstraints(min_length=1, pattern=r\"^[a-zA-Z][a-zA-Z0-9_]*$\")\n]\ninput_types: list[str]\noutput_type: str\nlibraries: list[str]\nfunction_file_contents: str\nis_rich_function: bool = False\nio_threads: Optional[int] = None\nclass_name: Optional[str] = None\ninitializer_arguments: list[str]\n\n@model_validator(mode=\"after\")\ndef validate_rich_function(self):\nif self.is_rich_function and not self.class_name:\nraise ValueError(\"class_name is required for rich functions\")\nreturn self\n\n\nclass IbisFeatureSpec(BaseModel):\ndataset_id: DatasetId\nudfs_spec: list[UdfFunctionSpec]\nencoded_table: str\n\n\nclass FeatureGroup(BaseModel):\nfeature_version: int = 0\nkey_field: str\nmeta: dict = Field(default_factory=dict)\nudfs_spec: list[UdfFunctionSpec]\nprimary_source_name: str\n\n\nclass BackEnd(Enum):\nRisingwave = auto()\nFlink = auto()\n\n\nclass ApiKey(BaseModel):\nid: int\n\"Unique identifier for the key\"\nsuffix: str\n\"Last 8 characters of the key\"\nexpire_at: Optional[datetime]\nlabel: Optional[str]\ncreated_at: datetime\nrevoked_at: Optional[datetime]\n\n\nclass FetchFeatureRequest(BaseModel):\ndataset_id: DatasetId\nlimit: int\n\n\nclass FeatureMaterializationRequest(BaseModel):\ndataset_id: DatasetId\nsql_feats: list[SqlFeatureSpec] = Field(default_factory=list)\nagg_feats: list[AggregateFeatureSpec] = Field(default_factory=list)\nudf_feats: list[UdfFeatureSpec] = Field(default_factory=list)\nudaf_feats: list[UdafFeatureSpec] = Field(default_factory=list)\nibis_feats: Optional[IbisFeatureSpec] = None\n\n\nclass FeaturePreviewRequest(FeatureMaterializationRequest):\nlimit: int = 10\n\n\nclass IbisFeatureMaterializationRequest(BaseModel):\nfeature_group_name: Annotated[\nstr,\nStringConstraints(min_length=1, pattern=r\"^[a-z]([a-z0-9_]{0,48}[a-z0-9])?$\"),\n]\nkey_field: str\nudfs_spec: list[UdfFunctionSpec]\nbackend: BackEnd\nencoded_table: str\nprimary_source_name: str\n\n@field_serializer(\"backend\")\ndef serialize_backend(self, backend: BackEnd, _info):\nreturn backend.value\n\n\nclass TimestampQuery(BaseModel):\ncolumn_name: str\ntimestamp_format: str\n\n\nclass LoginResponse(BaseModel):\naccess_token: str\ntoken_type: str\n\n\nclass Oauth2StartResponse(BaseModel):\nauth_uri: str\n\n\nclass NewApiKeyRequest(BaseModel):\nexpire_at: Optional[datetime]\nlabel: str\n\n@field_validator(\"expire_at\")\ndef validate_expire_at(cls, v):\nif v is not None and v < datetime.now():\nraise ValueError(\"expire_at must be in the future\")\nreturn v\n\n\nclass NewApiKeyResponse(BaseModel):\nkey: str\nexpire_at: Optional[datetime]\n\n\ndef _partial_model(model: Type[BaseModel]):\n\"\"\"\nDecorator to create a partial model, where all fields are optional.\nUseful for PATCH requests, where we want to allow partial updates\nand the models may be derived from the original model.\n\"\"\"\n\ndef make_field_optional(\nfield: FieldInfo, default: Any = None\n) -> Tuple[Any, FieldInfo]:\nnew = deepcopy(field)\nnew.default = default\nnew.annotation = Optional[field.annotation]  # type: ignore\nreturn new.annotation, new\n\nfields = {\nfield_name: make_field_optional(field_info)\nfor field_name, field_info in model.model_fields.items()\n}\nreturn create_model(  # type: ignore\nf\"Partial{model.__name__}\",\n__doc__=model.__doc__,\n__base__=model,\n__module__=model.__module__,\n**fields,  # type: ignore\n)\n\n\n@_partial_model\nclass ApiKeyPatchRequest(BaseModel):\nlabel: Optional[str]\nexpire_at: Optional[datetime]\n\n@field_validator(\"expire_at\", mode=\"before\")\ndef validate_expire_at(cls, v):\nif v is None:\nreturn None\nv = datetime.fromtimestamp(v, tz=timezone.utc)\nif v < datetime.now(timezone.utc):\nraise ValueError(\"expire_at must be in the future\")\nreturn v\n\n\nclass User(BaseModel):\nid: int\nusername: str\nemail: str | None = None\n\n\nclass NamespaceAcquisitionRequest(BaseModel):\nnamespace: Annotated[\nstr,\nStringConstraints(\nmin_length=1, max_length=32, pattern=r\"^[a-zA-Z][a-zA-Z0-9_]*$\"\n),\n]\n\n\nclass UserManager:\ndef __init__(self):\n# Cache username -> user\n## TODO: we should change this to user_id -> user, along\n# with changing the API/auth to use user_id instead of username.\n# This is part of our agreement to use username for display purposes only,\n# as well as most of our other resources using IDs.\nself.user_cache = {}\n\n\nclass TosResponse(BaseModel):\nversion: int\nformat: str  # text/plain or text/html\ncontent: str\n\n\nclass TosAcceptanceRequest(BaseModel):\nversion: int\n\n\nclass InputSpec(BaseModel):\nkey_field: str\ntime_field: Optional[str]\nnumerical_fields: list[str]\ncategorical_fields: list[str]\ntextual_fields: list[str]\nimaginal_fields: list[str]\n\nlabel_field: str\n\n\nclass ModelMetadata(BaseModel):\n# NOTE: We could use a proto-derived pydantic model here\n# (with `so1n/protobuf_to_pydantic`) but on our last attempt\n# the generated models were problematic for `oneof` proto fields and the hacks\n# weren't worth it. We can still revisit this in the future.\n# Ref: https://github.com/so1n/protobuf_to_pydantic/issues/31\nprocess_config: dict\noffset: str\ninput_db_source: str\ninput_db_columns: list[DbColumn]\nmetrics: list[str]\nlabel_association: LabelAssociation\ndrift: str\n\ndef get_input_spec(self) -> InputSpec:\nkey_field = self.process_config[\"inputConfig\"][\"keyField\"]\ntime_field = self.process_config[\"inputConfig\"].get(\"time_tick\", None)\nnumerical_fields = list(self.process_config[\"inputConfig\"].get(\"numerical\", []))\ncategorical_fields = list(\nself.process_config[\"inputConfig\"].get(\"categorical\", [])\n)\ntextual_fields = list(self.process_config[\"inputConfig\"].get(\"textual\", []))\nimaginal_fields = list(self.process_config[\"inputConfig\"].get(\"imaginal\", []))\n# label_field = self.label_association.field if self.label_association else None\nreturn InputSpec(\nkey_field=key_field,\ntime_field=time_field,\nnumerical_fields=numerical_fields,\ncategorical_fields=categorical_fields,\ntextual_fields=textual_fields,\nimaginal_fields=imaginal_fields,\nlabel_field=self.label_association.field,\n)\n\n\nclass ModelInfo(BaseModel):\nmetadata: ModelMetadata\nendpoint_paths: list[str]\n\n\nclass StoredModel(BaseModel):\nname: str\nversion: str\nstored_size: int\ncreated_at: datetime\n\n\nclass ProcessMeta(BaseModel):\ncaller: str\nnamespace: str\njob_id: str\n\n\nclass ProcessInfo(BaseModel):\npid: int\ncmd: list[str]\nstdout_path: str\nstderr_path: str\n# For turboml jobs\nmeta: Optional[ProcessMeta]\nrestart: bool\nstopped: bool = False\n\n@field_validator(\"cmd\", mode=\"before\")\ndef tolerate_string_cmd(cls, v):  # For backward compatibility\nif isinstance(v, str):\nreturn v.split()\nreturn v\n\n\nclass ProcessOutput(BaseModel):\nstdout: str\nstderr: str\n\n\nclass ProcessPatchRequest(BaseModel):\naction: Literal[\"kill\", \"restart\"]\n\n\nclass ModelPatchRequest(BaseModel):\naction: Literal[\"pause\", \"resume\"]\n\n\nclass ModelDeleteRequest(BaseModel):\ndelete_output_topic: StrictBool\n\n\n# Moved from ml_algs\nclass SupervisedAlgorithms(StrEnum):\nHoeffdingTreeClassifier = \"HoeffdingTreeClassifier\"\nHoeffdingTreeRegressor = \"HoeffdingTreeRegressor\"\nAMFClassifier = \"AMFClassifier\"\nAMFRegressor = \"AMFRegressor\"\nFFMClassifier = \"FFMClassifier\"\nFFMRegressor = \"FFMRegressor\"\nSGTClassifier = \"SGTClassifier\"\nSGTRegressor = \"SGTRegressor\"\nSNARIMAX = \"SNARIMAX\"\nLeveragingBaggingClassifier = \"LeveragingBaggingClassifier\"\nHeteroLeveragingBaggingClassifier = \"HeteroLeveragingBaggingClassifier\"\nAdaBoostClassifier = \"AdaBoostClassifier\"\nHeteroAdaBoostClassifier = \"HeteroAdaBoostClassifier\"\nRandomSampler = \"RandomSampler\"\nNeuralNetwork = \"NeuralNetwork\"\nONN = \"ONN\"\nPython = \"Python\"\nOVR = \"OVR\"\nBanditModelSelection = \"BanditModelSelection\"\nContextualBanditModelSelection = \"ContextualBanditModelSelection\"\nRandomProjectionEmbedding = \"RandomProjectionEmbedding\"\nEmbeddingModel = \"EmbeddingModel\"\nMultinomialNB = \"MultinomialNB\"\nGaussianNB = \"GaussianNB\"\nAdaptiveXGBoost = \"AdaptiveXGBoost\"\nAdaptiveLGBM = \"AdaptiveLGBM\"\nLLAMAEmbedding = \"LLAMAEmbedding\"\nLlamaText = \"LlamaText\"\nRestAPIClient = \"RestAPIClient\"\nClipEmbedding = \"ClipEmbedding\"\nPythonEnsembleModel = \"PythonEnsembleModel\"\nGRPCClient = \"GRPCClient\"\n\n\nclass UnsupervisedAlgorithms(StrEnum):\nMStream = \"MStream\"\nRCF = \"RCF\"\nHST = \"HST\"\nONNX = \"ONNX\"\n\n\nclass EvaluationMetrics(StrEnum):\nWindowedAUC = \"WindowedAUC\"\nWindowedMAE = \"WindowedMAE\"\nWindowedMSE = \"WindowedMSE\"\nWindowedRMSE = \"WindowedRMSE\"\nWindowedAccuracy = \"WindowedAccuracy\"\n\n\n# Timestamp conversion and support for duckdb and risingwave\nclass TimestampRealType(StrEnum):\nepoch_seconds = \"epoch_seconds\"\nepoch_milliseconds = \"epoch_milliseconds\"\n\n\nclass RisingWaveVarcharType(StrEnum):\nYYYY_MM_DD = \"YYYY MM DD\"\nYYYY_MM_DD_HH24_MI_SS_US = \"YYYY-MM-DD HH24:MI:SS.US\"\nYYYY_MM_DD_HH12_MI_SS_US = \"YYYY-MM-DD HH12:MI:SS.US\"\nYYYY_MM_DD_HH12_MI_SS_MS = \"YYYY-MM-DD HH12:MI:SS.MS\"\nYYYY_MM_DD_HH24_MI_SS_MS = \"YYYY-MM-DD HH24:MI:SS.MS\"\nYYYY_MM_DD_HH24_MI_SSTZH_TZM = \"YYYY-MM-DD HH24:MI:SSTZH:TZM\"\nYYYY_MM_DD_HH12_MI_SSTZH_TZM = \"YYYY-MM-DD HH12:MI:SSTZH:TZM\"\n\n\nclass DuckDbVarcharType(StrEnum):\nYYYY_MM_DD = \"%x\"\nYYYY_MM_DD_HH24_MI_SS_US = \"%x %H.%f\"\nYYYY_MM_DD_HH12_MI_SS_US = \"%x %I.%f %p\"\nYYYY_MM_DD_HH12_MI_SS_MS = \"%x %I.%g %p\"\nYYYY_MM_DD_HH24_MI_SS_MS = \"%x %H.%g\"\nYYYY_MM_DD_HH24_MI_SSTZH_TZM = \"%x %H.%g %z\"\nYYYY_MM_DD_HH12_MI_SSTZH_TZM = \"%x %I.%g %p %z\"\n\n\nclass Evaluations(BaseModel):\nclass ModelOutputType(StrEnum):\nPREDICTED_CLASS = \"predicted_class\"\nSCORE = \"score\"\n\nmodel_names: list\nmetric: str\nfilter_expression: str = \"\"\nwindow_size: PositiveInt = 1000\nlimit: PositiveInt = 100\nis_web: bool = False\noutput_type: Optional[ModelOutputType] = ModelOutputType.SCORE\n\nclass Config:\nprotected_namespaces = ()\n\n\nclass ModelScores(BaseModel):\nscores: List[float]\ntimestamps: List[int]\npage: int\nnext_page: Optional[List[int]] = None\n\n@validator(\"timestamps\", pre=True, each_item=True)\ndef convert_timestamp_to_epoch_microseconds(\ncls, value: Union[Timestamp, Any]\n) -> int:\nif isinstance(value, Timestamp):\nreturn int(value.timestamp() * 1_000_000)\nreturn int(float(value) * 1_000_000)\n\n\nclass KafkaTopicInfo(BaseModel):\nname: str\npartitions: int\nreplication_factor: int\nnum_messages: int\n\n\nclass KafkaTopicSettings(BaseModel):\n## TODO(maniktherana): prune as much Optional as we can to get stronger types\ncompression_type: Optional[str] = None\nleader_replication_throttled_replicas: Optional[str] = None\nremote_storage_enable: Optional[bool] = None\nmessage_downconversion_enable: Optional[bool] = None\nmin_insync_replicas: Optional[int] = None\nsegment_jitter_ms: Optional[int] = None\nlocal_retention_ms: Optional[int] = None\ncleanup_policy: Optional[str] = None\nflush_ms: Optional[int] = None\nfollower_replication_throttled_replicas: Optional[str] = None\nsegment_bytes: Optional[int] = None\nretention_ms: Optional[int] = None\nflush_messages: Optional[int] = None\nmessage_format_version: Optional[str] = None\nmax_compaction_lag_ms: Optional[int] = None\nfile_delete_delay_ms: Optional[int] = None\nmax_message_bytes: Optional[int] = None\nmin_compaction_lag_ms: Optional[int] = None\nmessage_timestamp_type: Optional[str] = None\nlocal_retention_bytes: Optional[int] = None\npreallocate: Optional[bool] = None\nindex_interval_bytes: Optional[int] = None\nmin_cleanable_dirty_ratio: Optional[float] = None\nunclean_leader_election_enable: Optional[bool] = None\nretention_bytes: Optional[int] = None\ndelete_retention_ms: Optional[int] = None\nmessage_timestamp_after_max_ms: Optional[int] = None\nmessage_timestamp_before_max_ms: Optional[int] = None\nsegment_ms: Optional[int] = None\nmessage_timestamp_difference_max_ms: Optional[int] = None\nsegment_index_bytes: Optional[int] = None\n\n\nclass DetailedKafkaTopicInfo(BaseModel):\nname: str\npartitions: int\nreplication_factor: int\nurp: int\nin_sync_replicas: int\ntotal_replicas: int\ncleanup_policy: str\nsegment_size: int\nsegment_count: int\n\n\nclass KafkaTopicConsumer(BaseModel):\ngroup_id: str\nactive_consumers: int\nstate: str\n\n\nclass SchemaInfo(BaseModel):\nsubject: str\nid: int\ntype: str\nversion: int\n\n\nclass DetailedSchemaInfo(BaseModel):\nsubject: str\nlatest_version: int\nlatest_id: int\nlatest_type: str\nall_versions: list[SchemaInfo]\n\n\nclass ServiceEndpoints(BaseModel):\narrow_server: str\nfeature_server: str\n\n```"
    },
    {
        "section": "model_comparison.py",
        "content": "# model_comparison.py\n-## Location -> root_directory.common\n```python\nimport turboml as tb\nfrom typing import List\n\n\ndef compare_model_metrics(models: List, metric: str, x_axis_title: str = \"Samples\"):\n\"\"\"Generates a plotly plot for Windowed metrics comparison for a list of models\n\nArgs:\nmodel_names (List): List of models to compare\nmetric (str): Metric for evaluation of models, should be chosen from tb.evaluation_metrics()\nx_axis_title (str, optional): X axis title for the plot. Defaults to \"Samples\".\n\nRaises:\nException: If other metrics are chosen then Execption is raised\n\"\"\"\nimport plotly.graph_objs as go\n\nmodel_traces = []\nwindowed_metrics = tb.evaluation_metrics()\nif metric in windowed_metrics:\nfor model in models:\n# It is assumed that the user registers the metric before comparing the models\nevals = model.get_evaluation(metric_name=metric)\nmodel_evaluations = [eval.metric for eval in evals]\nindex = [eval.index for eval in evals]\ntrace = go.Scatter(\nx=index,\ny=model_evaluations,\nmode=\"lines\",\nname=model.model_name,\n)\nmodel_traces.append(trace)\nlayout = go.Layout(\ntitle=metric,\nxaxis=dict(title=x_axis_title),  # noqa\nyaxis=dict(title=\"Metric\"),  # noqa\n)\nfig = go.Figure(data=model_traces, layout=layout)\nfig.show()\nelse:\nraise Exception(\nf\"The other Windowed metrics arent supported yet, please choose from {tb.evaluation_metrics()}, if you want to use batch metrics please use tb.compare_batch_metrics\"\n)\n\n```"
    },
    {
        "section": "namespaces.py",
        "content": "# namespaces.py\n-## Location -> root_directory.common\n```python\nfrom .api import api\nfrom .models import NamespaceAcquisitionRequest\n\n\ndef set_active_namespace(namespace: str) -> None:\napi.set_active_namespace(namespace)\n\n\ndef get_default_namespace() -> str:\nreturn api.get(endpoint=\"user/namespace/default\").json()\n\n\ndef set_default_namespace(namespace: str) -> None:\nresp = api.put(endpoint=f\"user/namespace/default?namespace={namespace}\")\nif resp.status_code not in range(200, 300):\nraise Exception(f\"Failed to set default namespace: {resp.json()['detail']}\")\n\n\ndef acquire_namespace(namespace: str) -> None:\npayload = NamespaceAcquisitionRequest(namespace=namespace)\nresp = api.post(\nendpoint=\"user/namespace/acquire\",\njson=payload.model_dump(),\nexclude_namespace=True,\n)\nif resp.status_code not in range(200, 300):\nraise Exception(f\"Failed to acquire namespace: {resp.json()['detail']}\")\n\n\ndef list_namespaces(include_shared: bool = False) -> list[str]:\nresp = api.get(endpoint=f\"user/namespaces?include_shared={include_shared}\")\nif resp.status_code not in range(200, 300):\nraise Exception(f\"Failed to list namespaces: {resp.json()['detail']}\")\nreturn resp.json()\n\n```"
    },
    {
        "section": "pymodel.py",
        "content": "# pymodel.py\n-## Location -> root_directory.common\n```python\nfrom turboml_bindings.pymodel import *  # noqa\n\n```"
    },
    {
        "section": "pytypes.py",
        "content": "# pytypes.py\n-## Location -> root_directory.common\n```python\nfrom turboml_bindings.pytypes import *  # noqa\n\n```"
    },
    {
        "section": "types.py",
        "content": "# types.py\n-## Location -> root_directory.common\n```python\nfrom __future__ import annotations\nfrom abc import abstractmethod\nfrom typing import NewType, TYPE_CHECKING\n\nif TYPE_CHECKING:\nimport turboml.common.pytypes as pytypes\n\nGGUFModelId = NewType(\"GGUFModelId\", str)\n\n\nclass PythonModel:\n@abstractmethod\ndef init_imports(self):\n\"\"\"\nMust import all libraries/modules needed in learn_one and predict_one\n\"\"\"\npass\n\n@abstractmethod\ndef learn_one(self, input: pytypes.InputData) -> None:\npass\n\n@abstractmethod\ndef predict_one(self, input: pytypes.InputData, output: pytypes.OutputData) -> None:\npass\n\n```"
    },
    {
        "section": "udf.py",
        "content": "# udf.py\n-## Location -> root_directory.common\n```python\nfrom typing import Any\n\n\nclass ModelMetricAggregateFunction:\n\"\"\"\nBase class for defining a Model Metric Aggregate Function.\n\"\"\"\n\ndef __init__(self):\npass\n\ndef create_state(self) -> Any:\n\"\"\"\nCreate the initial state for the UDAF.\n\nReturns:\nAny: The initial state.\n\"\"\"\nraise NotImplementedError(\n\"The 'create_state' method must be implemented by subclasses.\"\n)\n\ndef accumulate(self, state: Any, prediction: float, label: float) -> Any:\n\"\"\"\nAccumulate input data (prediction and label) into the state.\n\nArgs:\nstate (Any): The current state.\nprediction (float): The predicted value.\nlabel (float): The ground truth label.\n\nReturns:\nAny: The updated state.\n\"\"\"\nraise NotImplementedError(\n\"The 'accumulate' method must be implemented by subclasses.\"\n)\n\ndef retract(self, state: Any, prediction: float, label: float) -> Any:\n\"\"\"\nRetract input data from the state (optional).\n\nArgs:\nstate (Any): The current state.\nprediction (float): The predicted value.\nlabel (float): The ground truth label.\n\nReturns:\nAny: The updated state.\n\"\"\"\nraise NotImplementedError(\n\"The 'retract' method must be implemented by subclasses.\"\n)\n\ndef merge_states(self, state1: Any, state2: Any) -> Any:\n\"\"\"\nMerge two states into one.\n\nArgs:\nstate1 (Any): The first state.\nstate2 (Any): The second state.\n\nReturns:\nAny: The merged state.\n\"\"\"\nraise NotImplementedError(\n\"The 'merge_states' method must be implemented by subclasses.\"\n)\n\ndef finish(self, state: Any) -> float:\n\"\"\"\nFinalize the aggregation and compute the result.\n\nArgs:\nstate (Any): The final state.\n\nReturns:\nfloat: The result of the aggregation.\n\"\"\"\nraise NotImplementedError(\n\"The 'finish' method must be implemented by subclasses.\"\n)\n\n```"
    },
    {
        "section": "util.py",
        "content": "# util.py\n-## Location -> root_directory.common\n```python\nimport ast\nimport inspect\nfrom collections.abc import Sequence\nfrom typing import TypeVar, cast, List\n\nimport pyarrow as pa\n\nV = TypeVar(\"V\")\n\n\ndef promote_list(val: V | Sequence[V]) -> list[V]:\n\"\"\"Ensure that the value is a list.\n\nParameters\n----------\nval\nValue to promote\n\nReturns\n-------\nlist\n\n\"\"\"\nif isinstance(val, list):\nreturn val\nelif isinstance(val, dict):\nreturn [val]\nelif val is None:\nreturn []\nelse:\nreturn [val]\n\n\ndef risingwave_type_to_pyarrow(type: str):\n\"\"\"\nConvert a SQL data type string to `pyarrow.DataType`.\n\"\"\"\nt = type.upper()\nif t.endswith(\"[]\"):\nreturn pa.list_(risingwave_type_to_pyarrow(type[:-2]))\nelif t.startswith(\"STRUCT\"):\nreturn _parse_struct(t)\nreturn _simple_type(t)\n\n\ndef _parse_struct(type: str):\n# extract 'STRUCT<a:INT, b:VARCHAR, c:STRUCT<d:INT>, ...>'\ntype_list = type[7:-1]  # strip \"STRUCT<>\"\nfields = []\nstart = 0\ndepth = 0\nfor i, c in enumerate(type_list):\nif c == \"<\":\ndepth += 1\nelif c == \">\":\ndepth -= 1\nelif c == \",\" and depth == 0:\nname, t = type_list[start:i].split(\":\", maxsplit=1)\nname = name.strip()\nt = t.strip()\nfields.append(pa.field(name, risingwave_type_to_pyarrow(t)))\nstart = i + 1\nif \":\" in type_list[start:].strip():\nname, t = type_list[start:].split(\":\", maxsplit=1)\nname = name.strip()\nt = t.strip()\nfields.append(pa.field(name, risingwave_type_to_pyarrow(t)))\nreturn pa.struct(fields)\n\n\ndef _simple_type(t: str):\ntype_map = {\n\"NULL\": pa.null,\n\"BOOLEAN\": pa.bool_,\n\"BOOL\": pa.bool_,\n\"TINYINT\": pa.int8,\n\"INT8\": pa.int8,\n\"SMALLINT\": pa.int16,\n\"INT16\": pa.int16,\n\"INT\": pa.int32,\n\"INTEGER\": pa.int32,\n\"INT32\": pa.int32,\n\"BIGINT\": pa.int64,\n\"INT64\": pa.int64,\n\"UINT8\": pa.uint8,\n\"UINT16\": pa.uint16,\n\"UINT32\": pa.uint32,\n\"UINT64\": pa.uint64,\n\"FLOAT32\": pa.float32,\n\"REAL\": pa.float32,\n\"FLOAT64\": pa.float64,\n\"DOUBLE PRECISION\": pa.float64,\n\"DOUBLE\": pa.float64,\n\"DATE32\": pa.date32,\n\"DATE\": pa.date32,\n\"TIME64\": lambda: pa.time64(\"us\"),\n\"TIME\": lambda: pa.time64(\"us\"),\n\"TIME WITHOUT TIME ZONE\": lambda: pa.time64(\"us\"),\n\"TIMESTAMP\": lambda: pa.timestamp(\"us\"),\n\"TIMESTAMP WITHOUT TIME ZONE\": lambda: pa.timestamp(\"us\"),\n\"INTERVAL\": pa.month_day_nano_interval,\n\"STRING\": pa.string,\n\"VARCHAR\": pa.string,\n\"LARGE_STRING\": pa.large_string,\n\"BINARY\": pa.binary,\n\"BYTEA\": pa.binary,\n\"LARGE_BINARY\": pa.large_binary,\n}\n\nif t in type_map:\nreturn type_map[t]()\n\nraise ValueError(f\"Unsupported type: {t}\")\n\n\ndef _is_running_ipython() -> bool:\n\"\"\"Checks if we are currently running in IPython\"\"\"\ntry:\nreturn get_ipython() is not None  # type: ignore[name-defined]\nexcept NameError:\nreturn False\n\n\ndef _get_ipython_cell_sources() -> list[str]:\n\"\"\"Returns the source code of all cells in the running IPython session.\nSee https://github.com/wandb/weave/pull/1864\n\"\"\"\nshell = get_ipython()  # type: ignore[name-defined]  # noqa: F821\nif not hasattr(shell, \"user_ns\"):\nraise AttributeError(\"Cannot access user namespace\")\ncells = cast(list[str], shell.user_ns[\"In\"])\n# First cell is always empty\nreturn cells[1:]\n\n\ndef _extract_relevant_imports(\nimport_statements: List[ast.AST], used_names: set\n) -> List[str]:\n\"\"\"Filter and format relevant import statements based on used names.\"\"\"\nrelevant_imports = []\nfor node in import_statements:\nif isinstance(node, ast.Import):\nfor alias in node.names:\nif alias.asname in used_names or alias.name in used_names:\nrelevant_imports.append(\nf\"import {alias.name}\"\n+ (f\" as {alias.asname}\" if alias.asname else \"\")\n)\nelif isinstance(node, ast.ImportFrom):\nfor alias in node.names:\nif alias.asname in used_names or alias.name in used_names:\nrelevant_imports.append(\nf\"from {node.module} import {alias.name}\"\n+ (f\" as {alias.asname}\" if alias.asname else \"\")\n)\nreturn relevant_imports\n\n\ndef _find_imports_and_used_names(source_code: str, func_source: str) -> List[str]:\n\"\"\"Find imports and their relevance to the function source code.\"\"\"\nmodule_ast = ast.parse(source_code)\nfunc_ast = ast.parse(func_source).body[0]\n\nimport_statements = [\nnode\nfor node in ast.walk(module_ast)\nif isinstance(node, (ast.Import, ast.ImportFrom))\n]\n\nused_names = {node.id for node in ast.walk(func_ast) if isinstance(node, ast.Name)}\n\nreturn _extract_relevant_imports(import_statements, used_names)\n\n\ndef get_imports_used_in_function(func) -> str:\n\"\"\"Get all relevant imports used in a function.\"\"\"\nif _is_running_ipython():\ncell_sources = _get_ipython_cell_sources()\nimports = []\nfor cell_source in cell_sources:\ntry:\nimports.extend(\n_find_imports_and_used_names(cell_source, inspect.getsource(func))\n)\nexcept Exception:\ncontinue\nreturn \"\\n\".join(set(imports))\n\nelse:\nmodule_source_code = inspect.getsource(inspect.getmodule(func))\nfunc_source = inspect.getsource(func)\nreturn \"\\n\".join(_find_imports_and_used_names(module_source_code, func_source))\n\n```"
    },
    {
        "section": "config_pb2.py",
        "content": "# config_pb2.py\n-## Location -> root_directory.common.protos\n```python\n# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: config.proto\n# Protobuf Python Version: 4.25.5\n\"\"\"Generated protocol buffer code.\"\"\"\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import descriptor_pool as _descriptor_pool\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf.internal import builder as _builder\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\x0c\\x63onfig.proto\\x12\\x06\\x63onfig\\\"F\\n\\rMStreamConfig\\x12\\x10\\n\\x08num_rows\\x18\\x01 \\x01(\\x05\\x12\\x13\\n\\x0bnum_buckets\\x18\\x02 \\x01(\\x05\\x12\\x0e\\n\\x06\\x66\\x61\\x63tor\\x18\\x03 \\x01(\\x02\\\"c\\n\\tRCFConfig\\x12\\x12\\n\\ntime_decay\\x18\\x01 \\x01(\\x02\\x12\\x17\\n\\x0fnumber_of_trees\\x18\\x02 \\x01(\\x05\\x12\\x14\\n\\x0coutput_after\\x18\\x03 \\x01(\\x05\\x12\\x13\\n\\x0bsample_size\\x18\\x04 \\x01(\\x05\\\"A\\n\\tHSTConfig\\x12\\x0f\\n\\x07n_trees\\x18\\x01 \\x01(\\x05\\x12\\x0e\\n\\x06height\\x18\\x02 \\x01(\\x05\\x12\\x13\\n\\x0bwindow_size\\x18\\x03 \\x01(\\x05\\\"9\\n\\nONNXConfig\\x12\\x17\\n\\x0fmodel_save_name\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\nmodel_data\\x18\\x02 \\x01(\\x0c\\\"\\x90\\x01\\n\\x19HoeffdingClassifierConfig\\x12\\r\\n\\x05\\x64\\x65lta\\x18\\x01 \\x01(\\x02\\x12\\x0b\\n\\x03tau\\x18\\x02 \\x01(\\x02\\x12\\x14\\n\\x0cgrace_period\\x18\\x03 \\x01(\\x05\\x12\\x11\\n\\tn_classes\\x18\\x04 \\x01(\\x05\\x12\\x18\\n\\x10leaf_pred_method\\x18\\x05 \\x01(\\t\\x12\\x14\\n\\x0csplit_method\\x18\\x06 \\x01(\\t\\\"f\\n\\x18HoeffdingRegressorConfig\\x12\\r\\n\\x05\\x64\\x65lta\\x18\\x01 \\x01(\\x02\\x12\\x0b\\n\\x03tau\\x18\\x02 \\x01(\\x02\\x12\\x14\\n\\x0cgrace_period\\x18\\x03 \\x01(\\x05\\x12\\x18\\n\\x10leaf_pred_method\\x18\\x04 \\x01(\\t\\\"\\x8c\\x01\\n\\x13\\x41MFClassifierConfig\\x12\\x11\\n\\tn_classes\\x18\\x01 \\x01(\\x05\\x12\\x14\\n\\x0cn_estimators\\x18\\x02 \\x01(\\x05\\x12\\x0c\\n\\x04step\\x18\\x03 \\x01(\\x02\\x12\\x17\\n\\x0fuse_aggregation\\x18\\x04 \\x01(\\x08\\x12\\x11\\n\\tdirichlet\\x18\\x05 \\x01(\\x02\\x12\\x12\\n\\nsplit_pure\\x18\\x06 \\x01(\\x08\\\"d\\n\\x12\\x41MFRegressorConfig\\x12\\x14\\n\\x0cn_estimators\\x18\\x01 \\x01(\\x05\\x12\\x0c\\n\\x04step\\x18\\x02 \\x01(\\x02\\x12\\x17\\n\\x0fuse_aggregation\\x18\\x03 \\x01(\\x08\\x12\\x11\\n\\tdirichlet\\x18\\x04 \\x01(\\x02\\\"\\xb4\\x01\\n\\x13\\x46\\x46MClassifierConfig\\x12\\x11\\n\\tn_factors\\x18\\x01 \\x01(\\x05\\x12\\x11\\n\\tl1_weight\\x18\\x02 \\x01(\\x02\\x12\\x11\\n\\tl2_weight\\x18\\x03 \\x01(\\x02\\x12\\x11\\n\\tl1_latent\\x18\\x04 \\x01(\\x02\\x12\\x11\\n\\tl2_latent\\x18\\x05 \\x01(\\x02\\x12\\x11\\n\\tintercept\\x18\\x06 \\x01(\\x02\\x12\\x14\\n\\x0cintercept_lr\\x18\\x07 \\x01(\\x02\\x12\\x15\\n\\rclip_gradient\\x18\\x08 \\x01(\\x02\\\"\\xb3\\x01\\n\\x12\\x46\\x46MRegressorConfig\\x12\\x11\\n\\tn_factors\\x18\\x01 \\x01(\\x05\\x12\\x11\\n\\tl1_weight\\x18\\x02 \\x01(\\x02\\x12\\x11\\n\\tl2_weight\\x18\\x03 \\x01(\\x02\\x12\\x11\\n\\tl1_latent\\x18\\x04 \\x01(\\x02\\x12\\x11\\n\\tl2_latent\\x18\\x05 \\x01(\\x02\\x12\\x11\\n\\tintercept\\x18\\x06 \\x01(\\x02\\x12\\x14\\n\\x0cintercept_lr\\x18\\x07 \\x01(\\x02\\x12\\x15\\n\\rclip_gradient\\x18\\x08 \\x01(\\x02\\\"\\x87\\x01\\n\\x0eSNARIMAXConfig\\x12\\x0f\\n\\x07horizon\\x18\\x01 \\x01(\\x05\\x12\\t\\n\\x01p\\x18\\x02 \\x01(\\x05\\x12\\t\\n\\x01\\x64\\x18\\x03 \\x01(\\x05\\x12\\t\\n\\x01q\\x18\\x04 \\x01(\\x05\\x12\\t\\n\\x01m\\x18\\x05 \\x01(\\x05\\x12\\n\\n\\x02sp\\x18\\x06 \\x01(\\x05\\x12\\n\\n\\x02sd\\x18\\x07 \\x01(\\x05\\x12\\n\\n\\x02sq\\x18\\x08 \\x01(\\x05\\x12\\x14\\n\\x0cnum_children\\x18\\t \\x01(\\x05\\\"\\xd0\\x02\\n\\x13NeuralNetworkConfig\\x12>\\n\\x06layers\\x18\\x01 \\x03(\\x0b\\x32..config.NeuralNetworkConfig.NeuralNetworkLayer\\x12\\x0f\\n\\x07\\x64ropout\\x18\\x02 \\x01(\\x02\\x12\\x15\\n\\rloss_function\\x18\\x03 \\x01(\\t\\x12\\x11\\n\\toptimizer\\x18\\x04 \\x01(\\t\\x12\\x15\\n\\rlearning_rate\\x18\\x05 \\x01(\\x02\\x12\\x12\\n\\nbatch_size\\x18\\x06 \\x01(\\x05\\x1a\\x92\\x01\\n\\x12NeuralNetworkLayer\\x12\\x12\\n\\ninput_size\\x18\\x01 \\x01(\\x05\\x12\\x13\\n\\x0boutput_size\\x18\\x02 \\x01(\\x05\\x12\\x12\\n\\nactivation\\x18\\x03 \\x01(\\t\\x12\\x0f\\n\\x07\\x64ropout\\x18\\x04 \\x01(\\x02\\x12\\x1c\\n\\x14residual_connections\\x18\\x05 \\x03(\\x05\\x12\\x10\\n\\x08use_bias\\x18\\x06 \\x01(\\x08\\\"\\x7f\\n\\tONNConfig\\x12\\x1d\\n\\x15max_num_hidden_layers\\x18\\x01 \\x01(\\x05\\x12\\x1f\\n\\x17qtd_neuron_hidden_layer\\x18\\x02 \\x01(\\x05\\x12\\x11\\n\\tn_classes\\x18\\x03 \\x01(\\x05\\x12\\t\\n\\x01\\x62\\x18\\x04 \\x01(\\x02\\x12\\t\\n\\x01n\\x18\\x05 \\x01(\\x02\\x12\\t\\n\\x01s\\x18\\x06 \\x01(\\x02\\\"4\\n\\tOVRConfig\\x12\\x11\\n\\tn_classes\\x18\\x01 \\x01(\\x05\\x12\\x14\\n\\x0cnum_children\\x18\\x02 \\x01(\\x05\\\"O\\n\\x1fRandomProjectionEmbeddingConfig\\x12\\x14\\n\\x0cn_embeddings\\x18\\x01 \\x01(\\x05\\x12\\x16\\n\\x0etype_embedding\\x18\\x02 \\x01(\\t\\\",\\n\\x14\\x45mbeddingModelConfig\\x12\\x14\\n\\x0cnum_children\\x18\\x01 \\x01(\\x05\\\"A\\n\\x1a\\x42\\x61nditModelSelectionConfig\\x12\\x0e\\n\\x06\\x62\\x61ndit\\x18\\x01 \\x01(\\t\\x12\\x13\\n\\x0bmetric_name\\x18\\x02 \\x01(\\t\\\"U\\n$ContextualBanditModelSelectionConfig\\x12\\x18\\n\\x10\\x63ontextualbandit\\x18\\x01 \\x01(\\t\\x12\\x13\\n\\x0bmetric_name\\x18\\x02 \\x01(\\t\\\"\\x8f\\x01\\n!LeveragingBaggingClassifierConfig\\x12\\x10\\n\\x08n_models\\x18\\x01 \\x01(\\x05\\x12\\x11\\n\\tn_classes\\x18\\x02 \\x01(\\x05\\x12\\t\\n\\x01w\\x18\\x03 \\x01(\\x02\\x12\\x16\\n\\x0e\\x62\\x61gging_method\\x18\\x04 \\x01(\\t\\x12\\x0c\\n\\x04seed\\x18\\x05 \\x01(\\x05\\x12\\x14\\n\\x0cnum_children\\x18\\x06 \\x01(\\x05\\\"\\x83\\x01\\n\\'HeteroLeveragingBaggingClassifierConfig\\x12\\x11\\n\\tn_classes\\x18\\x01 \\x01(\\x05\\x12\\t\\n\\x01w\\x18\\x02 \\x01(\\x02\\x12\\x16\\n\\x0e\\x62\\x61gging_method\\x18\\x03 \\x01(\\t\\x12\\x0c\\n\\x04seed\\x18\\x04 \\x01(\\x05\\x12\\x14\\n\\x0cnum_children\\x18\\x05 \\x01(\\x05\\\"c\\n\\x18\\x41\\x64\\x61\\x42oostClassifierConfig\\x12\\x10\\n\\x08n_models\\x18\\x01 \\x01(\\x05\\x12\\x11\\n\\tn_classes\\x18\\x02 \\x01(\\x05\\x12\\x0c\\n\\x04seed\\x18\\x03 \\x01(\\x05\\x12\\x14\\n\\x0cnum_children\\x18\\x04 \\x01(\\x05\\\"W\\n\\x1eHeteroAdaBoostClassifierConfig\\x12\\x11\\n\\tn_classes\\x18\\x01 \\x01(\\x05\\x12\\x0c\\n\\x04seed\\x18\\x02 \\x01(\\x05\\x12\\x14\\n\\x0cnum_children\\x18\\x03 \\x01(\\x05\\\"Y\\n\\x13SGTClassifierConfig\\x12\\r\\n\\x05\\x64\\x65lta\\x18\\x01 \\x01(\\x02\\x12\\x14\\n\\x0cgrace_period\\x18\\x02 \\x01(\\x05\\x12\\x0e\\n\\x06lambda\\x18\\x03 \\x01(\\x02\\x12\\r\\n\\x05gamma\\x18\\x04 \\x01(\\x02\\\"X\\n\\x12SGTRegressorConfig\\x12\\r\\n\\x05\\x64\\x65lta\\x18\\x01 \\x01(\\x02\\x12\\x14\\n\\x0cgrace_period\\x18\\x02 \\x01(\\x05\\x12\\x0e\\n\\x06lambda\\x18\\x03 \\x01(\\x02\\x12\\r\\n\\x05gamma\\x18\\x04 \\x01(\\x02\\\"\\x92\\x01\\n\\x13RandomSamplerConfig\\x12\\x11\\n\\tn_classes\\x18\\x01 \\x01(\\x05\\x12\\x14\\n\\x0c\\x64\\x65sired_dist\\x18\\x02 \\x03(\\x02\\x12\\x17\\n\\x0fsampling_method\\x18\\x03 \\x01(\\t\\x12\\x15\\n\\rsampling_rate\\x18\\x04 \\x01(\\x02\\x12\\x0c\\n\\x04seed\\x18\\x05 \\x01(\\x05\\x12\\x14\\n\\x0cnum_children\\x18\\x06 \\x01(\\x05\\\"5\\n\\x11MultinomialConfig\\x12\\x11\\n\\tn_classes\\x18\\x01 \\x01(\\x05\\x12\\r\\n\\x05\\x61lpha\\x18\\x02 \\x01(\\x02\\\"#\\n\\x0eGaussianConfig\\x12\\x11\\n\\tn_classes\\x18\\x01 \\x01(\\x05\\\"X\\n\\x0cPythonConfig\\x12\\x13\\n\\x0bmodule_name\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\nclass_name\\x18\\x02 \\x01(\\t\\x12\\x11\\n\\tvenv_name\\x18\\x03 \\x01(\\t\\x12\\x0c\\n\\x04\\x63ode\\x18\\x04 \\x01(\\t\\\"`\\n\\x14PythonEnsembleConfig\\x12\\x13\\n\\x0bmodule_name\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\nclass_name\\x18\\x02 \\x01(\\t\\x12\\x11\\n\\tvenv_name\\x18\\x03 \\x01(\\t\\x12\\x0c\\n\\x04\\x63ode\\x18\\x04 \\x01(\\t\\\"\\xbf\\x01\\n\\x12PreProcessorConfig\\x12\\x19\\n\\x11preprocessor_name\\x18\\x01 \\x01(\\t\\x12\\x17\\n\\x0ftext_categories\\x18\\x02 \\x03(\\x05\\x12\\x14\\n\\x0cnum_children\\x18\\x03 \\x01(\\x05\\x12\\x15\\n\\rgguf_model_id\\x18\\x04 \\x01(\\t\\x12\\x1c\\n\\x14max_tokens_per_input\\x18\\x05 \\x01(\\x05\\x12\\x13\\n\\x0bimage_sizes\\x18\\x06 \\x03(\\x05\\x12\\x15\\n\\rchannel_first\\x18\\x07 \\x01(\\x08\\\"\\x8f\\x02\\n\\x15\\x41\\x64\\x61ptiveXGBoostConfig\\x12\\x11\\n\\tn_classes\\x18\\x01 \\x01(\\x05\\x12\\x15\\n\\rlearning_rate\\x18\\x02 \\x01(\\x02\\x12\\x11\\n\\tmax_depth\\x18\\x03 \\x01(\\x05\\x12\\x17\\n\\x0fmax_window_size\\x18\\x04 \\x01(\\x05\\x12\\x17\\n\\x0fmin_window_size\\x18\\x05 \\x01(\\x05\\x12\\x12\\n\\nmax_buffer\\x18\\x06 \\x01(\\x05\\x12\\x11\\n\\tpre_train\\x18\\x07 \\x01(\\x05\\x12\\x14\\n\\x0c\\x64\\x65tect_drift\\x18\\x08 \\x01(\\x08\\x12\\x13\\n\\x0buse_updater\\x18\\t \\x01(\\x08\\x12\\x17\\n\\x0ftrees_per_train\\x18\\n \\x01(\\x05\\x12\\x1c\\n\\x14percent_update_trees\\x18\\x0b \\x01(\\x02\\\"\\xee\\x01\\n\\x12\\x41\\x64\\x61ptiveLGBMConfig\\x12\\x11\\n\\tn_classes\\x18\\x01 \\x01(\\x05\\x12\\x15\\n\\rlearning_rate\\x18\\x02 \\x01(\\x02\\x12\\x11\\n\\tmax_depth\\x18\\x03 \\x01(\\x05\\x12\\x17\\n\\x0fmax_window_size\\x18\\x04 \\x01(\\x05\\x12\\x17\\n\\x0fmin_window_size\\x18\\x05 \\x01(\\x05\\x12\\x12\\n\\nmax_buffer\\x18\\x06 \\x01(\\x05\\x12\\x11\\n\\tpre_train\\x18\\x07 \\x01(\\x05\\x12\\x14\\n\\x0c\\x64\\x65tect_drift\\x18\\x08 \\x01(\\x08\\x12\\x13\\n\\x0buse_updater\\x18\\t \\x01(\\x08\\x12\\x17\\n\\x0ftrees_per_train\\x18\\n \\x01(\\x05\\\"t\\n\\x13RestAPIClientConfig\\x12\\x12\\n\\nserver_url\\x18\\x01 \\x01(\\t\\x12\\x13\\n\\x0bmax_retries\\x18\\x02 \\x01(\\x05\\x12\\x1a\\n\\x12\\x63onnection_timeout\\x18\\x03 \\x01(\\x05\\x12\\x18\\n\\x10max_request_time\\x18\\x04 \\x01(\\x05\\\"q\\n\\x10GRPCClientConfig\\x12\\x12\\n\\nserver_url\\x18\\x01 \\x01(\\t\\x12\\x13\\n\\x0bmax_retries\\x18\\x02 \\x01(\\x05\\x12\\x1a\\n\\x12\\x63onnection_timeout\\x18\\x03 \\x01(\\x05\\x12\\x18\\n\\x10max_request_time\\x18\\x04 \\x01(\\x05\\\"P\\n\\x19LLAMAEmbeddingModelConfig\\x12\\x15\\n\\rgguf_model_id\\x18\\x01 \\x01(\\t\\x12\\x1c\\n\\x14max_tokens_per_input\\x18\\x02 \\x01(\\x05\\\">\\n\\x0fLlamaTextConfig\\x12\\x15\\n\\rgguf_model_id\\x18\\x01 \\x01(\\t\\x12\\x14\\n\\x0cnum_children\\x18\\x02 \\x01(\\x05\\\"B\\n\\x13\\x43lipEmbeddingConfig\\x12\\x15\\n\\rgguf_model_id\\x18\\x01 \\x01(\\t\\x12\\x14\\n\\x0cnum_children\\x18\\x02 \\x01(\\x05\\\"\\xdf\\x12\\n\\x0bModelConfig\\x12\\x11\\n\\talgorithm\\x18\\x01 \\x01(\\t\\x12\\x14\\n\\x0cnum_children\\x18& \\x01(\\x05\\x12/\\n\\x0emstream_config\\x18\\x02 \\x01(\\x0b\\x32\\x15.config.MStreamConfigH\\x00\\x12\\'\\n\\nrcf_config\\x18\\x03 \\x01(\\x0b\\x32\\x11.config.RCFConfigH\\x00\\x12\\'\\n\\nhst_config\\x18\\x04 \\x01(\\x0b\\x32\\x11.config.HSTConfigH\\x00\\x12)\\n\\x0bonnx_config\\x18\\x06 \\x01(\\x0b\\x32\\x12.config.ONNXConfigH\\x00\\x12H\\n\\x1bhoeffding_classifier_config\\x18\\x07 \\x01(\\x0b\\x32!.config.HoeffdingClassifierConfigH\\x00\\x12\\x46\\n\\x1ahoeffding_regressor_config\\x18\\x08 \\x01(\\x0b\\x32 .config.HoeffdingRegressorConfigH\\x00\\x12<\\n\\x15\\x61mf_classifier_config\\x18\\t \\x01(\\x0b\\x32\\x1b.config.AMFClassifierConfigH\\x00\\x12:\\n\\x14\\x61mf_regressor_config\\x18\\n \\x01(\\x0b\\x32\\x1a.config.AMFRegressorConfigH\\x00\\x12<\\n\\x15\\x66\\x66m_classifier_config\\x18\\x0b \\x01(\\x0b\\x32\\x1b.config.FFMClassifierConfigH\\x00\\x12:\\n\\x14\\x66\\x66m_regressor_config\\x18\\x0c \\x01(\\x0b\\x32\\x1a.config.FFMRegressorConfigH\\x00\\x12\\x31\\n\\x0fsnarimax_config\\x18\\r \\x01(\\x0b\\x32\\x16.config.SNARIMAXConfigH\\x00\\x12\\x30\\n\\tnn_config\\x18\\x0e \\x01(\\x0b\\x32\\x1b.config.NeuralNetworkConfigH\\x00\\x12\\'\\n\\nonn_config\\x18\\x0f \\x01(\\x0b\\x32\\x11.config.ONNConfigH\\x00\\x12Y\\n$leveraging_bagging_classifier_config\\x18\\x10 \\x01(\\x0b\\x32).config.LeveragingBaggingClassifierConfigH\\x00\\x12\\x46\\n\\x1a\\x61\\x64\\x61\\x62oost_classifier_config\\x18\\x11 \\x01(\\x0b\\x32 .config.AdaBoostClassifierConfigH\\x00\\x12<\\n\\x15random_sampler_config\\x18\\x12 \\x01(\\x0b\\x32\\x1b.config.RandomSamplerConfigH\\x00\\x12K\\n\\x1d\\x62\\x61ndit_model_selection_config\\x18\\x13 \\x01(\\x0b\\x32\\\".config.BanditModelSelectionConfigH\\x00\\x12`\\n(contextual_bandit_model_selection_config\\x18\\x14 \\x01(\\x0b\\x32,.config.ContextualBanditModelSelectionConfigH\\x00\\x12-\\n\\rpython_config\\x18\\x15 \\x01(\\x0b\\x32\\x14.config.PythonConfigH\\x00\\x12\\x39\\n\\x13preprocessor_config\\x18\\x16 \\x01(\\x0b\\x32\\x1a.config.PreProcessorConfigH\\x00\\x12\\x37\\n\\x1aovr_model_selection_config\\x18\\x17 \\x01(\\x0b\\x32\\x11.config.OVRConfigH\\x00\\x12K\\n\\x18random_projection_config\\x18\\x18 \\x01(\\x0b\\x32\\'.config.RandomProjectionEmbeddingConfigH\\x00\\x12>\\n\\x16\\x65mbedding_model_config\\x18\\x19 \\x01(\\x0b\\x32\\x1c.config.EmbeddingModelConfigH\\x00\\x12\\x66\\n+hetero_leveraging_bagging_classifier_config\\x18\\x1a \\x01(\\x0b\\x32/.config.HeteroLeveragingBaggingClassifierConfigH\\x00\\x12S\\n!hetero_adaboost_classifier_config\\x18\\x1b \\x01(\\x0b\\x32&.config.HeteroAdaBoostClassifierConfigH\\x00\\x12<\\n\\x15sgt_classifier_config\\x18\\x1c \\x01(\\x0b\\x32\\x1b.config.SGTClassifierConfigH\\x00\\x12:\\n\\x14sgt_regressor_config\\x18\\x1d \\x01(\\x0b\\x32\\x1a.config.SGTRegressorConfigH\\x00\\x12\\x37\\n\\x12multinomial_config\\x18\\x1e \\x01(\\x0b\\x32\\x19.config.MultinomialConfigH\\x00\\x12\\x31\\n\\x0fgaussian_config\\x18\\x1f \\x01(\\x0b\\x32\\x16.config.GaussianConfigH\\x00\\x12@\\n\\x17\\x61\\x64\\x61ptive_xgboost_config\\x18  \\x01(\\x0b\\x32\\x1d.config.AdaptiveXGBoostConfigH\\x00\\x12:\\n\\x14\\x61\\x64\\x61ptive_lgbm_config\\x18! \\x01(\\x0b\\x32\\x1a.config.AdaptiveLGBMConfigH\\x00\\x12\\x43\\n\\x16llama_embedding_config\\x18\\\" \\x01(\\x0b\\x32!.config.LLAMAEmbeddingModelConfigH\\x00\\x12=\\n\\x16rest_api_client_config\\x18# \\x01(\\x0b\\x32\\x1b.config.RestAPIClientConfigH\\x00\\x12\\x34\\n\\x11llama_text_config\\x18$ \\x01(\\x0b\\x32\\x17.config.LlamaTextConfigH\\x00\\x12<\\n\\x15\\x63lip_embedding_config\\x18% \\x01(\\x0b\\x32\\x1b.config.ClipEmbeddingConfigH\\x00\\x12>\\n\\x16python_ensemble_config\\x18\\' \\x01(\\x0b\\x32\\x1c.config.PythonEnsembleConfigH\\x00\\x12\\x36\\n\\x12grpc_client_config\\x18( \\x01(\\x0b\\x32\\x18.config.GRPCClientConfigH\\x00\\x42\\x0e\\n\\x0cmodel_configJ\\x04\\x08\\x05\\x10\\x06\\\"^\\n\\x16\\x46\\x65\\x61tureRetrievalConfig\\x12\\x15\\n\\rsql_statement\\x18\\x01 \\x01(\\t\\x12\\x18\\n\\x10placeholder_cols\\x18\\x02 \\x03(\\t\\x12\\x13\\n\\x0bresult_cols\\x18\\x03 \\x03(\\t\\\"r\\n\\x13KafkaProducerConfig\\x12\\x13\\n\\x0bwrite_topic\\x18\\x01 \\x01(\\t\\x12\\x17\\n\\x0fproto_file_name\\x18\\x02 \\x01(\\t\\x12\\x1a\\n\\x12proto_message_name\\x18\\x03 \\x01(\\t\\x12\\x11\\n\\tschema_id\\x18\\x04 \\x01(\\x05\\\"v\\n\\x13KafkaConsumerConfig\\x12\\x12\\n\\nread_topic\\x18\\x01 \\x01(\\t\\x12\\x17\\n\\x0fproto_file_name\\x18\\x02 \\x01(\\t\\x12\\x1a\\n\\x12proto_message_name\\x18\\x03 \\x01(\\t\\x12\\x16\\n\\x0e\\x63onsumer_group\\x18\\x04 \\x01(\\t\\\"\\x93\\x01\\n\\x0bInputConfig\\x12\\x11\\n\\tkey_field\\x18\\x01 \\x01(\\t\\x12\\x13\\n\\x0blabel_field\\x18\\x02 \\x01(\\t\\x12\\x11\\n\\tnumerical\\x18\\x03 \\x03(\\t\\x12\\x13\\n\\x0b\\x63\\x61tegorical\\x18\\x04 \\x03(\\t\\x12\\x11\\n\\ttime_tick\\x18\\x05 \\x01(\\t\\x12\\x0f\\n\\x07textual\\x18\\x06 \\x03(\\t\\x12\\x10\\n\\x08imaginal\\x18\\x07 \\x03(\\t\\\"c\\n\\rLearnerConfig\\x12\\x17\\n\\x0fpredict_workers\\x18\\x01 \\x01(\\x05\\x12\\x19\\n\\x11update_batch_size\\x18\\x02 \\x01(\\x05\\x12\\x1e\\n\\x16synchronization_method\\x18\\x03 \\x01(\\t\\\"\\xd4\\x04\\n\\rTurboMLConfig\\x12\\x0f\\n\\x07\\x62rokers\\x18\\x01 \\x01(\\t\\x12\\x32\\n\\rfeat_consumer\\x18\\x02 \\x01(\\x0b\\x32\\x1b.config.KafkaConsumerConfig\\x12\\x34\\n\\x0foutput_producer\\x18\\x03 \\x01(\\x0b\\x32\\x1b.config.KafkaProducerConfig\\x12\\x33\\n\\x0elabel_consumer\\x18\\x04 \\x01(\\x0b\\x32\\x1b.config.KafkaConsumerConfig\\x12)\\n\\x0cinput_config\\x18\\x05 \\x01(\\x0b\\x32\\x13.config.InputConfig\\x12*\\n\\rmodel_configs\\x18\\x06 \\x03(\\x0b\\x32\\x13.config.ModelConfig\\x12\\x18\\n\\x10initial_model_id\\x18\\x07 \\x01(\\t\\x12\\x10\\n\\x08\\x61pi_port\\x18\\x08 \\x01(\\x05\\x12\\x12\\n\\narrow_port\\x18\\t \\x01(\\x05\\x12\\x39\\n\\x11\\x66\\x65\\x61ture_retrieval\\x18\\n \\x01(\\x0b\\x32\\x1e.config.FeatureRetrievalConfig\\x12\\x36\\n\\x11\\x63ombined_producer\\x18\\x0b \\x01(\\x0b\\x32\\x1b.config.KafkaProducerConfig\\x12\\x36\\n\\x11\\x63ombined_consumer\\x18\\x0c \\x01(\\x0b\\x32\\x1b.config.KafkaConsumerConfig\\x12-\\n\\x0elearner_config\\x18\\r \\x01(\\x0b\\x32\\x15.config.LearnerConfig\\x12\\\"\\n\\x1a\\x66ully_qualified_model_name\\x18\\x0e \\x01(\\t\\\"\\xac\\x01\\n\\x0eTrainJobConfig\\x12\\x19\\n\\x11initial_model_key\\x18\\x01 \\x01(\\t\\x12)\\n\\x0cinput_config\\x18\\x02 \\x01(\\x0b\\x32\\x13.config.InputConfig\\x12*\\n\\rmodel_configs\\x18\\x03 \\x03(\\x0b\\x32\\x13.config.ModelConfig\\x12\\x12\\n\\nmodel_name\\x18\\x04 \\x01(\\t\\x12\\x14\\n\\x0cversion_name\\x18\\x05 \\x01(\\t\\\"=\\n\\x0fModelConfigList\\x12*\\n\\rmodel_configs\\x18\\x01 \\x03(\\x0b\\x32\\x13.config.ModelConfig')\n\n_globals = globals()\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)\n_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'config_pb2', _globals)\nif _descriptor._USE_C_DESCRIPTORS == False:\nDESCRIPTOR._options = None\n_globals['_MSTREAMCONFIG']._serialized_start=24\n_globals['_MSTREAMCONFIG']._serialized_end=94\n_globals['_RCFCONFIG']._serialized_start=96\n_globals['_RCFCONFIG']._serialized_end=195\n_globals['_HSTCONFIG']._serialized_start=197\n_globals['_HSTCONFIG']._serialized_end=262\n_globals['_ONNXCONFIG']._serialized_start=264\n_globals['_ONNXCONFIG']._serialized_end=321\n_globals['_HOEFFDINGCLASSIFIERCONFIG']._serialized_start=324\n_globals['_HOEFFDINGCLASSIFIERCONFIG']._serialized_end=468\n_globals['_HOEFFDINGREGRESSORCONFIG']._serialized_start=470\n_globals['_HOEFFDINGREGRESSORCONFIG']._serialized_end=572\n_globals['_AMFCLASSIFIERCONFIG']._serialized_start=575\n_globals['_AMFCLASSIFIERCONFIG']._serialized_end=715\n_globals['_AMFREGRESSORCONFIG']._serialized_start=717\n_globals['_AMFREGRESSORCONFIG']._serialized_end=817\n_globals['_FFMCLASSIFIERCONFIG']._serialized_start=820\n_globals['_FFMCLASSIFIERCONFIG']._serialized_end=1000\n_globals['_FFMREGRESSORCONFIG']._serialized_start=1003\n_globals['_FFMREGRESSORCONFIG']._serialized_end=1182\n_globals['_SNARIMAXCONFIG']._serialized_start=1185\n_globals['_SNARIMAXCONFIG']._serialized_end=1320\n_globals['_NEURALNETWORKCONFIG']._serialized_start=1323\n_globals['_NEURALNETWORKCONFIG']._serialized_end=1659\n_globals['_NEURALNETWORKCONFIG_NEURALNETWORKLAYER']._serialized_start=1513\n_globals['_NEURALNETWORKCONFIG_NEURALNETWORKLAYER']._serialized_end=1659\n_globals['_ONNCONFIG']._serialized_start=1661\n_globals['_ONNCONFIG']._serialized_end=1788\n_globals['_OVRCONFIG']._serialized_start=1790\n_globals['_OVRCONFIG']._serialized_end=1842\n_globals['_RANDOMPROJECTIONEMBEDDINGCONFIG']._serialized_start=1844\n_globals['_RANDOMPROJECTIONEMBEDDINGCONFIG']._serialized_end=1923\n_globals['_EMBEDDINGMODELCONFIG']._serialized_start=1925\n_globals['_EMBEDDINGMODELCONFIG']._serialized_end=1969\n_globals['_BANDITMODELSELECTIONCONFIG']._serialized_start=1971\n_globals['_BANDITMODELSELECTIONCONFIG']._serialized_end=2036\n_globals['_CONTEXTUALBANDITMODELSELECTIONCONFIG']._serialized_start=2038\n_globals['_CONTEXTUALBANDITMODELSELECTIONCONFIG']._serialized_end=2123\n_globals['_LEVERAGINGBAGGINGCLASSIFIERCONFIG']._serialized_start=2126\n_globals['_LEVERAGINGBAGGINGCLASSIFIERCONFIG']._serialized_end=2269\n_globals['_HETEROLEVERAGINGBAGGINGCLASSIFIERCONFIG']._serialized_start=2272\n_globals['_HETEROLEVERAGINGBAGGINGCLASSIFIERCONFIG']._serialized_end=2403\n_globals['_ADABOOSTCLASSIFIERCONFIG']._serialized_start=2405\n_globals['_ADABOOSTCLASSIFIERCONFIG']._serialized_end=2504\n_globals['_HETEROADABOOSTCLASSIFIERCONFIG']._serialized_start=2506\n_globals['_HETEROADABOOSTCLASSIFIERCONFIG']._serialized_end=2593\n_globals['_SGTCLASSIFIERCONFIG']._serialized_start=2595\n_globals['_SGTCLASSIFIERCONFIG']._serialized_end=2684\n_globals['_SGTREGRESSORCONFIG']._serialized_start=2686\n_globals['_SGTREGRESSORCONFIG']._serialized_end=2774\n_globals['_RANDOMSAMPLERCONFIG']._serialized_start=2777\n_globals['_RANDOMSAMPLERCONFIG']._serialized_end=2923\n_globals['_MULTINOMIALCONFIG']._serialized_start=2925\n_globals['_MULTINOMIALCONFIG']._serialized_end=2978\n_globals['_GAUSSIANCONFIG']._serialized_start=2980\n_globals['_GAUSSIANCONFIG']._serialized_end=3015\n_globals['_PYTHONCONFIG']._serialized_start=3017\n_globals['_PYTHONCONFIG']._serialized_end=3105\n_globals['_PYTHONENSEMBLECONFIG']._serialized_start=3107\n_globals['_PYTHONENSEMBLECONFIG']._serialized_end=3203\n_globals['_PREPROCESSORCONFIG']._serialized_start=3206\n_globals['_PREPROCESSORCONFIG']._serialized_end=3397\n_globals['_ADAPTIVEXGBOOSTCONFIG']._serialized_start=3400\n_globals['_ADAPTIVEXGBOOSTCONFIG']._serialized_end=3671\n_globals['_ADAPTIVELGBMCONFIG']._serialized_start=3674\n_globals['_ADAPTIVELGBMCONFIG']._serialized_end=3912\n_globals['_RESTAPICLIENTCONFIG']._serialized_start=3914\n_globals['_RESTAPICLIENTCONFIG']._serialized_end=4030\n_globals['_GRPCCLIENTCONFIG']._serialized_start=4032\n_globals['_GRPCCLIENTCONFIG']._serialized_end=4145\n_globals['_LLAMAEMBEDDINGMODELCONFIG']._serialized_start=4147\n_globals['_LLAMAEMBEDDINGMODELCONFIG']._serialized_end=4227\n_globals['_LLAMATEXTCONFIG']._serialized_start=4229\n_globals['_LLAMATEXTCONFIG']._serialized_end=4291\n_globals['_CLIPEMBEDDINGCONFIG']._serialized_start=4293\n_globals['_CLIPEMBEDDINGCONFIG']._serialized_end=4359\n_globals['_MODELCONFIG']._serialized_start=4362\n_globals['_MODELCONFIG']._serialized_end=6761\n_globals['_FEATURERETRIEVALCONFIG']._serialized_start=6763\n_globals['_FEATURERETRIEVALCONFIG']._serialized_end=6857\n_globals['_KAFKAPRODUCERCONFIG']._serialized_start=6859\n_globals['_KAFKAPRODUCERCONFIG']._serialized_end=6973\n_globals['_KAFKACONSUMERCONFIG']._serialized_start=6975\n_globals['_KAFKACONSUMERCONFIG']._serialized_end=7093\n_globals['_INPUTCONFIG']._serialized_start=7096\n_globals['_INPUTCONFIG']._serialized_end=7243\n_globals['_LEARNERCONFIG']._serialized_start=7245\n_globals['_LEARNERCONFIG']._serialized_end=7344\n_globals['_TURBOMLCONFIG']._serialized_start=7347\n_globals['_TURBOMLCONFIG']._serialized_end=7943\n_globals['_TRAINJOBCONFIG']._serialized_start=7946\n_globals['_TRAINJOBCONFIG']._serialized_end=8118\n_globals['_MODELCONFIGLIST']._serialized_start=8120\n_globals['_MODELCONFIGLIST']._serialized_end=8181\n# @@protoc_insertion_point(module_scope)\n\n```  \n#### config_pb2.pyi\n```python\nfrom google.protobuf.internal import containers as _containers\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom typing import ClassVar as _ClassVar, Iterable as _Iterable, Mapping as _Mapping, Optional as _Optional, Union as _Union\n\nDESCRIPTOR: _descriptor.FileDescriptor\n\nclass MStreamConfig(_message.Message):\n__slots__ = (\"num_rows\", \"num_buckets\", \"factor\")\nNUM_ROWS_FIELD_NUMBER: _ClassVar[int]\nNUM_BUCKETS_FIELD_NUMBER: _ClassVar[int]\nFACTOR_FIELD_NUMBER: _ClassVar[int]\nnum_rows: int\nnum_buckets: int\nfactor: float\ndef __init__(self, num_rows: _Optional[int] = ..., num_buckets: _Optional[int] = ..., factor: _Optional[float] = ...) -> None: ...\n\nclass RCFConfig(_message.Message):\n__slots__ = (\"time_decay\", \"number_of_trees\", \"output_after\", \"sample_size\")\nTIME_DECAY_FIELD_NUMBER: _ClassVar[int]\nNUMBER_OF_TREES_FIELD_NUMBER: _ClassVar[int]\nOUTPUT_AFTER_FIELD_NUMBER: _ClassVar[int]\nSAMPLE_SIZE_FIELD_NUMBER: _ClassVar[int]\ntime_decay: float\nnumber_of_trees: int\noutput_after: int\nsample_size: int\ndef __init__(self, time_decay: _Optional[float] = ..., number_of_trees: _Optional[int] = ..., output_after: _Optional[int] = ..., sample_size: _Optional[int] = ...) -> None: ...\n\nclass HSTConfig(_message.Message):\n__slots__ = (\"n_trees\", \"height\", \"window_size\")\nN_TREES_FIELD_NUMBER: _ClassVar[int]\nHEIGHT_FIELD_NUMBER: _ClassVar[int]\nWINDOW_SIZE_FIELD_NUMBER: _ClassVar[int]\nn_trees: int\nheight: int\nwindow_size: int\ndef __init__(self, n_trees: _Optional[int] = ..., height: _Optional[int] = ..., window_size: _Optional[int] = ...) -> None: ...\n\nclass ONNXConfig(_message.Message):\n__slots__ = (\"model_save_name\", \"model_data\")\nMODEL_SAVE_NAME_FIELD_NUMBER: _ClassVar[int]\nMODEL_DATA_FIELD_NUMBER: _ClassVar[int]\nmodel_save_name: str\nmodel_data: bytes\ndef __init__(self, model_save_name: _Optional[str] = ..., model_data: _Optional[bytes] = ...) -> None: ...\n\nclass HoeffdingClassifierConfig(_message.Message):\n__slots__ = (\"delta\", \"tau\", \"grace_period\", \"n_classes\", \"leaf_pred_method\", \"split_method\")\nDELTA_FIELD_NUMBER: _ClassVar[int]\nTAU_FIELD_NUMBER: _ClassVar[int]\nGRACE_PERIOD_FIELD_NUMBER: _ClassVar[int]\nN_CLASSES_FIELD_NUMBER: _ClassVar[int]\nLEAF_PRED_METHOD_FIELD_NUMBER: _ClassVar[int]\nSPLIT_METHOD_FIELD_NUMBER: _ClassVar[int]\ndelta: float\ntau: float\ngrace_period: int\nn_classes: int\nleaf_pred_method: str\nsplit_method: str\ndef __init__(self, delta: _Optional[float] = ..., tau: _Optional[float] = ..., grace_period: _Optional[int] = ..., n_classes: _Optional[int] = ..., leaf_pred_method: _Optional[str] = ..., split_method: _Optional[str] = ...) -> None: ...\n\nclass HoeffdingRegressorConfig(_message.Message):\n__slots__ = (\"delta\", \"tau\", \"grace_period\", \"leaf_pred_method\")\nDELTA_FIELD_NUMBER: _ClassVar[int]\nTAU_FIELD_NUMBER: _ClassVar[int]\nGRACE_PERIOD_FIELD_NUMBER: _ClassVar[int]\nLEAF_PRED_METHOD_FIELD_NUMBER: _ClassVar[int]\ndelta: float\ntau: float\ngrace_period: int\nleaf_pred_method: str\ndef __init__(self, delta: _Optional[float] = ..., tau: _Optional[float] = ..., grace_period: _Optional[int] = ..., leaf_pred_method: _Optional[str] = ...) -> None: ...\n\nclass AMFClassifierConfig(_message.Message):\n__slots__ = (\"n_classes\", \"n_estimators\", \"step\", \"use_aggregation\", \"dirichlet\", \"split_pure\")\nN_CLASSES_FIELD_NUMBER: _ClassVar[int]\nN_ESTIMATORS_FIELD_NUMBER: _ClassVar[int]\nSTEP_FIELD_NUMBER: _ClassVar[int]\nUSE_AGGREGATION_FIELD_NUMBER: _ClassVar[int]\nDIRICHLET_FIELD_NUMBER: _ClassVar[int]\nSPLIT_PURE_FIELD_NUMBER: _ClassVar[int]\nn_classes: int\nn_estimators: int\nstep: float\nuse_aggregation: bool\ndirichlet: float\nsplit_pure: bool\ndef __init__(self, n_classes: _Optional[int] = ..., n_estimators: _Optional[int] = ..., step: _Optional[float] = ..., use_aggregation: bool = ..., dirichlet: _Optional[float] = ..., split_pure: bool = ...) -> None: ...\n\nclass AMFRegressorConfig(_message.Message):\n__slots__ = (\"n_estimators\", \"step\", \"use_aggregation\", \"dirichlet\")\nN_ESTIMATORS_FIELD_NUMBER: _ClassVar[int]\nSTEP_FIELD_NUMBER: _ClassVar[int]\nUSE_AGGREGATION_FIELD_NUMBER: _ClassVar[int]\nDIRICHLET_FIELD_NUMBER: _ClassVar[int]\nn_estimators: int\nstep: float\nuse_aggregation: bool\ndirichlet: float\ndef __init__(self, n_estimators: _Optional[int] = ..., step: _Optional[float] = ..., use_aggregation: bool = ..., dirichlet: _Optional[float] = ...) -> None: ...\n\nclass FFMClassifierConfig(_message.Message):\n__slots__ = (\"n_factors\", \"l1_weight\", \"l2_weight\", \"l1_latent\", \"l2_latent\", \"intercept\", \"intercept_lr\", \"clip_gradient\")\nN_FACTORS_FIELD_NUMBER: _ClassVar[int]\nL1_WEIGHT_FIELD_NUMBER: _ClassVar[int]\nL2_WEIGHT_FIELD_NUMBER: _ClassVar[int]\nL1_LATENT_FIELD_NUMBER: _ClassVar[int]\nL2_LATENT_FIELD_NUMBER: _ClassVar[int]\nINTERCEPT_FIELD_NUMBER: _ClassVar[int]\nINTERCEPT_LR_FIELD_NUMBER: _ClassVar[int]\nCLIP_GRADIENT_FIELD_NUMBER: _ClassVar[int]\nn_factors: int\nl1_weight: float\nl2_weight: float\nl1_latent: float\nl2_latent: float\nintercept: float\nintercept_lr: float\nclip_gradient: float\ndef __init__(self, n_factors: _Optional[int] = ..., l1_weight: _Optional[float] = ..., l2_weight: _Optional[float] = ..., l1_latent: _Optional[float] = ..., l2_latent: _Optional[float] = ..., intercept: _Optional[float] = ..., intercept_lr: _Optional[float] = ..., clip_gradient: _Optional[float] = ...) -> None: ...\n\nclass FFMRegressorConfig(_message.Message):\n__slots__ = (\"n_factors\", \"l1_weight\", \"l2_weight\", \"l1_latent\", \"l2_latent\", \"intercept\", \"intercept_lr\", \"clip_gradient\")\nN_FACTORS_FIELD_NUMBER: _ClassVar[int]\nL1_WEIGHT_FIELD_NUMBER: _ClassVar[int]\nL2_WEIGHT_FIELD_NUMBER: _ClassVar[int]\nL1_LATENT_FIELD_NUMBER: _ClassVar[int]\nL2_LATENT_FIELD_NUMBER: _ClassVar[int]\nINTERCEPT_FIELD_NUMBER: _ClassVar[int]\nINTERCEPT_LR_FIELD_NUMBER: _ClassVar[int]\nCLIP_GRADIENT_FIELD_NUMBER: _ClassVar[int]\nn_factors: int\nl1_weight: float\nl2_weight: float\nl1_latent: float\nl2_latent: float\nintercept: float\nintercept_lr: float\nclip_gradient: float\ndef __init__(self, n_factors: _Optional[int] = ..., l1_weight: _Optional[float] = ..., l2_weight: _Optional[float] = ..., l1_latent: _Optional[float] = ..., l2_latent: _Optional[float] = ..., intercept: _Optional[float] = ..., intercept_lr: _Optional[float] = ..., clip_gradient: _Optional[float] = ...) -> None: ...\n\nclass SNARIMAXConfig(_message.Message):\n__slots__ = (\"horizon\", \"p\", \"d\", \"q\", \"m\", \"sp\", \"sd\", \"sq\", \"num_children\")\nHORIZON_FIELD_NUMBER: _ClassVar[int]\nP_FIELD_NUMBER: _ClassVar[int]\nD_FIELD_NUMBER: _ClassVar[int]\nQ_FIELD_NUMBER: _ClassVar[int]\nM_FIELD_NUMBER: _ClassVar[int]\nSP_FIELD_NUMBER: _ClassVar[int]\nSD_FIELD_NUMBER: _ClassVar[int]\nSQ_FIELD_NUMBER: _ClassVar[int]\nNUM_CHILDREN_FIELD_NUMBER: _ClassVar[int]\nhorizon: int\np: int\nd: int\nq: int\nm: int\nsp: int\nsd: int\nsq: int\nnum_children: int\ndef __init__(self, horizon: _Optional[int] = ..., p: _Optional[int] = ..., d: _Optional[int] = ..., q: _Optional[int] = ..., m: _Optional[int] = ..., sp: _Optional[int] = ..., sd: _Optional[int] = ..., sq: _Optional[int] = ..., num_children: _Optional[int] = ...) -> None: ...\n\nclass NeuralNetworkConfig(_message.Message):\n__slots__ = (\"layers\", \"dropout\", \"loss_function\", \"optimizer\", \"learning_rate\", \"batch_size\")\nclass NeuralNetworkLayer(_message.Message):\n__slots__ = (\"input_size\", \"output_size\", \"activation\", \"dropout\", \"residual_connections\", \"use_bias\")\nINPUT_SIZE_FIELD_NUMBER: _ClassVar[int]\nOUTPUT_SIZE_FIELD_NUMBER: _ClassVar[int]\nACTIVATION_FIELD_NUMBER: _ClassVar[int]\nDROPOUT_FIELD_NUMBER: _ClassVar[int]\nRESIDUAL_CONNECTIONS_FIELD_NUMBER: _ClassVar[int]\nUSE_BIAS_FIELD_NUMBER: _ClassVar[int]\ninput_size: int\noutput_size: int\nactivation: str\ndropout: float\nresidual_connections: _containers.RepeatedScalarFieldContainer[int]\nuse_bias: bool\ndef __init__(self, input_size: _Optional[int] = ..., output_size: _Optional[int] = ..., activation: _Optional[str] = ..., dropout: _Optional[float] = ..., residual_connections: _Optional[_Iterable[int]] = ..., use_bias: bool = ...) -> None: ...\nLAYERS_FIELD_NUMBER: _ClassVar[int]\nDROPOUT_FIELD_NUMBER: _ClassVar[int]\nLOSS_FUNCTION_FIELD_NUMBER: _ClassVar[int]\nOPTIMIZER_FIELD_NUMBER: _ClassVar[int]\nLEARNING_RATE_FIELD_NUMBER: _ClassVar[int]\nBATCH_SIZE_FIELD_NUMBER: _ClassVar[int]\nlayers: _containers.RepeatedCompositeFieldContainer[NeuralNetworkConfig.NeuralNetworkLayer]\ndropout: float\nloss_function: str\noptimizer: str\nlearning_rate: float\nbatch_size: int\ndef __init__(self, layers: _Optional[_Iterable[_Union[NeuralNetworkConfig.NeuralNetworkLayer, _Mapping]]] = ..., dropout: _Optional[float] = ..., loss_function: _Optional[str] = ..., optimizer: _Optional[str] = ..., learning_rate: _Optional[float] = ..., batch_size: _Optional[int] = ...) -> None: ...\n\nclass ONNConfig(_message.Message):\n__slots__ = (\"max_num_hidden_layers\", \"qtd_neuron_hidden_layer\", \"n_classes\", \"b\", \"n\", \"s\")\nMAX_NUM_HIDDEN_LAYERS_FIELD_NUMBER: _ClassVar[int]\nQTD_NEURON_HIDDEN_LAYER_FIELD_NUMBER: _ClassVar[int]\nN_CLASSES_FIELD_NUMBER: _ClassVar[int]\nB_FIELD_NUMBER: _ClassVar[int]\nN_FIELD_NUMBER: _ClassVar[int]\nS_FIELD_NUMBER: _ClassVar[int]\nmax_num_hidden_layers: int\nqtd_neuron_hidden_layer: int\nn_classes: int\nb: float\nn: float\ns: float\ndef __init__(self, max_num_hidden_layers: _Optional[int] = ..., qtd_neuron_hidden_layer: _Optional[int] = ..., n_classes: _Optional[int] = ..., b: _Optional[float] = ..., n: _Optional[float] = ..., s: _Optional[float] = ...) -> None: ...\n\nclass OVRConfig(_message.Message):\n__slots__ = (\"n_classes\", \"num_children\")\nN_CLASSES_FIELD_NUMBER: _ClassVar[int]\nNUM_CHILDREN_FIELD_NUMBER: _ClassVar[int]\nn_classes: int\nnum_children: int\ndef __init__(self, n_classes: _Optional[int] = ..., num_children: _Optional[int] = ...) -> None: ...\n\nclass RandomProjectionEmbeddingConfig(_message.Message):\n__slots__ = (\"n_embeddings\", \"type_embedding\")\nN_EMBEDDINGS_FIELD_NUMBER: _ClassVar[int]\nTYPE_EMBEDDING_FIELD_NUMBER: _ClassVar[int]\nn_embeddings: int\ntype_embedding: str\ndef __init__(self, n_embeddings: _Optional[int] = ..., type_embedding: _Optional[str] = ...) -> None: ...\n\nclass EmbeddingModelConfig(_message.Message):\n__slots__ = (\"num_children\",)\nNUM_CHILDREN_FIELD_NUMBER: _ClassVar[int]\nnum_children: int\ndef __init__(self, num_children: _Optional[int] = ...) -> None: ...\n\nclass BanditModelSelectionConfig(_message.Message):\n__slots__ = (\"bandit\", \"metric_name\")\nBANDIT_FIELD_NUMBER: _ClassVar[int]\nMETRIC_NAME_FIELD_NUMBER: _ClassVar[int]\nbandit: str\nmetric_name: str\ndef __init__(self, bandit: _Optional[str] = ..., metric_name: _Optional[str] = ...) -> None: ...\n\nclass ContextualBanditModelSelectionConfig(_message.Message):\n__slots__ = (\"contextualbandit\", \"metric_name\")\nCONTEXTUALBANDIT_FIELD_NUMBER: _ClassVar[int]\nMETRIC_NAME_FIELD_NUMBER: _ClassVar[int]\ncontextualbandit: str\nmetric_name: str\ndef __init__(self, contextualbandit: _Optional[str] = ..., metric_name: _Optional[str] = ...) -> None: ...\n\nclass LeveragingBaggingClassifierConfig(_message.Message):\n__slots__ = (\"n_models\", \"n_classes\", \"w\", \"bagging_method\", \"seed\", \"num_children\")\nN_MODELS_FIELD_NUMBER: _ClassVar[int]\nN_CLASSES_FIELD_NUMBER: _ClassVar[int]\nW_FIELD_NUMBER: _ClassVar[int]\nBAGGING_METHOD_FIELD_NUMBER: _ClassVar[int]\nSEED_FIELD_NUMBER: _ClassVar[int]\nNUM_CHILDREN_FIELD_NUMBER: _ClassVar[int]\nn_models: int\nn_classes: int\nw: float\nbagging_method: str\nseed: int\nnum_children: int\ndef __init__(self, n_models: _Optional[int] = ..., n_classes: _Optional[int] = ..., w: _Optional[float] = ..., bagging_method: _Optional[str] = ..., seed: _Optional[int] = ..., num_children: _Optional[int] = ...) -> None: ...\n\nclass HeteroLeveragingBaggingClassifierConfig(_message.Message):\n__slots__ = (\"n_classes\", \"w\", \"bagging_method\", \"seed\", \"num_children\")\nN_CLASSES_FIELD_NUMBER: _ClassVar[int]\nW_FIELD_NUMBER: _ClassVar[int]\nBAGGING_METHOD_FIELD_NUMBER: _ClassVar[int]\nSEED_FIELD_NUMBER: _ClassVar[int]\nNUM_CHILDREN_FIELD_NUMBER: _ClassVar[int]\nn_classes: int\nw: float\nbagging_method: str\nseed: int\nnum_children: int\ndef __init__(self, n_classes: _Optional[int] = ..., w: _Optional[float] = ..., bagging_method: _Optional[str] = ..., seed: _Optional[int] = ..., num_children: _Optional[int] = ...) -> None: ...\n\nclass AdaBoostClassifierConfig(_message.Message):\n__slots__ = (\"n_models\", \"n_classes\", \"seed\", \"num_children\")\nN_MODELS_FIELD_NUMBER: _ClassVar[int]\nN_CLASSES_FIELD_NUMBER: _ClassVar[int]\nSEED_FIELD_NUMBER: _ClassVar[int]\nNUM_CHILDREN_FIELD_NUMBER: _ClassVar[int]\nn_models: int\nn_classes: int\nseed: int\nnum_children: int\ndef __init__(self, n_models: _Optional[int] = ..., n_classes: _Optional[int] = ..., seed: _Optional[int] = ..., num_children: _Optional[int] = ...) -> None: ...\n\nclass HeteroAdaBoostClassifierConfig(_message.Message):\n__slots__ = (\"n_classes\", \"seed\", \"num_children\")\nN_CLASSES_FIELD_NUMBER: _ClassVar[int]\nSEED_FIELD_NUMBER: _ClassVar[int]\nNUM_CHILDREN_FIELD_NUMBER: _ClassVar[int]\nn_classes: int\nseed: int\nnum_children: int\ndef __init__(self, n_classes: _Optional[int] = ..., seed: _Optional[int] = ..., num_children: _Optional[int] = ...) -> None: ...\n\nclass SGTClassifierConfig(_message.Message):\n__slots__ = (\"delta\", \"grace_period\", \"gamma\")\nDELTA_FIELD_NUMBER: _ClassVar[int]\nGRACE_PERIOD_FIELD_NUMBER: _ClassVar[int]\nLAMBDA_FIELD_NUMBER: _ClassVar[int]\nGAMMA_FIELD_NUMBER: _ClassVar[int]\ndelta: float\ngrace_period: int\ngamma: float\ndef __init__(self, delta: _Optional[float] = ..., grace_period: _Optional[int] = ..., gamma: _Optional[float] = ..., **kwargs) -> None: ...\n\nclass SGTRegressorConfig(_message.Message):\n__slots__ = (\"delta\", \"grace_period\", \"gamma\")\nDELTA_FIELD_NUMBER: _ClassVar[int]\nGRACE_PERIOD_FIELD_NUMBER: _ClassVar[int]\nLAMBDA_FIELD_NUMBER: _ClassVar[int]\nGAMMA_FIELD_NUMBER: _ClassVar[int]\ndelta: float\ngrace_period: int\ngamma: float\ndef __init__(self, delta: _Optional[float] = ..., grace_period: _Optional[int] = ..., gamma: _Optional[float] = ..., **kwargs) -> None: ...\n\nclass RandomSamplerConfig(_message.Message):\n__slots__ = (\"n_classes\", \"desired_dist\", \"sampling_method\", \"sampling_rate\", \"seed\", \"num_children\")\nN_CLASSES_FIELD_NUMBER: _ClassVar[int]\nDESIRED_DIST_FIELD_NUMBER: _ClassVar[int]\nSAMPLING_METHOD_FIELD_NUMBER: _ClassVar[int]\nSAMPLING_RATE_FIELD_NUMBER: _ClassVar[int]\nSEED_FIELD_NUMBER: _ClassVar[int]\nNUM_CHILDREN_FIELD_NUMBER: _ClassVar[int]\nn_classes: int\ndesired_dist: _containers.RepeatedScalarFieldContainer[float]\nsampling_method: str\nsampling_rate: float\nseed: int\nnum_children: int\ndef __init__(self, n_classes: _Optional[int] = ..., desired_dist: _Optional[_Iterable[float]] = ..., sampling_method: _Optional[str] = ..., sampling_rate: _Optional[float] = ..., seed: _Optional[int] = ..., num_children: _Optional[int] = ...) -> None: ...\n\nclass MultinomialConfig(_message.Message):\n__slots__ = (\"n_classes\", \"alpha\")\nN_CLASSES_FIELD_NUMBER: _ClassVar[int]\nALPHA_FIELD_NUMBER: _ClassVar[int]\nn_classes: int\nalpha: float\ndef __init__(self, n_classes: _Optional[int] = ..., alpha: _Optional[float] = ...) -> None: ...\n\nclass GaussianConfig(_message.Message):\n__slots__ = (\"n_classes\",)\nN_CLASSES_FIELD_NUMBER: _ClassVar[int]\nn_classes: int\ndef __init__(self, n_classes: _Optional[int] = ...) -> None: ...\n\nclass PythonConfig(_message.Message):\n__slots__ = (\"module_name\", \"class_name\", \"venv_name\", \"code\")\nMODULE_NAME_FIELD_NUMBER: _ClassVar[int]\nCLASS_NAME_FIELD_NUMBER: _ClassVar[int]\nVENV_NAME_FIELD_NUMBER: _ClassVar[int]\nCODE_FIELD_NUMBER: _ClassVar[int]\nmodule_name: str\nclass_name: str\nvenv_name: str\ncode: str\ndef __init__(self, module_name: _Optional[str] = ..., class_name: _Optional[str] = ..., venv_name: _Optional[str] = ..., code: _Optional[str] = ...) -> None: ...\n\nclass PythonEnsembleConfig(_message.Message):\n__slots__ = (\"module_name\", \"class_name\", \"venv_name\", \"code\")\nMODULE_NAME_FIELD_NUMBER: _ClassVar[int]\nCLASS_NAME_FIELD_NUMBER: _ClassVar[int]\nVENV_NAME_FIELD_NUMBER: _ClassVar[int]\nCODE_FIELD_NUMBER: _ClassVar[int]\nmodule_name: str\nclass_name: str\nvenv_name: str\ncode: str\ndef __init__(self, module_name: _Optional[str] = ..., class_name: _Optional[str] = ..., venv_name: _Optional[str] = ..., code: _Optional[str] = ...) -> None: ...\n\nclass PreProcessorConfig(_message.Message):\n__slots__ = (\"preprocessor_name\", \"text_categories\", \"num_children\", \"gguf_model_id\", \"max_tokens_per_input\", \"image_sizes\", \"channel_first\")\nPREPROCESSOR_NAME_FIELD_NUMBER: _ClassVar[int]\nTEXT_CATEGORIES_FIELD_NUMBER: _ClassVar[int]\nNUM_CHILDREN_FIELD_NUMBER: _ClassVar[int]\nGGUF_MODEL_ID_FIELD_NUMBER: _ClassVar[int]\nMAX_TOKENS_PER_INPUT_FIELD_NUMBER: _ClassVar[int]\nIMAGE_SIZES_FIELD_NUMBER: _ClassVar[int]\nCHANNEL_FIRST_FIELD_NUMBER: _ClassVar[int]\npreprocessor_name: str\ntext_categories: _containers.RepeatedScalarFieldContainer[int]\nnum_children: int\ngguf_model_id: str\nmax_tokens_per_input: int\nimage_sizes: _containers.RepeatedScalarFieldContainer[int]\nchannel_first: bool\ndef __init__(self, preprocessor_name: _Optional[str] = ..., text_categories: _Optional[_Iterable[int]] = ..., num_children: _Optional[int] = ..., gguf_model_id: _Optional[str] = ..., max_tokens_per_input: _Optional[int] = ..., image_sizes: _Optional[_Iterable[int]] = ..., channel_first: bool = ...) -> None: ...\n\nclass AdaptiveXGBoostConfig(_message.Message):\n__slots__ = (\"n_classes\", \"learning_rate\", \"max_depth\", \"max_window_size\", \"min_window_size\", \"max_buffer\", \"pre_train\", \"detect_drift\", \"use_updater\", \"trees_per_train\", \"percent_update_trees\")\nN_CLASSES_FIELD_NUMBER: _ClassVar[int]\nLEARNING_RATE_FIELD_NUMBER: _ClassVar[int]\nMAX_DEPTH_FIELD_NUMBER: _ClassVar[int]\nMAX_WINDOW_SIZE_FIELD_NUMBER: _ClassVar[int]\nMIN_WINDOW_SIZE_FIELD_NUMBER: _ClassVar[int]\nMAX_BUFFER_FIELD_NUMBER: _ClassVar[int]\nPRE_TRAIN_FIELD_NUMBER: _ClassVar[int]\nDETECT_DRIFT_FIELD_NUMBER: _ClassVar[int]\nUSE_UPDATER_FIELD_NUMBER: _ClassVar[int]\nTREES_PER_TRAIN_FIELD_NUMBER: _ClassVar[int]\nPERCENT_UPDATE_TREES_FIELD_NUMBER: _ClassVar[int]\nn_classes: int\nlearning_rate: float\nmax_depth: int\nmax_window_size: int\nmin_window_size: int\nmax_buffer: int\npre_train: int\ndetect_drift: bool\nuse_updater: bool\ntrees_per_train: int\npercent_update_trees: float\ndef __init__(self, n_classes: _Optional[int] = ..., learning_rate: _Optional[float] = ..., max_depth: _Optional[int] = ..., max_window_size: _Optional[int] = ..., min_window_size: _Optional[int] = ..., max_buffer: _Optional[int] = ..., pre_train: _Optional[int] = ..., detect_drift: bool = ..., use_updater: bool = ..., trees_per_train: _Optional[int] = ..., percent_update_trees: _Optional[float] = ...) -> None: ...\n\nclass AdaptiveLGBMConfig(_message.Message):\n__slots__ = (\"n_classes\", \"learning_rate\", \"max_depth\", \"max_window_size\", \"min_window_size\", \"max_buffer\", \"pre_train\", \"detect_drift\", \"use_updater\", \"trees_per_train\")\nN_CLASSES_FIELD_NUMBER: _ClassVar[int]\nLEARNING_RATE_FIELD_NUMBER: _ClassVar[int]\nMAX_DEPTH_FIELD_NUMBER: _ClassVar[int]\nMAX_WINDOW_SIZE_FIELD_NUMBER: _ClassVar[int]\nMIN_WINDOW_SIZE_FIELD_NUMBER: _ClassVar[int]\nMAX_BUFFER_FIELD_NUMBER: _ClassVar[int]\nPRE_TRAIN_FIELD_NUMBER: _ClassVar[int]\nDETECT_DRIFT_FIELD_NUMBER: _ClassVar[int]\nUSE_UPDATER_FIELD_NUMBER: _ClassVar[int]\nTREES_PER_TRAIN_FIELD_NUMBER: _ClassVar[int]\nn_classes: int\nlearning_rate: float\nmax_depth: int\nmax_window_size: int\nmin_window_size: int\nmax_buffer: int\npre_train: int\ndetect_drift: bool\nuse_updater: bool\ntrees_per_train: int\ndef __init__(self, n_classes: _Optional[int] = ..., learning_rate: _Optional[float] = ..., max_depth: _Optional[int] = ..., max_window_size: _Optional[int] = ..., min_window_size: _Optional[int] = ..., max_buffer: _Optional[int] = ..., pre_train: _Optional[int] = ..., detect_drift: bool = ..., use_updater: bool = ..., trees_per_train: _Optional[int] = ...) -> None: ...\n\nclass RestAPIClientConfig(_message.Message):\n__slots__ = (\"server_url\", \"max_retries\", \"connection_timeout\", \"max_request_time\")\nSERVER_URL_FIELD_NUMBER: _ClassVar[int]\nMAX_RETRIES_FIELD_NUMBER: _ClassVar[int]\nCONNECTION_TIMEOUT_FIELD_NUMBER: _ClassVar[int]\nMAX_REQUEST_TIME_FIELD_NUMBER: _ClassVar[int]\nserver_url: str\nmax_retries: int\nconnection_timeout: int\nmax_request_time: int\ndef __init__(self, server_url: _Optional[str] = ..., max_retries: _Optional[int] = ..., connection_timeout: _Optional[int] = ..., max_request_time: _Optional[int] = ...) -> None: ...\n\nclass GRPCClientConfig(_message.Message):\n__slots__ = (\"server_url\", \"max_retries\", \"connection_timeout\", \"max_request_time\")\nSERVER_URL_FIELD_NUMBER: _ClassVar[int]\nMAX_RETRIES_FIELD_NUMBER: _ClassVar[int]\nCONNECTION_TIMEOUT_FIELD_NUMBER: _ClassVar[int]\nMAX_REQUEST_TIME_FIELD_NUMBER: _ClassVar[int]\nserver_url: str\nmax_retries: int\nconnection_timeout: int\nmax_request_time: int\ndef __init__(self, server_url: _Optional[str] = ..., max_retries: _Optional[int] = ..., connection_timeout: _Optional[int] = ..., max_request_time: _Optional[int] = ...) -> None: ...\n\nclass LLAMAEmbeddingModelConfig(_message.Message):\n__slots__ = (\"gguf_model_id\", \"max_tokens_per_input\")\nGGUF_MODEL_ID_FIELD_NUMBER: _ClassVar[int]\nMAX_TOKENS_PER_INPUT_FIELD_NUMBER: _ClassVar[int]\ngguf_model_id: str\nmax_tokens_per_input: int\ndef __init__(self, gguf_model_id: _Optional[str] = ..., max_tokens_per_input: _Optional[int] = ...) -> None: ...\n\nclass LlamaTextConfig(_message.Message):\n__slots__ = (\"gguf_model_id\", \"num_children\")\nGGUF_MODEL_ID_FIELD_NUMBER: _ClassVar[int]\nNUM_CHILDREN_FIELD_NUMBER: _ClassVar[int]\ngguf_model_id: str\nnum_children: int\ndef __init__(self, gguf_model_id: _Optional[str] = ..., num_children: _Optional[int] = ...) -> None: ...\n\nclass ClipEmbeddingConfig(_message.Message):\n__slots__ = (\"gguf_model_id\", \"num_children\")\nGGUF_MODEL_ID_FIELD_NUMBER: _ClassVar[int]\nNUM_CHILDREN_FIELD_NUMBER: _ClassVar[int]\ngguf_model_id: str\nnum_children: int\ndef __init__(self, gguf_model_id: _Optional[str] = ..., num_children: _Optional[int] = ...) -> None: ...\n\nclass ModelConfig(_message.Message):\n__slots__ = (\"algorithm\", \"num_children\", \"mstream_config\", \"rcf_config\", \"hst_config\", \"onnx_config\", \"hoeffding_classifier_config\", \"hoeffding_regressor_config\", \"amf_classifier_config\", \"amf_regressor_config\", \"ffm_classifier_config\", \"ffm_regressor_config\", \"snarimax_config\", \"nn_config\", \"onn_config\", \"leveraging_bagging_classifier_config\", \"adaboost_classifier_config\", \"random_sampler_config\", \"bandit_model_selection_config\", \"contextual_bandit_model_selection_config\", \"python_config\", \"preprocessor_config\", \"ovr_model_selection_config\", \"random_projection_config\", \"embedding_model_config\", \"hetero_leveraging_bagging_classifier_config\", \"hetero_adaboost_classifier_config\", \"sgt_classifier_config\", \"sgt_regressor_config\", \"multinomial_config\", \"gaussian_config\", \"adaptive_xgboost_config\", \"adaptive_lgbm_config\", \"llama_embedding_config\", \"rest_api_client_config\", \"llama_text_config\", \"clip_embedding_config\", \"python_ensemble_config\", \"grpc_client_config\")\nALGORITHM_FIELD_NUMBER: _ClassVar[int]\nNUM_CHILDREN_FIELD_NUMBER: _ClassVar[int]\nMSTREAM_CONFIG_FIELD_NUMBER: _ClassVar[int]\nRCF_CONFIG_FIELD_NUMBER: _ClassVar[int]\nHST_CONFIG_FIELD_NUMBER: _ClassVar[int]\nONNX_CONFIG_FIELD_NUMBER: _ClassVar[int]\nHOEFFDING_CLASSIFIER_CONFIG_FIELD_NUMBER: _ClassVar[int]\nHOEFFDING_REGRESSOR_CONFIG_FIELD_NUMBER: _ClassVar[int]\nAMF_CLASSIFIER_CONFIG_FIELD_NUMBER: _ClassVar[int]\nAMF_REGRESSOR_CONFIG_FIELD_NUMBER: _ClassVar[int]\nFFM_CLASSIFIER_CONFIG_FIELD_NUMBER: _ClassVar[int]\nFFM_REGRESSOR_CONFIG_FIELD_NUMBER: _ClassVar[int]\nSNARIMAX_CONFIG_FIELD_NUMBER: _ClassVar[int]\nNN_CONFIG_FIELD_NUMBER: _ClassVar[int]\nONN_CONFIG_FIELD_NUMBER: _ClassVar[int]\nLEVERAGING_BAGGING_CLASSIFIER_CONFIG_FIELD_NUMBER: _ClassVar[int]\nADABOOST_CLASSIFIER_CONFIG_FIELD_NUMBER: _ClassVar[int]\nRANDOM_SAMPLER_CONFIG_FIELD_NUMBER: _ClassVar[int]\nBANDIT_MODEL_SELECTION_CONFIG_FIELD_NUMBER: _ClassVar[int]\nCONTEXTUAL_BANDIT_MODEL_SELECTION_CONFIG_FIELD_NUMBER: _ClassVar[int]\nPYTHON_CONFIG_FIELD_NUMBER: _ClassVar[int]\nPREPROCESSOR_CONFIG_FIELD_NUMBER: _ClassVar[int]\nOVR_MODEL_SELECTION_CONFIG_FIELD_NUMBER: _ClassVar[int]\nRANDOM_PROJECTION_CONFIG_FIELD_NUMBER: _ClassVar[int]\nEMBEDDING_MODEL_CONFIG_FIELD_NUMBER: _ClassVar[int]\nHETERO_LEVERAGING_BAGGING_CLASSIFIER_CONFIG_FIELD_NUMBER: _ClassVar[int]\nHETERO_ADABOOST_CLASSIFIER_CONFIG_FIELD_NUMBER: _ClassVar[int]\nSGT_CLASSIFIER_CONFIG_FIELD_NUMBER: _ClassVar[int]\nSGT_REGRESSOR_CONFIG_FIELD_NUMBER: _ClassVar[int]\nMULTINOMIAL_CONFIG_FIELD_NUMBER: _ClassVar[int]\nGAUSSIAN_CONFIG_FIELD_NUMBER: _ClassVar[int]\nADAPTIVE_XGBOOST_CONFIG_FIELD_NUMBER: _ClassVar[int]\nADAPTIVE_LGBM_CONFIG_FIELD_NUMBER: _ClassVar[int]\nLLAMA_EMBEDDING_CONFIG_FIELD_NUMBER: _ClassVar[int]\nREST_API_CLIENT_CONFIG_FIELD_NUMBER: _ClassVar[int]\nLLAMA_TEXT_CONFIG_FIELD_NUMBER: _ClassVar[int]\nCLIP_EMBEDDING_CONFIG_FIELD_NUMBER: _ClassVar[int]\nPYTHON_ENSEMBLE_CONFIG_FIELD_NUMBER: _ClassVar[int]\nGRPC_CLIENT_CONFIG_FIELD_NUMBER: _ClassVar[int]\nalgorithm: str\nnum_children: int\nmstream_config: MStreamConfig\nrcf_config: RCFConfig\nhst_config: HSTConfig\nonnx_config: ONNXConfig\nhoeffding_classifier_config: HoeffdingClassifierConfig\nhoeffding_regressor_config: HoeffdingRegressorConfig\namf_classifier_config: AMFClassifierConfig\namf_regressor_config: AMFRegressorConfig\nffm_classifier_config: FFMClassifierConfig\nffm_regressor_config: FFMRegressorConfig\nsnarimax_config: SNARIMAXConfig\nnn_config: NeuralNetworkConfig\nonn_config: ONNConfig\nleveraging_bagging_classifier_config: LeveragingBaggingClassifierConfig\nadaboost_classifier_config: AdaBoostClassifierConfig\nrandom_sampler_config: RandomSamplerConfig\nbandit_model_selection_config: BanditModelSelectionConfig\ncontextual_bandit_model_selection_config: ContextualBanditModelSelectionConfig\npython_config: PythonConfig\npreprocessor_config: PreProcessorConfig\novr_model_selection_config: OVRConfig\nrandom_projection_config: RandomProjectionEmbeddingConfig\nembedding_model_config: EmbeddingModelConfig\nhetero_leveraging_bagging_classifier_config: HeteroLeveragingBaggingClassifierConfig\nhetero_adaboost_classifier_config: HeteroAdaBoostClassifierConfig\nsgt_classifier_config: SGTClassifierConfig\nsgt_regressor_config: SGTRegressorConfig\nmultinomial_config: MultinomialConfig\ngaussian_config: GaussianConfig\nadaptive_xgboost_config: AdaptiveXGBoostConfig\nadaptive_lgbm_config: AdaptiveLGBMConfig\nllama_embedding_config: LLAMAEmbeddingModelConfig\nrest_api_client_config: RestAPIClientConfig\nllama_text_config: LlamaTextConfig\nclip_embedding_config: ClipEmbeddingConfig\npython_ensemble_config: PythonEnsembleConfig\ngrpc_client_config: GRPCClientConfig\ndef __init__(self, algorithm: _Optional[str] = ..., num_children: _Optional[int] = ..., mstream_config: _Optional[_Union[MStreamConfig, _Mapping]] = ..., rcf_config: _Optional[_Union[RCFConfig, _Mapping]] = ..., hst_config: _Optional[_Union[HSTConfig, _Mapping]] = ..., onnx_config: _Optional[_Union[ONNXConfig, _Mapping]] = ..., hoeffding_classifier_config: _Optional[_Union[HoeffdingClassifierConfig, _Mapping]] = ..., hoeffding_regressor_config: _Optional[_Union[HoeffdingRegressorConfig, _Mapping]] = ..., amf_classifier_config: _Optional[_Union[AMFClassifierConfig, _Mapping]] = ..., amf_regressor_config: _Optional[_Union[AMFRegressorConfig, _Mapping]] = ..., ffm_classifier_config: _Optional[_Union[FFMClassifierConfig, _Mapping]] = ..., ffm_regressor_config: _Optional[_Union[FFMRegressorConfig, _Mapping]] = ..., snarimax_config: _Optional[_Union[SNARIMAXConfig, _Mapping]] = ..., nn_config: _Optional[_Union[NeuralNetworkConfig, _Mapping]] = ..., onn_config: _Optional[_Union[ONNConfig, _Mapping]] = ..., leveraging_bagging_classifier_config: _Optional[_Union[LeveragingBaggingClassifierConfig, _Mapping]] = ..., adaboost_classifier_config: _Optional[_Union[AdaBoostClassifierConfig, _Mapping]] = ..., random_sampler_config: _Optional[_Union[RandomSamplerConfig, _Mapping]] = ..., bandit_model_selection_config: _Optional[_Union[BanditModelSelectionConfig, _Mapping]] = ..., contextual_bandit_model_selection_config: _Optional[_Union[ContextualBanditModelSelectionConfig, _Mapping]] = ..., python_config: _Optional[_Union[PythonConfig, _Mapping]] = ..., preprocessor_config: _Optional[_Union[PreProcessorConfig, _Mapping]] = ..., ovr_model_selection_config: _Optional[_Union[OVRConfig, _Mapping]] = ..., random_projection_config: _Optional[_Union[RandomProjectionEmbeddingConfig, _Mapping]] = ..., embedding_model_config: _Optional[_Union[EmbeddingModelConfig, _Mapping]] = ..., hetero_leveraging_bagging_classifier_config: _Optional[_Union[HeteroLeveragingBaggingClassifierConfig, _Mapping]] = ..., hetero_adaboost_classifier_config: _Optional[_Union[HeteroAdaBoostClassifierConfig, _Mapping]] = ..., sgt_classifier_config: _Optional[_Union[SGTClassifierConfig, _Mapping]] = ..., sgt_regressor_config: _Optional[_Union[SGTRegressorConfig, _Mapping]] = ..., multinomial_config: _Optional[_Union[MultinomialConfig, _Mapping]] = ..., gaussian_config: _Optional[_Union[GaussianConfig, _Mapping]] = ..., adaptive_xgboost_config: _Optional[_Union[AdaptiveXGBoostConfig, _Mapping]] = ..., adaptive_lgbm_config: _Optional[_Union[AdaptiveLGBMConfig, _Mapping]] = ..., llama_embedding_config: _Optional[_Union[LLAMAEmbeddingModelConfig, _Mapping]] = ..., rest_api_client_config: _Optional[_Union[RestAPIClientConfig, _Mapping]] = ..., llama_text_config: _Optional[_Union[LlamaTextConfig, _Mapping]] = ..., clip_embedding_config: _Optional[_Union[ClipEmbeddingConfig, _Mapping]] = ..., python_ensemble_config: _Optional[_Union[PythonEnsembleConfig, _Mapping]] = ..., grpc_client_config: _Optional[_Union[GRPCClientConfig, _Mapping]] = ...) -> None: ...\n\nclass FeatureRetrievalConfig(_message.Message):\n__slots__ = (\"sql_statement\", \"placeholder_cols\", \"result_cols\")\nSQL_STATEMENT_FIELD_NUMBER: _ClassVar[int]\nPLACEHOLDER_COLS_FIELD_NUMBER: _ClassVar[int]\nRESULT_COLS_FIELD_NUMBER: _ClassVar[int]\nsql_statement: str\nplaceholder_cols: _containers.RepeatedScalarFieldContainer[str]\nresult_cols: _containers.RepeatedScalarFieldContainer[str]\ndef __init__(self, sql_statement: _Optional[str] = ..., placeholder_cols: _Optional[_Iterable[str]] = ..., result_cols: _Optional[_Iterable[str]] = ...) -> None: ...\n\nclass KafkaProducerConfig(_message.Message):\n__slots__ = (\"write_topic\", \"proto_file_name\", \"proto_message_name\", \"schema_id\")\nWRITE_TOPIC_FIELD_NUMBER: _ClassVar[int]\nPROTO_FILE_NAME_FIELD_NUMBER: _ClassVar[int]\nPROTO_MESSAGE_NAME_FIELD_NUMBER: _ClassVar[int]\nSCHEMA_ID_FIELD_NUMBER: _ClassVar[int]\nwrite_topic: str\nproto_file_name: str\nproto_message_name: str\nschema_id: int\ndef __init__(self, write_topic: _Optional[str] = ..., proto_file_name: _Optional[str] = ..., proto_message_name: _Optional[str] = ..., schema_id: _Optional[int] = ...) -> None: ...\n\nclass KafkaConsumerConfig(_message.Message):\n__slots__ = (\"read_topic\", \"proto_file_name\", \"proto_message_name\", \"consumer_group\")\nREAD_TOPIC_FIELD_NUMBER: _ClassVar[int]\nPROTO_FILE_NAME_FIELD_NUMBER: _ClassVar[int]\nPROTO_MESSAGE_NAME_FIELD_NUMBER: _ClassVar[int]\nCONSUMER_GROUP_FIELD_NUMBER: _ClassVar[int]\nread_topic: str\nproto_file_name: str\nproto_message_name: str\nconsumer_group: str\ndef __init__(self, read_topic: _Optional[str] = ..., proto_file_name: _Optional[str] = ..., proto_message_name: _Optional[str] = ..., consumer_group: _Optional[str] = ...) -> None: ...\n\nclass InputConfig(_message.Message):\n__slots__ = (\"key_field\", \"label_field\", \"numerical\", \"categorical\", \"time_tick\", \"textual\", \"imaginal\")\nKEY_FIELD_FIELD_NUMBER: _ClassVar[int]\nLABEL_FIELD_FIELD_NUMBER: _ClassVar[int]\nNUMERICAL_FIELD_NUMBER: _ClassVar[int]\nCATEGORICAL_FIELD_NUMBER: _ClassVar[int]\nTIME_TICK_FIELD_NUMBER: _ClassVar[int]\nTEXTUAL_FIELD_NUMBER: _ClassVar[int]\nIMAGINAL_FIELD_NUMBER: _ClassVar[int]\nkey_field: str\nlabel_field: str\nnumerical: _containers.RepeatedScalarFieldContainer[str]\ncategorical: _containers.RepeatedScalarFieldContainer[str]\ntime_tick: str\ntextual: _containers.RepeatedScalarFieldContainer[str]\nimaginal: _containers.RepeatedScalarFieldContainer[str]\ndef __init__(self, key_field: _Optional[str] = ..., label_field: _Optional[str] = ..., numerical: _Optional[_Iterable[str]] = ..., categorical: _Optional[_Iterable[str]] = ..., time_tick: _Optional[str] = ..., textual: _Optional[_Iterable[str]] = ..., imaginal: _Optional[_Iterable[str]] = ...) -> None: ...\n\nclass LearnerConfig(_message.Message):\n__slots__ = (\"predict_workers\", \"update_batch_size\", \"synchronization_method\")\nPREDICT_WORKERS_FIELD_NUMBER: _ClassVar[int]\nUPDATE_BATCH_SIZE_FIELD_NUMBER: _ClassVar[int]\nSYNCHRONIZATION_METHOD_FIELD_NUMBER: _ClassVar[int]\npredict_workers: int\nupdate_batch_size: int\nsynchronization_method: str\ndef __init__(self, predict_workers: _Optional[int] = ..., update_batch_size: _Optional[int] = ..., synchronization_method: _Optional[str] = ...) -> None: ...\n\nclass TurboMLConfig(_message.Message):\n__slots__ = (\"brokers\", \"feat_consumer\", \"output_producer\", \"label_consumer\", \"input_config\", \"model_configs\", \"initial_model_id\", \"api_port\", \"arrow_port\", \"feature_retrieval\", \"combined_producer\", \"combined_consumer\", \"learner_config\", \"fully_qualified_model_name\")\nBROKERS_FIELD_NUMBER: _ClassVar[int]\nFEAT_CONSUMER_FIELD_NUMBER: _ClassVar[int]\nOUTPUT_PRODUCER_FIELD_NUMBER: _ClassVar[int]\nLABEL_CONSUMER_FIELD_NUMBER: _ClassVar[int]\nINPUT_CONFIG_FIELD_NUMBER: _ClassVar[int]\nMODEL_CONFIGS_FIELD_NUMBER: _ClassVar[int]\nINITIAL_MODEL_ID_FIELD_NUMBER: _ClassVar[int]\nAPI_PORT_FIELD_NUMBER: _ClassVar[int]\nARROW_PORT_FIELD_NUMBER: _ClassVar[int]\nFEATURE_RETRIEVAL_FIELD_NUMBER: _ClassVar[int]\nCOMBINED_PRODUCER_FIELD_NUMBER: _ClassVar[int]\nCOMBINED_CONSUMER_FIELD_NUMBER: _ClassVar[int]\nLEARNER_CONFIG_FIELD_NUMBER: _ClassVar[int]\nFULLY_QUALIFIED_MODEL_NAME_FIELD_NUMBER: _ClassVar[int]\nbrokers: str\nfeat_consumer: KafkaConsumerConfig\noutput_producer: KafkaProducerConfig\nlabel_consumer: KafkaConsumerConfig\ninput_config: InputConfig\nmodel_configs: _containers.RepeatedCompositeFieldContainer[ModelConfig]\ninitial_model_id: str\napi_port: int\narrow_port: int\nfeature_retrieval: FeatureRetrievalConfig\ncombined_producer: KafkaProducerConfig\ncombined_consumer: KafkaConsumerConfig\nlearner_config: LearnerConfig\nfully_qualified_model_name: str\ndef __init__(self, brokers: _Optional[str] = ..., feat_consumer: _Optional[_Union[KafkaConsumerConfig, _Mapping]] = ..., output_producer: _Optional[_Union[KafkaProducerConfig, _Mapping]] = ..., label_consumer: _Optional[_Union[KafkaConsumerConfig, _Mapping]] = ..., input_config: _Optional[_Union[InputConfig, _Mapping]] = ..., model_configs: _Optional[_Iterable[_Union[ModelConfig, _Mapping]]] = ..., initial_model_id: _Optional[str] = ..., api_port: _Optional[int] = ..., arrow_port: _Optional[int] = ..., feature_retrieval: _Optional[_Union[FeatureRetrievalConfig, _Mapping]] = ..., combined_producer: _Optional[_Union[KafkaProducerConfig, _Mapping]] = ..., combined_consumer: _Optional[_Union[KafkaConsumerConfig, _Mapping]] = ..., learner_config: _Optional[_Union[LearnerConfig, _Mapping]] = ..., fully_qualified_model_name: _Optional[str] = ...) -> None: ...\n\nclass TrainJobConfig(_message.Message):\n__slots__ = (\"initial_model_key\", \"input_config\", \"model_configs\", \"model_name\", \"version_name\")\nINITIAL_MODEL_KEY_FIELD_NUMBER: _ClassVar[int]\nINPUT_CONFIG_FIELD_NUMBER: _ClassVar[int]\nMODEL_CONFIGS_FIELD_NUMBER: _ClassVar[int]\nMODEL_NAME_FIELD_NUMBER: _ClassVar[int]\nVERSION_NAME_FIELD_NUMBER: _ClassVar[int]\ninitial_model_key: str\ninput_config: InputConfig\nmodel_configs: _containers.RepeatedCompositeFieldContainer[ModelConfig]\nmodel_name: str\nversion_name: str\ndef __init__(self, initial_model_key: _Optional[str] = ..., input_config: _Optional[_Union[InputConfig, _Mapping]] = ..., model_configs: _Optional[_Iterable[_Union[ModelConfig, _Mapping]]] = ..., model_name: _Optional[str] = ..., version_name: _Optional[str] = ...) -> None: ...\n\nclass ModelConfigList(_message.Message):\n__slots__ = (\"model_configs\",)\nMODEL_CONFIGS_FIELD_NUMBER: _ClassVar[int]\nmodel_configs: _containers.RepeatedCompositeFieldContainer[ModelConfig]\ndef __init__(self, model_configs: _Optional[_Iterable[_Union[ModelConfig, _Mapping]]] = ...) -> None: ...\n\n```"
    },
    {
        "section": "flink_pb2.py",
        "content": "# flink_pb2.py\n-## Location -> root_directory.common.protos\n```python\n# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: flink.proto\n# Protobuf Python Version: 4.25.5\n\"\"\"Generated protocol buffer code.\"\"\"\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import descriptor_pool as _descriptor_pool\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf.internal import builder as _builder\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nimport sources_pb2 as sources__pb2\n\n\nDESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\x0b\\x66link.proto\\x12\\x05\\x66link\\x1a\\rsources.proto\\\"\\'\\n\\x17\\x46linkDeploymentResource\\x12\\x0c\\n\\x04name\\x18\\x01 \\x02(\\t\\\"\\xba\\x0f\\n\\x11\\x44\\x65ploymentRequest\\x12\\x17\\n\\x0f\\x64\\x65ployment_name\\x18\\x01 \\x02(\\t\\x12@\\n\\x0esql_deployment\\x18\\x02 \\x01(\\x0b\\x32&.flink.DeploymentRequest.SqlDeploymentH\\x00\\x12J\\n\\x13\\x61rtifact_deployment\\x18\\x03 \\x01(\\x0b\\x32+.flink.DeploymentRequest.ArtifactDeploymentH\\x00\\x12G\\n\\x10\\x66link_properties\\x18\\x05 \\x03(\\x0b\\x32-.flink.DeploymentRequest.FlinkPropertiesEntry\\x12\\x45\\n\\x12job_manager_config\\x18\\x06 \\x01(\\x0b\\x32).flink.DeploymentRequest.JobManagerConfig\\x12G\\n\\x13task_manager_config\\x18\\x07 \\x01(\\x0b\\x32*.flink.DeploymentRequest.TaskManagerConfig\\x12\\x37\\n\\x08\\x65nv_vars\\x18\\x08 \\x03(\\x0b\\x32%.flink.DeploymentRequest.EnvVarsEntry\\x12\\x16\\n\\x0e\\x66rom_savepoint\\x18\\t \\x01(\\t\\x12\\x1a\\n\\x12recreate_on_update\\x18\\n \\x01(\\x08\\x12\\x35\\n\\tsavepoint\\x18\\x0b \\x01(\\x0b\\x32\\\".flink.DeploymentRequest.Savepoint\\x12 \\n\\x18\\x61llow_non_restored_state\\x18\\x0c \\x01(\\x08\\x12 \\n\\x18take_savepoint_on_update\\x18\\r \\x01(\\x08\\x12\\x13\\n\\x0bparallelism\\x18\\x0e \\x01(\\r\\x12\\x16\\n\\x0erestart_policy\\x18\\x0f \\x01(\\t\\x12\\x16\\n\\x0e\\x63leanup_policy\\x18\\x10 \\x01(\\t\\x12\\x1c\\n\\x14savepoint_generation\\x18\\x11 \\x01(\\x03\\x12\\x18\\n\\x10\\x63\\x61ncel_requested\\x18\\x12 \\x01(\\x08\\x12\\x17\\n\\x0flocal_time_zone\\x18\\x13 \\x01(\\t\\x1a\\xdb\\x01\\n\\rSqlDeployment\\x12\\r\\n\\x05query\\x18\\x01 \\x02(\\t\\x12)\\n\\x0c\\x64\\x61ta_sources\\x18\\x02 \\x03(\\x0b\\x32\\x13.sources.DataSource\\x12\\x38\\n\\x04udfs\\x18\\x03 \\x03(\\x0b\\x32*.flink.DeploymentRequest.SqlDeployment.Udf\\x12\\x12\\n\\nsink_topic\\x18\\x04 \\x02(\\t\\x12\\x1f\\n\\x17sink_topic_message_name\\x18\\x05 \\x02(\\t\\x1a!\\n\\x03Udf\\x12\\x0c\\n\\x04name\\x18\\x01 \\x02(\\t\\x12\\x0c\\n\\x04\\x63ode\\x18\\x02 \\x02(\\t\\x1a\\xb4\\x03\\n\\x12\\x41rtifactDeployment\\x12G\\n\\x08java_job\\x18\\x01 \\x01(\\x0b\\x32\\x33.flink.DeploymentRequest.ArtifactDeployment.JavaJobH\\x00\\x12K\\n\\npython_job\\x18\\x02 \\x01(\\x0b\\x32\\x35.flink.DeploymentRequest.ArtifactDeployment.PythonJobH\\x00\\x12\\x17\\n\\x0f\\x66iles_base_path\\x18\\x03 \\x02(\\t\\x12Z\\n\\x10\\x66link_properties\\x18\\x04 \\x03(\\x0b\\x32@.flink.DeploymentRequest.ArtifactDeployment.FlinkPropertiesEntry\\x1a/\\n\\x07JavaJob\\x12\\x10\\n\\x08jar_name\\x18\\x01 \\x02(\\t\\x12\\x12\\n\\nclass_name\\x18\\x02 \\x02(\\t\\x1a\\x1e\\n\\tPythonJob\\x12\\x11\\n\\tfile_name\\x18\\x01 \\x02(\\t\\x1a\\x36\\n\\x14\\x46linkPropertiesEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\t:\\x02\\x38\\x01\\x42\\n\\n\\x08job_type\\x1a\\x42\\n\\x17\\x43ontainerResourceLimits\\x12\\x11\\n\\tcpu_limit\\x18\\x01 \\x02(\\t\\x12\\x14\\n\\x0cmemory_limit\\x18\\x02 \\x02(\\t\\x1a\\x83\\x01\\n\\x10JobManagerConfig\\x12V\\n\\x1cjob_manager_resources_limits\\x18\\x01 \\x01(\\x0b\\x32\\x30.flink.DeploymentRequest.ContainerResourceLimits\\x12\\x17\\n\\x0fnum_of_replicas\\x18\\x02 \\x01(\\x05\\x1a\\x85\\x01\\n\\x11TaskManagerConfig\\x12W\\n\\x1dtask_manager_resources_limits\\x18\\x01 \\x01(\\x0b\\x32\\x30.flink.DeploymentRequest.ContainerResourceLimits\\x12\\x17\\n\\x0fnum_of_replicas\\x18\\x02 \\x01(\\x05\\x1a\\x43\\n\\tSavepoint\\x12\\x1e\\n\\x16\\x61uto_savepoint_seconds\\x18\\x01 \\x01(\\x05\\x12\\x16\\n\\x0esavepoints_dir\\x18\\x02 \\x01(\\t\\x1a\\x36\\n\\x14\\x46linkPropertiesEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\t:\\x02\\x38\\x01\\x1a.\\n\\x0c\\x45nvVarsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\t:\\x02\\x38\\x01\\x42\\x13\\n\\x11\\x64\\x65ployment_config\\\"\\n\\n\\x08Response2\\x96\\x01\\n\\x05\\x46link\\x12\\x46\\n\\x17SubmitDeploymentRequest\\x12\\x18.flink.DeploymentRequest\\x1a\\x0f.flink.Response\\\"\\x00\\x12\\x45\\n\\x10\\x44\\x65leteDeployment\\x12\\x1e.flink.FlinkDeploymentResource\\x1a\\x0f.flink.Response\\\"\\x00\\x42\\\"\\n\\x11\\x63om.turboml.flinkB\\x0b\\x46linkServerP\\x01')\n\n_globals = globals()\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)\n_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'flink_pb2', _globals)\nif _descriptor._USE_C_DESCRIPTORS == False:\n_globals['DESCRIPTOR']._options = None\n_globals['DESCRIPTOR']._serialized_options = b'\\n\\021com.turboml.flinkB\\013FlinkServerP\\001'\n_globals['_DEPLOYMENTREQUEST_ARTIFACTDEPLOYMENT_FLINKPROPERTIESENTRY']._options = None\n_globals['_DEPLOYMENTREQUEST_ARTIFACTDEPLOYMENT_FLINKPROPERTIESENTRY']._serialized_options = b'8\\001'\n_globals['_DEPLOYMENTREQUEST_FLINKPROPERTIESENTRY']._options = None\n_globals['_DEPLOYMENTREQUEST_FLINKPROPERTIESENTRY']._serialized_options = b'8\\001'\n_globals['_DEPLOYMENTREQUEST_ENVVARSENTRY']._options = None\n_globals['_DEPLOYMENTREQUEST_ENVVARSENTRY']._serialized_options = b'8\\001'\n_globals['_FLINKDEPLOYMENTRESOURCE']._serialized_start=37\n_globals['_FLINKDEPLOYMENTRESOURCE']._serialized_end=76\n_globals['_DEPLOYMENTREQUEST']._serialized_start=79\n_globals['_DEPLOYMENTREQUEST']._serialized_end=2057\n_globals['_DEPLOYMENTREQUEST_SQLDEPLOYMENT']._serialized_start=867\n_globals['_DEPLOYMENTREQUEST_SQLDEPLOYMENT']._serialized_end=1086\n_globals['_DEPLOYMENTREQUEST_SQLDEPLOYMENT_UDF']._serialized_start=1053\n_globals['_DEPLOYMENTREQUEST_SQLDEPLOYMENT_UDF']._serialized_end=1086\n_globals['_DEPLOYMENTREQUEST_ARTIFACTDEPLOYMENT']._serialized_start=1089\n_globals['_DEPLOYMENTREQUEST_ARTIFACTDEPLOYMENT']._serialized_end=1525\n_globals['_DEPLOYMENTREQUEST_ARTIFACTDEPLOYMENT_JAVAJOB']._serialized_start=1378\n_globals['_DEPLOYMENTREQUEST_ARTIFACTDEPLOYMENT_JAVAJOB']._serialized_end=1425\n_globals['_DEPLOYMENTREQUEST_ARTIFACTDEPLOYMENT_PYTHONJOB']._serialized_start=1427\n_globals['_DEPLOYMENTREQUEST_ARTIFACTDEPLOYMENT_PYTHONJOB']._serialized_end=1457\n_globals['_DEPLOYMENTREQUEST_ARTIFACTDEPLOYMENT_FLINKPROPERTIESENTRY']._serialized_start=1459\n_globals['_DEPLOYMENTREQUEST_ARTIFACTDEPLOYMENT_FLINKPROPERTIESENTRY']._serialized_end=1513\n_globals['_DEPLOYMENTREQUEST_CONTAINERRESOURCELIMITS']._serialized_start=1527\n_globals['_DEPLOYMENTREQUEST_CONTAINERRESOURCELIMITS']._serialized_end=1593\n_globals['_DEPLOYMENTREQUEST_JOBMANAGERCONFIG']._serialized_start=1596\n_globals['_DEPLOYMENTREQUEST_JOBMANAGERCONFIG']._serialized_end=1727\n_globals['_DEPLOYMENTREQUEST_TASKMANAGERCONFIG']._serialized_start=1730\n_globals['_DEPLOYMENTREQUEST_TASKMANAGERCONFIG']._serialized_end=1863\n_globals['_DEPLOYMENTREQUEST_SAVEPOINT']._serialized_start=1865\n_globals['_DEPLOYMENTREQUEST_SAVEPOINT']._serialized_end=1932\n_globals['_DEPLOYMENTREQUEST_FLINKPROPERTIESENTRY']._serialized_start=1459\n_globals['_DEPLOYMENTREQUEST_FLINKPROPERTIESENTRY']._serialized_end=1513\n_globals['_DEPLOYMENTREQUEST_ENVVARSENTRY']._serialized_start=1990\n_globals['_DEPLOYMENTREQUEST_ENVVARSENTRY']._serialized_end=2036\n_globals['_RESPONSE']._serialized_start=2059\n_globals['_RESPONSE']._serialized_end=2069\n_globals['_FLINK']._serialized_start=2072\n_globals['_FLINK']._serialized_end=2222\n# @@protoc_insertion_point(module_scope)\n\n```  \n#### flink_pb2.pyi\n```python\nimport sources_pb2 as _sources_pb2\nfrom google.protobuf.internal import containers as _containers\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom typing import ClassVar as _ClassVar, Iterable as _Iterable, Mapping as _Mapping, Optional as _Optional, Union as _Union\n\nDESCRIPTOR: _descriptor.FileDescriptor\n\nclass FlinkDeploymentResource(_message.Message):\n__slots__ = (\"name\",)\nNAME_FIELD_NUMBER: _ClassVar[int]\nname: str\ndef __init__(self, name: _Optional[str] = ...) -> None: ...\n\nclass DeploymentRequest(_message.Message):\n__slots__ = (\"deployment_name\", \"sql_deployment\", \"artifact_deployment\", \"flink_properties\", \"job_manager_config\", \"task_manager_config\", \"env_vars\", \"from_savepoint\", \"recreate_on_update\", \"savepoint\", \"allow_non_restored_state\", \"take_savepoint_on_update\", \"parallelism\", \"restart_policy\", \"cleanup_policy\", \"savepoint_generation\", \"cancel_requested\", \"local_time_zone\")\nclass SqlDeployment(_message.Message):\n__slots__ = (\"query\", \"data_sources\", \"udfs\", \"sink_topic\", \"sink_topic_message_name\")\nclass Udf(_message.Message):\n__slots__ = (\"name\", \"code\")\nNAME_FIELD_NUMBER: _ClassVar[int]\nCODE_FIELD_NUMBER: _ClassVar[int]\nname: str\ncode: str\ndef __init__(self, name: _Optional[str] = ..., code: _Optional[str] = ...) -> None: ...\nQUERY_FIELD_NUMBER: _ClassVar[int]\nDATA_SOURCES_FIELD_NUMBER: _ClassVar[int]\nUDFS_FIELD_NUMBER: _ClassVar[int]\nSINK_TOPIC_FIELD_NUMBER: _ClassVar[int]\nSINK_TOPIC_MESSAGE_NAME_FIELD_NUMBER: _ClassVar[int]\nquery: str\ndata_sources: _containers.RepeatedCompositeFieldContainer[_sources_pb2.DataSource]\nudfs: _containers.RepeatedCompositeFieldContainer[DeploymentRequest.SqlDeployment.Udf]\nsink_topic: str\nsink_topic_message_name: str\ndef __init__(self, query: _Optional[str] = ..., data_sources: _Optional[_Iterable[_Union[_sources_pb2.DataSource, _Mapping]]] = ..., udfs: _Optional[_Iterable[_Union[DeploymentRequest.SqlDeployment.Udf, _Mapping]]] = ..., sink_topic: _Optional[str] = ..., sink_topic_message_name: _Optional[str] = ...) -> None: ...\nclass ArtifactDeployment(_message.Message):\n__slots__ = (\"java_job\", \"python_job\", \"files_base_path\", \"flink_properties\")\nclass JavaJob(_message.Message):\n__slots__ = (\"jar_name\", \"class_name\")\nJAR_NAME_FIELD_NUMBER: _ClassVar[int]\nCLASS_NAME_FIELD_NUMBER: _ClassVar[int]\njar_name: str\nclass_name: str\ndef __init__(self, jar_name: _Optional[str] = ..., class_name: _Optional[str] = ...) -> None: ...\nclass PythonJob(_message.Message):\n__slots__ = (\"file_name\",)\nFILE_NAME_FIELD_NUMBER: _ClassVar[int]\nfile_name: str\ndef __init__(self, file_name: _Optional[str] = ...) -> None: ...\nclass FlinkPropertiesEntry(_message.Message):\n__slots__ = (\"key\", \"value\")\nKEY_FIELD_NUMBER: _ClassVar[int]\nVALUE_FIELD_NUMBER: _ClassVar[int]\nkey: str\nvalue: str\ndef __init__(self, key: _Optional[str] = ..., value: _Optional[str] = ...) -> None: ...\nJAVA_JOB_FIELD_NUMBER: _ClassVar[int]\nPYTHON_JOB_FIELD_NUMBER: _ClassVar[int]\nFILES_BASE_PATH_FIELD_NUMBER: _ClassVar[int]\nFLINK_PROPERTIES_FIELD_NUMBER: _ClassVar[int]\njava_job: DeploymentRequest.ArtifactDeployment.JavaJob\npython_job: DeploymentRequest.ArtifactDeployment.PythonJob\nfiles_base_path: str\nflink_properties: _containers.ScalarMap[str, str]\ndef __init__(self, java_job: _Optional[_Union[DeploymentRequest.ArtifactDeployment.JavaJob, _Mapping]] = ..., python_job: _Optional[_Union[DeploymentRequest.ArtifactDeployment.PythonJob, _Mapping]] = ..., files_base_path: _Optional[str] = ..., flink_properties: _Optional[_Mapping[str, str]] = ...) -> None: ...\nclass ContainerResourceLimits(_message.Message):\n__slots__ = (\"cpu_limit\", \"memory_limit\")\nCPU_LIMIT_FIELD_NUMBER: _ClassVar[int]\nMEMORY_LIMIT_FIELD_NUMBER: _ClassVar[int]\ncpu_limit: str\nmemory_limit: str\ndef __init__(self, cpu_limit: _Optional[str] = ..., memory_limit: _Optional[str] = ...) -> None: ...\nclass JobManagerConfig(_message.Message):\n__slots__ = (\"job_manager_resources_limits\", \"num_of_replicas\")\nJOB_MANAGER_RESOURCES_LIMITS_FIELD_NUMBER: _ClassVar[int]\nNUM_OF_REPLICAS_FIELD_NUMBER: _ClassVar[int]\njob_manager_resources_limits: DeploymentRequest.ContainerResourceLimits\nnum_of_replicas: int\ndef __init__(self, job_manager_resources_limits: _Optional[_Union[DeploymentRequest.ContainerResourceLimits, _Mapping]] = ..., num_of_replicas: _Optional[int] = ...) -> None: ...\nclass TaskManagerConfig(_message.Message):\n__slots__ = (\"task_manager_resources_limits\", \"num_of_replicas\")\nTASK_MANAGER_RESOURCES_LIMITS_FIELD_NUMBER: _ClassVar[int]\nNUM_OF_REPLICAS_FIELD_NUMBER: _ClassVar[int]\ntask_manager_resources_limits: DeploymentRequest.ContainerResourceLimits\nnum_of_replicas: int\ndef __init__(self, task_manager_resources_limits: _Optional[_Union[DeploymentRequest.ContainerResourceLimits, _Mapping]] = ..., num_of_replicas: _Optional[int] = ...) -> None: ...\nclass Savepoint(_message.Message):\n__slots__ = (\"auto_savepoint_seconds\", \"savepoints_dir\")\nAUTO_SAVEPOINT_SECONDS_FIELD_NUMBER: _ClassVar[int]\nSAVEPOINTS_DIR_FIELD_NUMBER: _ClassVar[int]\nauto_savepoint_seconds: int\nsavepoints_dir: str\ndef __init__(self, auto_savepoint_seconds: _Optional[int] = ..., savepoints_dir: _Optional[str] = ...) -> None: ...\nclass FlinkPropertiesEntry(_message.Message):\n__slots__ = (\"key\", \"value\")\nKEY_FIELD_NUMBER: _ClassVar[int]\nVALUE_FIELD_NUMBER: _ClassVar[int]\nkey: str\nvalue: str\ndef __init__(self, key: _Optional[str] = ..., value: _Optional[str] = ...) -> None: ...\nclass EnvVarsEntry(_message.Message):\n__slots__ = (\"key\", \"value\")\nKEY_FIELD_NUMBER: _ClassVar[int]\nVALUE_FIELD_NUMBER: _ClassVar[int]\nkey: str\nvalue: str\ndef __init__(self, key: _Optional[str] = ..., value: _Optional[str] = ...) -> None: ...\nDEPLOYMENT_NAME_FIELD_NUMBER: _ClassVar[int]\nSQL_DEPLOYMENT_FIELD_NUMBER: _ClassVar[int]\nARTIFACT_DEPLOYMENT_FIELD_NUMBER: _ClassVar[int]\nFLINK_PROPERTIES_FIELD_NUMBER: _ClassVar[int]\nJOB_MANAGER_CONFIG_FIELD_NUMBER: _ClassVar[int]\nTASK_MANAGER_CONFIG_FIELD_NUMBER: _ClassVar[int]\nENV_VARS_FIELD_NUMBER: _ClassVar[int]\nFROM_SAVEPOINT_FIELD_NUMBER: _ClassVar[int]\nRECREATE_ON_UPDATE_FIELD_NUMBER: _ClassVar[int]\nSAVEPOINT_FIELD_NUMBER: _ClassVar[int]\nALLOW_NON_RESTORED_STATE_FIELD_NUMBER: _ClassVar[int]\nTAKE_SAVEPOINT_ON_UPDATE_FIELD_NUMBER: _ClassVar[int]\nPARALLELISM_FIELD_NUMBER: _ClassVar[int]\nRESTART_POLICY_FIELD_NUMBER: _ClassVar[int]\nCLEANUP_POLICY_FIELD_NUMBER: _ClassVar[int]\nSAVEPOINT_GENERATION_FIELD_NUMBER: _ClassVar[int]\nCANCEL_REQUESTED_FIELD_NUMBER: _ClassVar[int]\nLOCAL_TIME_ZONE_FIELD_NUMBER: _ClassVar[int]\ndeployment_name: str\nsql_deployment: DeploymentRequest.SqlDeployment\nartifact_deployment: DeploymentRequest.ArtifactDeployment\nflink_properties: _containers.ScalarMap[str, str]\njob_manager_config: DeploymentRequest.JobManagerConfig\ntask_manager_config: DeploymentRequest.TaskManagerConfig\nenv_vars: _containers.ScalarMap[str, str]\nfrom_savepoint: str\nrecreate_on_update: bool\nsavepoint: DeploymentRequest.Savepoint\nallow_non_restored_state: bool\ntake_savepoint_on_update: bool\nparallelism: int\nrestart_policy: str\ncleanup_policy: str\nsavepoint_generation: int\ncancel_requested: bool\nlocal_time_zone: str\ndef __init__(self, deployment_name: _Optional[str] = ..., sql_deployment: _Optional[_Union[DeploymentRequest.SqlDeployment, _Mapping]] = ..., artifact_deployment: _Optional[_Union[DeploymentRequest.ArtifactDeployment, _Mapping]] = ..., flink_properties: _Optional[_Mapping[str, str]] = ..., job_manager_config: _Optional[_Union[DeploymentRequest.JobManagerConfig, _Mapping]] = ..., task_manager_config: _Optional[_Union[DeploymentRequest.TaskManagerConfig, _Mapping]] = ..., env_vars: _Optional[_Mapping[str, str]] = ..., from_savepoint: _Optional[str] = ..., recreate_on_update: bool = ..., savepoint: _Optional[_Union[DeploymentRequest.Savepoint, _Mapping]] = ..., allow_non_restored_state: bool = ..., take_savepoint_on_update: bool = ..., parallelism: _Optional[int] = ..., restart_policy: _Optional[str] = ..., cleanup_policy: _Optional[str] = ..., savepoint_generation: _Optional[int] = ..., cancel_requested: bool = ..., local_time_zone: _Optional[str] = ...) -> None: ...\n\nclass Response(_message.Message):\n__slots__ = ()\ndef __init__(self) -> None: ...\n\n```"
    },
    {
        "section": "flink_pb2_grpc.py",
        "content": "# flink_pb2_grpc.py\n-## Location -> root_directory.common.protos\n```python\n# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\n\"\"\"Client and server classes corresponding to protobuf-defined services.\"\"\"\nimport grpc\nimport warnings\n\nimport flink_pb2 as flink__pb2\n\nGRPC_GENERATED_VERSION = '1.68.1'\nGRPC_VERSION = grpc.__version__\n_version_not_supported = False\n\ntry:\nfrom grpc._utilities import first_version_is_lower\n_version_not_supported = first_version_is_lower(GRPC_VERSION, GRPC_GENERATED_VERSION)\nexcept ImportError:\n_version_not_supported = True\n\nif _version_not_supported:\nraise RuntimeError(\nf'The grpc package installed is at version {GRPC_VERSION},'\n+ f' but the generated code in flink_pb2_grpc.py depends on'\n+ f' grpcio>={GRPC_GENERATED_VERSION}.'\n+ f' Please upgrade your grpc module to grpcio>={GRPC_GENERATED_VERSION}'\n+ f' or downgrade your generated code using grpcio-tools<={GRPC_VERSION}.'\n)\n\n\nclass FlinkStub(object):\n\"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\ndef __init__(self, channel):\n\"\"\"Constructor.\n\nArgs:\nchannel: A grpc.Channel.\n\"\"\"\nself.SubmitDeploymentRequest = channel.unary_unary(\n'/flink.Flink/SubmitDeploymentRequest',\nrequest_serializer=flink__pb2.DeploymentRequest.SerializeToString,\nresponse_deserializer=flink__pb2.Response.FromString,\n_registered_method=True)\nself.DeleteDeployment = channel.unary_unary(\n'/flink.Flink/DeleteDeployment',\nrequest_serializer=flink__pb2.FlinkDeploymentResource.SerializeToString,\nresponse_deserializer=flink__pb2.Response.FromString,\n_registered_method=True)\n\n\nclass FlinkServicer(object):\n\"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\ndef SubmitDeploymentRequest(self, request, context):\n\"\"\"Missing associated documentation comment in .proto file.\"\"\"\ncontext.set_code(grpc.StatusCode.UNIMPLEMENTED)\ncontext.set_details('Method not implemented!')\nraise NotImplementedError('Method not implemented!')\n\ndef DeleteDeployment(self, request, context):\n\"\"\"Missing associated documentation comment in .proto file.\"\"\"\ncontext.set_code(grpc.StatusCode.UNIMPLEMENTED)\ncontext.set_details('Method not implemented!')\nraise NotImplementedError('Method not implemented!')\n\n\ndef add_FlinkServicer_to_server(servicer, server):\nrpc_method_handlers = {\n'SubmitDeploymentRequest': grpc.unary_unary_rpc_method_handler(\nservicer.SubmitDeploymentRequest,\nrequest_deserializer=flink__pb2.DeploymentRequest.FromString,\nresponse_serializer=flink__pb2.Response.SerializeToString,\n),\n'DeleteDeployment': grpc.unary_unary_rpc_method_handler(\nservicer.DeleteDeployment,\nrequest_deserializer=flink__pb2.FlinkDeploymentResource.FromString,\nresponse_serializer=flink__pb2.Response.SerializeToString,\n),\n}\ngeneric_handler = grpc.method_handlers_generic_handler(\n'flink.Flink', rpc_method_handlers)\nserver.add_generic_rpc_handlers((generic_handler,))\nserver.add_registered_method_handlers('flink.Flink', rpc_method_handlers)\n\n\n# This class is part of an EXPERIMENTAL API.\nclass Flink(object):\n\"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\n@staticmethod\ndef SubmitDeploymentRequest(request,\ntarget,\noptions=(),\nchannel_credentials=None,\ncall_credentials=None,\ninsecure=False,\ncompression=None,\nwait_for_ready=None,\ntimeout=None,\nmetadata=None):\nreturn grpc.experimental.unary_unary(\nrequest,\ntarget,\n'/flink.Flink/SubmitDeploymentRequest',\nflink__pb2.DeploymentRequest.SerializeToString,\nflink__pb2.Response.FromString,\noptions,\nchannel_credentials,\ninsecure,\ncall_credentials,\ncompression,\nwait_for_ready,\ntimeout,\nmetadata,\n_registered_method=True)\n\n@staticmethod\ndef DeleteDeployment(request,\ntarget,\noptions=(),\nchannel_credentials=None,\ncall_credentials=None,\ninsecure=False,\ncompression=None,\nwait_for_ready=None,\ntimeout=None,\nmetadata=None):\nreturn grpc.experimental.unary_unary(\nrequest,\ntarget,\n'/flink.Flink/DeleteDeployment',\nflink__pb2.FlinkDeploymentResource.SerializeToString,\nflink__pb2.Response.FromString,\noptions,\nchannel_credentials,\ninsecure,\ncall_credentials,\ncompression,\nwait_for_ready,\ntimeout,\nmetadata,\n_registered_method=True)\n\n```"
    },
    {
        "section": "input_pb2.py",
        "content": "# input_pb2.py\n-## Location -> root_directory.common.protos\n```python\n# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: input.proto\n# Protobuf Python Version: 4.25.5\n\"\"\"Generated protocol buffer code.\"\"\"\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import descriptor_pool as _descriptor_pool\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf.internal import builder as _builder\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\x0binput.proto\\x12\\x05input\\\"t\\n\\x05Input\\x12\\x0f\\n\\x07numeric\\x18\\x01 \\x03(\\x02\\x12\\r\\n\\x05\\x63\\x61teg\\x18\\x02 \\x03(\\x05\\x12\\x0c\\n\\x04text\\x18\\x03 \\x03(\\t\\x12\\x0e\\n\\x06images\\x18\\x04 \\x03(\\x0c\\x12\\x11\\n\\ttime_tick\\x18\\x05 \\x01(\\x05\\x12\\r\\n\\x05label\\x18\\x06 \\x01(\\x02\\x12\\x0b\\n\\x03key\\x18\\x07 \\x01(\\t')\n\n_globals = globals()\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)\n_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'input_pb2', _globals)\nif _descriptor._USE_C_DESCRIPTORS == False:\nDESCRIPTOR._options = None\n_globals['_INPUT']._serialized_start=22\n_globals['_INPUT']._serialized_end=138\n# @@protoc_insertion_point(module_scope)\n\n```  \n#### input_pb2.pyi\n```python\nfrom google.protobuf.internal import containers as _containers\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom typing import ClassVar as _ClassVar, Iterable as _Iterable, Optional as _Optional\n\nDESCRIPTOR: _descriptor.FileDescriptor\n\nclass Input(_message.Message):\n__slots__ = (\"numeric\", \"categ\", \"text\", \"images\", \"time_tick\", \"label\", \"key\")\nNUMERIC_FIELD_NUMBER: _ClassVar[int]\nCATEG_FIELD_NUMBER: _ClassVar[int]\nTEXT_FIELD_NUMBER: _ClassVar[int]\nIMAGES_FIELD_NUMBER: _ClassVar[int]\nTIME_TICK_FIELD_NUMBER: _ClassVar[int]\nLABEL_FIELD_NUMBER: _ClassVar[int]\nKEY_FIELD_NUMBER: _ClassVar[int]\nnumeric: _containers.RepeatedScalarFieldContainer[float]\ncateg: _containers.RepeatedScalarFieldContainer[int]\ntext: _containers.RepeatedScalarFieldContainer[str]\nimages: _containers.RepeatedScalarFieldContainer[bytes]\ntime_tick: int\nlabel: float\nkey: str\ndef __init__(self, numeric: _Optional[_Iterable[float]] = ..., categ: _Optional[_Iterable[int]] = ..., text: _Optional[_Iterable[str]] = ..., images: _Optional[_Iterable[bytes]] = ..., time_tick: _Optional[int] = ..., label: _Optional[float] = ..., key: _Optional[str] = ...) -> None: ...\n\n```"
    },
    {
        "section": "metrics_pb2.py",
        "content": "# metrics_pb2.py\n-## Location -> root_directory.common.protos\n```python\n# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: metrics.proto\n# Protobuf Python Version: 4.25.5\n\"\"\"Generated protocol buffer code.\"\"\"\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import descriptor_pool as _descriptor_pool\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf.internal import builder as _builder\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\rmetrics.proto\\x12\\x07metrics\\\"(\\n\\x07Metrics\\x12\\r\\n\\x05index\\x18\\x01 \\x01(\\x05\\x12\\x0e\\n\\x06metric\\x18\\x02 \\x01(\\x02')\n\n_globals = globals()\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)\n_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'metrics_pb2', _globals)\nif _descriptor._USE_C_DESCRIPTORS == False:\nDESCRIPTOR._options = None\n_globals['_METRICS']._serialized_start=26\n_globals['_METRICS']._serialized_end=66\n# @@protoc_insertion_point(module_scope)\n\n```  \n#### metrics_pb2.pyi\n```python\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom typing import ClassVar as _ClassVar, Optional as _Optional\n\nDESCRIPTOR: _descriptor.FileDescriptor\n\nclass Metrics(_message.Message):\n__slots__ = (\"index\", \"metric\")\nINDEX_FIELD_NUMBER: _ClassVar[int]\nMETRIC_FIELD_NUMBER: _ClassVar[int]\nindex: int\nmetric: float\ndef __init__(self, index: _Optional[int] = ..., metric: _Optional[float] = ...) -> None: ...\n\n```"
    },
    {
        "section": "ml_service_pb2.py",
        "content": "# ml_service_pb2.py\n-## Location -> root_directory.common.protos\n```python\n# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: ml_service.proto\n# Protobuf Python Version: 4.25.5\n\"\"\"Generated protocol buffer code.\"\"\"\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import descriptor_pool as _descriptor_pool\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf.internal import builder as _builder\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nimport input_pb2 as input__pb2\nimport output_pb2 as output__pb2\n\n\nDESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\x10ml_service.proto\\x12\\nml_service\\x1a\\x0binput.proto\\x1a\\x0coutput.proto\\\"\\x07\\n\\x05\\x45mpty2^\\n\\tMLService\\x12(\\n\\x05Learn\\x12\\x0c.input.Input\\x1a\\x11.ml_service.Empty\\x12\\'\\n\\x07Predict\\x12\\x0c.input.Input\\x1a\\x0e.output.Output')\n\n_globals = globals()\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)\n_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'ml_service_pb2', _globals)\nif _descriptor._USE_C_DESCRIPTORS == False:\nDESCRIPTOR._options = None\n_globals['_EMPTY']._serialized_start=59\n_globals['_EMPTY']._serialized_end=66\n_globals['_MLSERVICE']._serialized_start=68\n_globals['_MLSERVICE']._serialized_end=162\n# @@protoc_insertion_point(module_scope)\n\n```  \n#### ml_service_pb2.pyi\n```python\nimport input_pb2 as _input_pb2\nimport output_pb2 as _output_pb2\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom typing import ClassVar as _ClassVar\n\nDESCRIPTOR: _descriptor.FileDescriptor\n\nclass Empty(_message.Message):\n__slots__ = ()\ndef __init__(self) -> None: ...\n\n```"
    },
    {
        "section": "ml_service_pb2_grpc.py",
        "content": "# ml_service_pb2_grpc.py\n-## Location -> root_directory.common.protos\n```python\n# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\n\"\"\"Client and server classes corresponding to protobuf-defined services.\"\"\"\nimport grpc\nimport warnings\n\nimport input_pb2 as input__pb2\nimport ml_service_pb2 as ml__service__pb2\nimport output_pb2 as output__pb2\n\nGRPC_GENERATED_VERSION = '1.68.1'\nGRPC_VERSION = grpc.__version__\n_version_not_supported = False\n\ntry:\nfrom grpc._utilities import first_version_is_lower\n_version_not_supported = first_version_is_lower(GRPC_VERSION, GRPC_GENERATED_VERSION)\nexcept ImportError:\n_version_not_supported = True\n\nif _version_not_supported:\nraise RuntimeError(\nf'The grpc package installed is at version {GRPC_VERSION},'\n+ f' but the generated code in ml_service_pb2_grpc.py depends on'\n+ f' grpcio>={GRPC_GENERATED_VERSION}.'\n+ f' Please upgrade your grpc module to grpcio>={GRPC_GENERATED_VERSION}'\n+ f' or downgrade your generated code using grpcio-tools<={GRPC_VERSION}.'\n)\n\n\nclass MLServiceStub(object):\n\"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\ndef __init__(self, channel):\n\"\"\"Constructor.\n\nArgs:\nchannel: A grpc.Channel.\n\"\"\"\nself.Learn = channel.unary_unary(\n'/ml_service.MLService/Learn',\nrequest_serializer=input__pb2.Input.SerializeToString,\nresponse_deserializer=ml__service__pb2.Empty.FromString,\n_registered_method=True)\nself.Predict = channel.unary_unary(\n'/ml_service.MLService/Predict',\nrequest_serializer=input__pb2.Input.SerializeToString,\nresponse_deserializer=output__pb2.Output.FromString,\n_registered_method=True)\n\n\nclass MLServiceServicer(object):\n\"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\ndef Learn(self, request, context):\n\"\"\"Missing associated documentation comment in .proto file.\"\"\"\ncontext.set_code(grpc.StatusCode.UNIMPLEMENTED)\ncontext.set_details('Method not implemented!')\nraise NotImplementedError('Method not implemented!')\n\ndef Predict(self, request, context):\n\"\"\"Missing associated documentation comment in .proto file.\"\"\"\ncontext.set_code(grpc.StatusCode.UNIMPLEMENTED)\ncontext.set_details('Method not implemented!')\nraise NotImplementedError('Method not implemented!')\n\n\ndef add_MLServiceServicer_to_server(servicer, server):\nrpc_method_handlers = {\n'Learn': grpc.unary_unary_rpc_method_handler(\nservicer.Learn,\nrequest_deserializer=input__pb2.Input.FromString,\nresponse_serializer=ml__service__pb2.Empty.SerializeToString,\n),\n'Predict': grpc.unary_unary_rpc_method_handler(\nservicer.Predict,\nrequest_deserializer=input__pb2.Input.FromString,\nresponse_serializer=output__pb2.Output.SerializeToString,\n),\n}\ngeneric_handler = grpc.method_handlers_generic_handler(\n'ml_service.MLService', rpc_method_handlers)\nserver.add_generic_rpc_handlers((generic_handler,))\nserver.add_registered_method_handlers('ml_service.MLService', rpc_method_handlers)\n\n\n# This class is part of an EXPERIMENTAL API.\nclass MLService(object):\n\"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\n@staticmethod\ndef Learn(request,\ntarget,\noptions=(),\nchannel_credentials=None,\ncall_credentials=None,\ninsecure=False,\ncompression=None,\nwait_for_ready=None,\ntimeout=None,\nmetadata=None):\nreturn grpc.experimental.unary_unary(\nrequest,\ntarget,\n'/ml_service.MLService/Learn',\ninput__pb2.Input.SerializeToString,\nml__service__pb2.Empty.FromString,\noptions,\nchannel_credentials,\ninsecure,\ncall_credentials,\ncompression,\nwait_for_ready,\ntimeout,\nmetadata,\n_registered_method=True)\n\n@staticmethod\ndef Predict(request,\ntarget,\noptions=(),\nchannel_credentials=None,\ncall_credentials=None,\ninsecure=False,\ncompression=None,\nwait_for_ready=None,\ntimeout=None,\nmetadata=None):\nreturn grpc.experimental.unary_unary(\nrequest,\ntarget,\n'/ml_service.MLService/Predict',\ninput__pb2.Input.SerializeToString,\noutput__pb2.Output.FromString,\noptions,\nchannel_credentials,\ninsecure,\ncall_credentials,\ncompression,\nwait_for_ready,\ntimeout,\nmetadata,\n_registered_method=True)\n\n```"
    },
    {
        "section": "output_pb2.py",
        "content": "# output_pb2.py\n-## Location -> root_directory.common.protos\n```python\n# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: output.proto\n# Protobuf Python Version: 4.25.5\n\"\"\"Generated protocol buffer code.\"\"\"\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import descriptor_pool as _descriptor_pool\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf.internal import builder as _builder\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\x0coutput.proto\\x12\\x06output\\\"\\x9a\\x01\\n\\x06Output\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05score\\x18\\x02 \\x01(\\x02\\x12\\x15\\n\\rfeature_score\\x18\\x03 \\x03(\\x02\\x12\\x1b\\n\\x13\\x63lass_probabilities\\x18\\x04 \\x03(\\x02\\x12\\x17\\n\\x0fpredicted_class\\x18\\x05 \\x01(\\x05\\x12\\x12\\n\\nembeddings\\x18\\x06 \\x03(\\x02\\x12\\x13\\n\\x0btext_output\\x18\\x07 \\x01(\\t')\n\n_globals = globals()\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)\n_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'output_pb2', _globals)\nif _descriptor._USE_C_DESCRIPTORS == False:\nDESCRIPTOR._options = None\n_globals['_OUTPUT']._serialized_start=25\n_globals['_OUTPUT']._serialized_end=179\n# @@protoc_insertion_point(module_scope)\n\n```  \n#### output_pb2.pyi\n```python\nfrom google.protobuf.internal import containers as _containers\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom typing import ClassVar as _ClassVar, Iterable as _Iterable, Optional as _Optional\n\nDESCRIPTOR: _descriptor.FileDescriptor\n\nclass Output(_message.Message):\n__slots__ = (\"key\", \"score\", \"feature_score\", \"class_probabilities\", \"predicted_class\", \"embeddings\", \"text_output\")\nKEY_FIELD_NUMBER: _ClassVar[int]\nSCORE_FIELD_NUMBER: _ClassVar[int]\nFEATURE_SCORE_FIELD_NUMBER: _ClassVar[int]\nCLASS_PROBABILITIES_FIELD_NUMBER: _ClassVar[int]\nPREDICTED_CLASS_FIELD_NUMBER: _ClassVar[int]\nEMBEDDINGS_FIELD_NUMBER: _ClassVar[int]\nTEXT_OUTPUT_FIELD_NUMBER: _ClassVar[int]\nkey: str\nscore: float\nfeature_score: _containers.RepeatedScalarFieldContainer[float]\nclass_probabilities: _containers.RepeatedScalarFieldContainer[float]\npredicted_class: int\nembeddings: _containers.RepeatedScalarFieldContainer[float]\ntext_output: str\ndef __init__(self, key: _Optional[str] = ..., score: _Optional[float] = ..., feature_score: _Optional[_Iterable[float]] = ..., class_probabilities: _Optional[_Iterable[float]] = ..., predicted_class: _Optional[int] = ..., embeddings: _Optional[_Iterable[float]] = ..., text_output: _Optional[str] = ...) -> None: ...\n\n```"
    },
    {
        "section": "sources_pb2.py",
        "content": "# sources_pb2.py\n-## Location -> root_directory.common.protos\n```python\n# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: sources.proto\n# Protobuf Python Version: 4.25.5\n\"\"\"Generated protocol buffer code.\"\"\"\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import descriptor_pool as _descriptor_pool\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf.internal import builder as _builder\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\rsources.proto\\x12\\x07sources\\\"P\\n\\x0bKafkaSource\\x12\\r\\n\\x05topic\\x18\\x01 \\x01(\\t\\x12\\x1a\\n\\x12proto_message_name\\x18\\x02 \\x01(\\t\\x12\\x16\\n\\x0eschema_version\\x18\\x03 \\x01(\\x05\\\"\\\"\\n\\x12\\x46\\x65\\x61tureGroupSource\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\\"\\xb2\\x01\\n\\x08S3Config\\x12\\x0e\\n\\x06\\x62ucket\\x18\\x01 \\x01(\\t\\x12\\x1a\\n\\raccess_key_id\\x18\\x02 \\x01(\\tH\\x00\\x88\\x01\\x01\\x12\\x1e\\n\\x11secret_access_key\\x18\\x03 \\x01(\\tH\\x01\\x88\\x01\\x01\\x12\\x0e\\n\\x06region\\x18\\x04 \\x01(\\t\\x12\\x15\\n\\x08\\x65ndpoint\\x18\\x05 \\x01(\\tH\\x02\\x88\\x01\\x01\\x42\\x10\\n\\x0e_access_key_idB\\x14\\n\\x12_secret_access_keyB\\x0b\\n\\t_endpoint\\\"\\xbd\\x01\\n\\nFileSource\\x12\\x0c\\n\\x04path\\x18\\x01 \\x01(\\t\\x12*\\n\\x06\\x66ormat\\x18\\x02 \\x01(\\x0e\\x32\\x1a.sources.FileSource.Format\\x12&\\n\\ts3_config\\x18\\x03 \\x01(\\x0b\\x32\\x11.sources.S3ConfigH\\x00\\\";\\n\\x06\\x46ormat\\x12\\x07\\n\\x03\\x43SV\\x10\\x00\\x12\\x08\\n\\x04JSON\\x10\\x01\\x12\\x08\\n\\x04\\x41VRO\\x10\\x02\\x12\\x0b\\n\\x07PARQUET\\x10\\x03\\x12\\x07\\n\\x03ORC\\x10\\x04\\x42\\x10\\n\\x0estorage_config\\\"\\xa8\\x01\\n\\x0ePostgresSource\\x12\\x0c\\n\\x04host\\x18\\x01 \\x01(\\t\\x12\\x0c\\n\\x04port\\x18\\x02 \\x01(\\x05\\x12\\x10\\n\\x08username\\x18\\x03 \\x01(\\t\\x12\\x10\\n\\x08password\\x18\\x04 \\x01(\\t\\x12\\r\\n\\x05table\\x18\\x05 \\x01(\\t\\x12\\x10\\n\\x08\\x64\\x61tabase\\x18\\x06 \\x01(\\t\\x12\\x1b\\n\\x13incrementing_column\\x18\\x07 \\x01(\\t\\x12\\x18\\n\\x10timestamp_column\\x18\\x08 \\x01(\\t\\\"\\xf1\\x01\\n\\x15TimestampFormatConfig\\x12>\\n\\x0b\\x66ormat_type\\x18\\x01 \\x01(\\x0e\\x32).sources.TimestampFormatConfig.FormatType\\x12\\x1a\\n\\rformat_string\\x18\\x02 \\x01(\\tH\\x00\\x88\\x01\\x01\\x12\\x16\\n\\ttime_zone\\x18\\x03 \\x01(\\tH\\x01\\x88\\x01\\x01\\\"D\\n\\nFormatType\\x12\\x0f\\n\\x0b\\x45pochMillis\\x10\\x00\\x12\\x10\\n\\x0c\\x45pochSeconds\\x10\\x01\\x12\\x13\\n\\x0fStringTimestamp\\x10\\x02\\x42\\x10\\n\\x0e_format_stringB\\x0c\\n\\n_time_zone\\\"\\x8e\\x01\\n\\tWatermark\\x12\\x10\\n\\x08time_col\\x18\\x01 \\x01(\\t\\x12\\x1d\\n\\x15\\x61llowed_delay_seconds\\x18\\x02 \\x01(\\x03\\x12<\\n\\x0ftime_col_config\\x18\\x03 \\x01(\\x0b\\x32\\x1e.sources.TimestampFormatConfigH\\x00\\x88\\x01\\x01\\x42\\x12\\n\\x10_time_col_config\\\"\\xc7\\x03\\n\\nDataSource\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\nkey_fields\\x18\\x02 \\x03(\\t\\x12\\x18\\n\\x0b\\x65nvironment\\x18\\x04 \\x01(\\tH\\x01\\x88\\x01\\x01\\x12\\x1b\\n\\x0e\\x65ncoded_schema\\x18\\x05 \\x01(\\tH\\x02\\x88\\x01\\x01\\x12\\x30\\n\\rdelivery_mode\\x18\\x06 \\x01(\\x0e\\x32\\x19.sources.DataDeliveryMode\\x12*\\n\\twatermark\\x18\\x07 \\x01(\\x0b\\x32\\x12.sources.WatermarkH\\x03\\x88\\x01\\x01\\x12*\\n\\x0b\\x66ile_source\\x18\\x08 \\x01(\\x0b\\x32\\x13.sources.FileSourceH\\x00\\x12\\x32\\n\\x0fpostgres_source\\x18\\t \\x01(\\x0b\\x32\\x17.sources.PostgresSourceH\\x00\\x12,\\n\\x0ckafka_source\\x18\\n \\x01(\\x0b\\x32\\x14.sources.KafkaSourceH\\x00\\x12;\\n\\x14\\x66\\x65\\x61ture_group_source\\x18\\x0b \\x01(\\x0b\\x32\\x1b.sources.FeatureGroupSourceH\\x00\\x42\\x06\\n\\x04typeB\\x0e\\n\\x0c_environmentB\\x11\\n\\x0f_encoded_schemaB\\x0c\\n\\n_watermark*+\\n\\x10\\x44\\x61taDeliveryMode\\x12\\n\\n\\x06STATIC\\x10\\x00\\x12\\x0b\\n\\x07\\x44YNAMIC\\x10\\x01\\x62\\x06proto3')\n\n_globals = globals()\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)\n_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'sources_pb2', _globals)\nif _descriptor._USE_C_DESCRIPTORS == False:\nDESCRIPTOR._options = None\n_globals['_DATADELIVERYMODE']._serialized_start=1535\n_globals['_DATADELIVERYMODE']._serialized_end=1578\n_globals['_KAFKASOURCE']._serialized_start=26\n_globals['_KAFKASOURCE']._serialized_end=106\n_globals['_FEATUREGROUPSOURCE']._serialized_start=108\n_globals['_FEATUREGROUPSOURCE']._serialized_end=142\n_globals['_S3CONFIG']._serialized_start=145\n_globals['_S3CONFIG']._serialized_end=323\n_globals['_FILESOURCE']._serialized_start=326\n_globals['_FILESOURCE']._serialized_end=515\n_globals['_FILESOURCE_FORMAT']._serialized_start=438\n_globals['_FILESOURCE_FORMAT']._serialized_end=497\n_globals['_POSTGRESSOURCE']._serialized_start=518\n_globals['_POSTGRESSOURCE']._serialized_end=686\n_globals['_TIMESTAMPFORMATCONFIG']._serialized_start=689\n_globals['_TIMESTAMPFORMATCONFIG']._serialized_end=930\n_globals['_TIMESTAMPFORMATCONFIG_FORMATTYPE']._serialized_start=830\n_globals['_TIMESTAMPFORMATCONFIG_FORMATTYPE']._serialized_end=898\n_globals['_WATERMARK']._serialized_start=933\n_globals['_WATERMARK']._serialized_end=1075\n_globals['_DATASOURCE']._serialized_start=1078\n_globals['_DATASOURCE']._serialized_end=1533\n# @@protoc_insertion_point(module_scope)\n\n```  \n#### sources_pb2.pyi\n```python\nfrom google.protobuf.internal import containers as _containers\nfrom google.protobuf.internal import enum_type_wrapper as _enum_type_wrapper\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom typing import ClassVar as _ClassVar, Iterable as _Iterable, Mapping as _Mapping, Optional as _Optional, Union as _Union\n\nDESCRIPTOR: _descriptor.FileDescriptor\n\nclass DataDeliveryMode(int, metaclass=_enum_type_wrapper.EnumTypeWrapper):\n__slots__ = ()\nSTATIC: _ClassVar[DataDeliveryMode]\nDYNAMIC: _ClassVar[DataDeliveryMode]\nSTATIC: DataDeliveryMode\nDYNAMIC: DataDeliveryMode\n\nclass KafkaSource(_message.Message):\n__slots__ = (\"topic\", \"proto_message_name\", \"schema_version\")\nTOPIC_FIELD_NUMBER: _ClassVar[int]\nPROTO_MESSAGE_NAME_FIELD_NUMBER: _ClassVar[int]\nSCHEMA_VERSION_FIELD_NUMBER: _ClassVar[int]\ntopic: str\nproto_message_name: str\nschema_version: int\ndef __init__(self, topic: _Optional[str] = ..., proto_message_name: _Optional[str] = ..., schema_version: _Optional[int] = ...) -> None: ...\n\nclass FeatureGroupSource(_message.Message):\n__slots__ = (\"name\",)\nNAME_FIELD_NUMBER: _ClassVar[int]\nname: str\ndef __init__(self, name: _Optional[str] = ...) -> None: ...\n\nclass S3Config(_message.Message):\n__slots__ = (\"bucket\", \"access_key_id\", \"secret_access_key\", \"region\", \"endpoint\")\nBUCKET_FIELD_NUMBER: _ClassVar[int]\nACCESS_KEY_ID_FIELD_NUMBER: _ClassVar[int]\nSECRET_ACCESS_KEY_FIELD_NUMBER: _ClassVar[int]\nREGION_FIELD_NUMBER: _ClassVar[int]\nENDPOINT_FIELD_NUMBER: _ClassVar[int]\nbucket: str\naccess_key_id: str\nsecret_access_key: str\nregion: str\nendpoint: str\ndef __init__(self, bucket: _Optional[str] = ..., access_key_id: _Optional[str] = ..., secret_access_key: _Optional[str] = ..., region: _Optional[str] = ..., endpoint: _Optional[str] = ...) -> None: ...\n\nclass FileSource(_message.Message):\n__slots__ = (\"path\", \"format\", \"s3_config\")\nclass Format(int, metaclass=_enum_type_wrapper.EnumTypeWrapper):\n__slots__ = ()\nCSV: _ClassVar[FileSource.Format]\nJSON: _ClassVar[FileSource.Format]\nAVRO: _ClassVar[FileSource.Format]\nPARQUET: _ClassVar[FileSource.Format]\nORC: _ClassVar[FileSource.Format]\nCSV: FileSource.Format\nJSON: FileSource.Format\nAVRO: FileSource.Format\nPARQUET: FileSource.Format\nORC: FileSource.Format\nPATH_FIELD_NUMBER: _ClassVar[int]\nFORMAT_FIELD_NUMBER: _ClassVar[int]\nS3_CONFIG_FIELD_NUMBER: _ClassVar[int]\npath: str\nformat: FileSource.Format\ns3_config: S3Config\ndef __init__(self, path: _Optional[str] = ..., format: _Optional[_Union[FileSource.Format, str]] = ..., s3_config: _Optional[_Union[S3Config, _Mapping]] = ...) -> None: ...\n\nclass PostgresSource(_message.Message):\n__slots__ = (\"host\", \"port\", \"username\", \"password\", \"table\", \"database\", \"incrementing_column\", \"timestamp_column\")\nHOST_FIELD_NUMBER: _ClassVar[int]\nPORT_FIELD_NUMBER: _ClassVar[int]\nUSERNAME_FIELD_NUMBER: _ClassVar[int]\nPASSWORD_FIELD_NUMBER: _ClassVar[int]\nTABLE_FIELD_NUMBER: _ClassVar[int]\nDATABASE_FIELD_NUMBER: _ClassVar[int]\nINCREMENTING_COLUMN_FIELD_NUMBER: _ClassVar[int]\nTIMESTAMP_COLUMN_FIELD_NUMBER: _ClassVar[int]\nhost: str\nport: int\nusername: str\npassword: str\ntable: str\ndatabase: str\nincrementing_column: str\ntimestamp_column: str\ndef __init__(self, host: _Optional[str] = ..., port: _Optional[int] = ..., username: _Optional[str] = ..., password: _Optional[str] = ..., table: _Optional[str] = ..., database: _Optional[str] = ..., incrementing_column: _Optional[str] = ..., timestamp_column: _Optional[str] = ...) -> None: ...\n\nclass TimestampFormatConfig(_message.Message):\n__slots__ = (\"format_type\", \"format_string\", \"time_zone\")\nclass FormatType(int, metaclass=_enum_type_wrapper.EnumTypeWrapper):\n__slots__ = ()\nEpochMillis: _ClassVar[TimestampFormatConfig.FormatType]\nEpochSeconds: _ClassVar[TimestampFormatConfig.FormatType]\nStringTimestamp: _ClassVar[TimestampFormatConfig.FormatType]\nEpochMillis: TimestampFormatConfig.FormatType\nEpochSeconds: TimestampFormatConfig.FormatType\nStringTimestamp: TimestampFormatConfig.FormatType\nFORMAT_TYPE_FIELD_NUMBER: _ClassVar[int]\nFORMAT_STRING_FIELD_NUMBER: _ClassVar[int]\nTIME_ZONE_FIELD_NUMBER: _ClassVar[int]\nformat_type: TimestampFormatConfig.FormatType\nformat_string: str\ntime_zone: str\ndef __init__(self, format_type: _Optional[_Union[TimestampFormatConfig.FormatType, str]] = ..., format_string: _Optional[str] = ..., time_zone: _Optional[str] = ...) -> None: ...\n\nclass Watermark(_message.Message):\n__slots__ = (\"time_col\", \"allowed_delay_seconds\", \"time_col_config\")\nTIME_COL_FIELD_NUMBER: _ClassVar[int]\nALLOWED_DELAY_SECONDS_FIELD_NUMBER: _ClassVar[int]\nTIME_COL_CONFIG_FIELD_NUMBER: _ClassVar[int]\ntime_col: str\nallowed_delay_seconds: int\ntime_col_config: TimestampFormatConfig\ndef __init__(self, time_col: _Optional[str] = ..., allowed_delay_seconds: _Optional[int] = ..., time_col_config: _Optional[_Union[TimestampFormatConfig, _Mapping]] = ...) -> None: ...\n\nclass DataSource(_message.Message):\n__slots__ = (\"name\", \"key_fields\", \"environment\", \"encoded_schema\", \"delivery_mode\", \"watermark\", \"file_source\", \"postgres_source\", \"kafka_source\", \"feature_group_source\")\nNAME_FIELD_NUMBER: _ClassVar[int]\nKEY_FIELDS_FIELD_NUMBER: _ClassVar[int]\nENVIRONMENT_FIELD_NUMBER: _ClassVar[int]\nENCODED_SCHEMA_FIELD_NUMBER: _ClassVar[int]\nDELIVERY_MODE_FIELD_NUMBER: _ClassVar[int]\nWATERMARK_FIELD_NUMBER: _ClassVar[int]\nFILE_SOURCE_FIELD_NUMBER: _ClassVar[int]\nPOSTGRES_SOURCE_FIELD_NUMBER: _ClassVar[int]\nKAFKA_SOURCE_FIELD_NUMBER: _ClassVar[int]\nFEATURE_GROUP_SOURCE_FIELD_NUMBER: _ClassVar[int]\nname: str\nkey_fields: _containers.RepeatedScalarFieldContainer[str]\nenvironment: str\nencoded_schema: str\ndelivery_mode: DataDeliveryMode\nwatermark: Watermark\nfile_source: FileSource\npostgres_source: PostgresSource\nkafka_source: KafkaSource\nfeature_group_source: FeatureGroupSource\ndef __init__(self, name: _Optional[str] = ..., key_fields: _Optional[_Iterable[str]] = ..., environment: _Optional[str] = ..., encoded_schema: _Optional[str] = ..., delivery_mode: _Optional[_Union[DataDeliveryMode, str]] = ..., watermark: _Optional[_Union[Watermark, _Mapping]] = ..., file_source: _Optional[_Union[FileSource, _Mapping]] = ..., postgres_source: _Optional[_Union[PostgresSource, _Mapping]] = ..., kafka_source: _Optional[_Union[KafkaSource, _Mapping]] = ..., feature_group_source: _Optional[_Union[FeatureGroupSource, _Mapping]] = ...) -> None: ...\n\n```"
    },
    {
        "section": "sources_p2p.py",
        "content": "# sources_p2p.py\n-## Location -> root_directory.common.sources\n```python\n# This is an automatically generated file, please do not change\n# gen by protobuf_to_pydantic[v0.3.0.2](https://github.com/so1n/protobuf_to_pydantic)\n# Protobuf Version: 5.29.1\n# Pydantic Version: 2.10.4\nimport typing\nfrom enum import IntEnum\n\nfrom google.protobuf.message import Message  # type: ignore\nfrom protobuf_to_pydantic.customer_validator import check_one_of\nfrom pydantic import BaseModel, Field, model_validator\n\n\nclass DataDeliveryMode(IntEnum):\nSTATIC = 0\nDYNAMIC = 1\n\nclass KafkaSource(BaseModel):\ntopic: str = Field(default=\"\")\nproto_message_name: str = Field(default=\"\")\nschema_version: int = Field(default=0)\n\nclass FeatureGroupSource(BaseModel):\nname: str = Field(default=\"\")\n\nclass S3Config(BaseModel):\nbucket: str = Field()\naccess_key_id: typing.Optional[str] = Field(default=\"\")\nsecret_access_key: typing.Optional[str] = Field(default=\"\")\nregion: str = Field()\nendpoint: typing.Optional[str] = Field(default=\"\")\n\nclass FileSource(BaseModel):\nclass Format(IntEnum):\nCSV = 0\nJSON = 1\nAVRO = 2\nPARQUET = 3\nORC = 4\n\n_one_of_dict = {\"FileSource.storage_config\": {\"fields\": {\"s3_config\"}, \"required\": True}}\none_of_validator = model_validator(mode=\"before\")(check_one_of)\npath: str = Field(default=\"\")\nformat: \"FileSource.Format\" = Field(default=0)\ns3_config: typing.Optional[S3Config] = Field(default=None)\n\nclass PostgresSource(BaseModel):\nhost: str = Field(default=\"\")\nport: int = Field(default=0)\nusername: str = Field(default=\"\")\npassword: str = Field(default=\"\")\ntable: str = Field(default=\"\")\ndatabase: str = Field(default=\"\")\nincrementing_column: str = Field(default=\"\")\ntimestamp_column: str = Field(default=\"\")\n\nclass TimestampFormatConfig(BaseModel):\nclass FormatType(IntEnum):\nEpochMillis = 0\nEpochSeconds = 1\nStringTimestamp = 2\n\nformat_type: \"TimestampFormatConfig.FormatType\" = Field(default=0)\nformat_string: typing.Optional[str] = Field(default=\"\")\ntime_zone: typing.Optional[str] = Field(default=\"\")\n\nclass Watermark(BaseModel):\ntime_col: str = Field(default=\"\")\nallowed_delay_seconds: int = Field(default=0)\ntime_col_config: typing.Optional[TimestampFormatConfig] = Field(default=None)\n\nclass DataSource(BaseModel):\n_one_of_dict = {\"DataSource.type\": {\"fields\": {\"feature_group_source\", \"file_source\", \"kafka_source\", \"postgres_source\"}, \"required\": True}}\none_of_validator = model_validator(mode=\"before\")(check_one_of)\nname: str = Field(default=\"\", min_length=1, pattern=\"^[a-z]([a-z0-9_]{0,48}[a-z0-9])?$\")\nkey_fields: typing.List[str] = Field(default_factory=list)\nenvironment: typing.Optional[str] = Field(default=\"\")\nencoded_schema: typing.Optional[str] = Field(default=\"\")\ndelivery_mode: DataDeliveryMode = Field(default=0)\nwatermark: typing.Optional[Watermark] = Field(default=None)\nfile_source: typing.Optional[FileSource] = Field(default=None)\npostgres_source: typing.Optional[PostgresSource] = Field(default=None)\nkafka_source: typing.Optional[KafkaSource] = Field(default=None)\nfeature_group_source: typing.Optional[FeatureGroupSource] = Field(default=None)\n\n```"
    }
]