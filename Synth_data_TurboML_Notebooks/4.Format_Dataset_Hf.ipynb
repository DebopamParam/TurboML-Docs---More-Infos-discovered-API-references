{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 89 contexts and 217 answer groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing answer groups: 100%|██████████| 217/217 [00:00<00:00, 216964.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1343 structured rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load datasets\n",
    "with open(\"merged_contexts.json\", \"r\") as f:\n",
    "    merged_contexts = json.load(f)\n",
    "\n",
    "with open(\"full_answers.json\", \"r\") as f:\n",
    "    full_answers = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(merged_contexts)} contexts and {len(full_answers)} answer groups\")\n",
    "\n",
    "# Create mapping dictionary for quick context lookup\n",
    "context_map = {ctx[\"base_chunk\"]: ctx for ctx in merged_contexts}\n",
    "\n",
    "# Structure the dataset\n",
    "structured_data = []\n",
    "\n",
    "for answer_group in tqdm(full_answers, desc=\"Processing answer groups\"):\n",
    "    # Get corresponding context from merged_contexts using context_id\n",
    "    context_id = answer_group[\"context_id\"]\n",
    "    original_context = merged_contexts[context_id]\n",
    "    \n",
    "    for answer in answer_group[\"answers\"]:\n",
    "        # Create data row\n",
    "        data_row = {\n",
    "            \"question\": answer[\"question\"],\n",
    "            \"answer\": answer[\"answer\"],\n",
    "            \"context\": original_context[\"content\"],\n",
    "            \"base_chunk\": original_context[\"base_chunk\"],\n",
    "            \"context_sections\": original_context[\"context_sections\"],\n",
    "            \"generation_timestamp_ns\": answer[\"generation_time\"]\n",
    "        }\n",
    "        \n",
    "        structured_data.append(data_row)\n",
    "\n",
    "print(f\"Created {len(structured_data)} structured rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to hf_turboml_dataset.json\n"
     ]
    }
   ],
   "source": [
    "# Save final dataset\n",
    "with open(\"hf_turboml_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(structured_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Dataset saved to hf_turboml_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Statistics:\n",
      "- Total Q&A pairs: 1,343\n",
      "- Average context length: 48,265 chars\n",
      "- Unique documentation sections referenced: 89\n",
      "- Time range: 1740785987488093300 to 1740828912971381500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataset statistics\n",
    "total_questions = len(structured_data)\n",
    "avg_context_length = sum(len(row[\"context\"]) for row in structured_data) / total_questions\n",
    "unique_sections = len(set(section for row in structured_data for section in row[\"context_sections\"]))\n",
    "\n",
    "print(f\"\"\"\n",
    "Dataset Statistics:\n",
    "- Total Q&A pairs: {total_questions:,}\n",
    "- Average context length: {avg_context_length:,.0f} chars\n",
    "- Unique documentation sections referenced: {unique_sections}\n",
    "- Time range: {min(row[\"generation_timestamp_ns\"] for row in structured_data)} to {max(row[\"generation_timestamp_ns\"] for row in structured_data)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(structured_data) == sum(len(g[\"answers\"]) for g in full_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_row = structured_data[0]\n",
    "original_ctx = merged_contexts[[ctx[\"base_chunk\"] for ctx in merged_contexts].index(sample_row[\"base_chunk\"])]\n",
    "assert sample_row[\"context\"] == original_ctx[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"Write code to ingest data from a Pandas DataFrame into TurboML using the `OnlineDataset.from_pd()` method, setting 'transactionID' as the key field and handling the case where the dataset already exists.\",\n",
       " 'answer': 'Okay, let\\'s break down this task and address it using the provided TurboML documentation.\\n\\n**Problem Analysis:**\\n\\nThe user wants to ingest data from a Pandas DataFrame into TurboML.  They\\'ve specified the `OnlineDataset.from_pd()` method, the key field (\\'transactionID\\'), and need to handle potential dataset pre-existence. The core task is data ingestion using the push-based method.\\n\\n**Step-by-Step Solution:**\\n\\n1.  **Import `turboml`:** Begin by importing the necessary library.\\n2.  **Create DataFrame:** We\\'ll assume a DataFrame named `df` is already available, as per the question.\\n3.  **Instantiate `OnlineDataset`:** Use `OnlineDataset.from_pd()`, providing the DataFrame (`df`), a dataset ID (`id`), the `key_field`, and setting `load_if_exists=True`.\\n\\n**Code Implementation:**\\n\\n```python\\nimport turboml as tb\\n\\n# Assuming \\'df\\' is your pre-existing Pandas DataFrame\\n# Example df creation (replace with your actual DataFrame)\\nimport pandas as pd\\ndata = {\\'transactionID\\': [1, 2, 3], \\'amount\\': [10, 20, 30], \\'location\\': [\\'A\\', \\'B\\', \\'C\\']}\\ndf = pd.DataFrame(data)\\n\\n# Data Ingestion\\ntransactions = tb.OnlineDataset.from_pd(\\n    id=\"my_transactions\",  # Choose a unique dataset ID\\n    df=df,\\n    key_field=\"transactionID\",\\n    load_if_exists=True,\\n)\\n\\n```\\n\\n**Explanation and Documentation References:**\\n\\n*   We use `tb.OnlineDataset.from_pd()` as specified in the question and documented in [Data Ingestion].\\n*   `id=\"my_transactions\"`:  This assigns a unique identifier to your dataset within TurboML.  Choose a descriptive name.\\n*   `df=df`: This passes your Pandas DataFrame to TurboML.\\n*   `key_field=\"transactionID\"`: This designates the \\'transactionID\\' column as the primary key, as requested.\\n*   `load_if_exists=True`: This crucial part handles the case where a dataset with the same ID might already exist. Instead of throwing an error, it loads the existing dataset.  This is directly from the documentation [Data Ingestion].\\n\\n**Common Pitfalls & Debugging Tips:**\\n\\n*   **Incorrect `key_field`:** Ensure the `key_field` you specify *exactly* matches a column name in your DataFrame. Case sensitivity matters.  If this is incorrect, you\\'ll likely encounter errors during the `from_pd` call.\\n*   **Schema Mismatches:** If `load_if_exists=True` and the existing dataset has a *different* schema (column names or data types) than your DataFrame, you will get an error. The error message will usually indicate a schema mismatch.  You\\'ll need to either delete the existing dataset (if appropriate) or adjust your DataFrame to match the existing schema.\\n*   **Missing Imports:** Make sure you have imported `turboml` as `tb`.\\n* **Authentication:** Ensure you are logged in using `tb.login()` or have set the `TURBOML_API_KEY` environment variable.\\n\\nExample of a schema mismatch error (if, for example, the existing dataset didn\\'t have an \\'amount\\' column):\\n\\n```\\nValueError: Schema mismatch: the dataframe does not match the dataset\\'s input schema. Expected: DatasetSchema(fields={\\'transactionID\\': <Datatype.STRING: \\'STRING\\'>, \\'location\\': <Datatype.STRING: \\'STRING\\'>}), got: DatasetSchema(fields={\\'transactionID\\': <Datatype.INT64: \\'INT64\\'>, \\'amount\\': <Datatype.INT64: \\'INT64\\'>, \\'location\\': <Datatype.STRING: \\'STRING\\'>})\\n```\\n\\nIn this case, the error message clearly indicates that the expected and actual schemas don\\'t match.\\nsource: [File: common/datasets.py]',\n",
       " 'context': '# **TurboML: A Real-Time Machine Learning Platform - Detailed Summary**\\n\\nTurboML is a platform designed for building, deploying, and managing real-time machine learning applications. It emphasizes streaming data and provides tools for the entire ML lifecycle, from data ingestion to model monitoring.\\n\\n**1. Data Ingestion and Management:**\\n\\n*   **Core Principle:** TurboML treats data as continuous streams, enabling real-time processing and updates.\\n*   **Ingestion Methods:**\\n    *   **Pull-based:**\\n        *   Uses pre-built connectors to continuously pull data from various sources.\\n        *   Supported sources include cloud storage (e.g., S3) and databases (e.g., Postgres). *While Kafka is used internally, the documentation doesn\\'t explicitly present it as a direct pull-based source for end-users in the introductory sections.*\\n        *   Connectors are configured to handle data formats and connection details.\\n    *   **Push-based:**\\n        *   Allows direct data injection into the TurboML platform.\\n        *   Methods:\\n            *   **REST API:** Send data via HTTP requests using the `dataset/{dataset_id}/upload` endpoint.\\n            *   **Client SDKs:** More performant options for high-volume data. The Python SDK provides convenient methods for working with Pandas DataFrames.\\n            *   **gRPC API:** Upload data using Arrow Flight gRPC, providing the most performant option.\\n        *   Example (Pandas DataFrame):\\n            ```python\\n            transactions = tb.OnlineDataset.from_pd(\\n                id=\"qs_transactions\",\\n                df=transactions_df,\\n                key_field=\"transactionID\",\\n                load_if_exists=True,\\n            )\\n            ```\\n*   **Dataset Classes:**\\n    *   **`OnlineDataset`:**\\n        *   Represents a dataset managed by the TurboML platform.\\n        *   Supports continuous data ingestion (pull or push).\\n        *   Provides methods for feature engineering, model deployment, and monitoring.\\n        *   Can be created from Pandas DataFrames (`from_pd`), or loaded if it already exists (`load`).\\n        *   `add_pd()` method allows adding new data to an existing `OnlineDataset` (using Arrow Flight Protocol over gRPC). There are also `dataset/dataset_id/upload` REST API endpoint and direct gRPC API options for data upload.\\n        *   `sync_features()` method synchronizes materialized streaming features to the `OnlineDataset` object.  This is important after uploading new data or materializing features.\\n    *   **`LocalDataset`:**\\n        *   Represents an in-memory dataset, primarily for local experimentation and development.\\n        *   Can be created from Pandas DataFrames (`from_pd`).\\n        *   Useful for testing feature engineering logic before deploying to an `OnlineDataset`.\\n        *   Can be converted to an `OnlineDataset` using `to_online()`.\\n    * **`PandasDataset`:** *This class is present in the `intro` documentation, but its role and usage are less clearly defined compared to `OnlineDataset` and `LocalDataset`. It appears to be a less preferred way of interacting with data.*\\n* **Data Schema:**\\n    *   Datasets have a defined schema, specifying field names and data types.\\n    *   Schemas are automatically inferred from Pandas DataFrames.\\n    *   Schemas are managed by the platform and used for data validation and consistency.\\n    *   Supported data types include: INT32, INT64, FLOAT, DOUBLE, STRING, BOOL, BYTES.\\n* **Key Field:**\\n    *   Each dataset must have a primary key field (`key_field`) to uniquely identify records.\\n    *   Used for merging data, performing lookups, and ensuring data integrity.\\n\\n**2. Feature Engineering:**\\n\\n*   **Core Philosophy:** Define features once, test them locally, and then deploy them for continuous, real-time computation.\\n*   **Feature Definition Interfaces:**\\n    *   **SQL Features:**\\n        *   Define features using standard SQL expressions.\\n        *   Column names are enclosed in double quotes.\\n        *   Example:\\n            ```python\\n            transactions.feature_engineering.create_sql_features(\\n                sql_definition=\\'\"transactionAmount\" + \"localHour\"\\',\\n                new_feature_name=\"my_sql_feat\",\\n            )\\n            ```\\n    *   **Aggregate Features:**\\n        *   Define time-windowed aggregations (SUM, COUNT, AVG, MIN, MAX, etc.).\\n        *   Require a registered timestamp column.\\n        *   Specify:\\n            *   `column_to_operate`: The column to aggregate.\\n            *   `column_to_group`: The column(s) to group by.\\n            *   `operation`: The aggregation function (SUM, COUNT, etc.).\\n            *   `new_feature_name`: The name of the new feature column.\\n            *   `timestamp_column`: The column containing timestamps.\\n            *   `window_duration`: The length of the time window.\\n            *   `window_unit`: The unit of the window duration (seconds, minutes, hours, etc.).\\n        *   Example:\\n            ```python\\n            transactions.feature_engineering.register_timestamp(column_name=\"timestamp\", format_type=\"epoch_seconds\")\\n\\n            transactions.feature_engineering.create_aggregate_features(\\n                column_to_operate=\"transactionAmount\",\\n                column_to_group=\"accountID\",\\n                operation=\"SUM\",\\n                new_feature_name=\"my_sum_feat\",\\n                timestamp_column=\"timestamp\",\\n                window_duration=24,\\n                window_unit=\"hours\",\\n            )\\n            ```\\n    *   **User-Defined Functions (UDFs):**\\n        *   Define features using custom Python code.\\n        *   **Simple UDFs:** Functions that take column values as input and return a single value.\\n            ```python\\n            myfunction_contents = \"\"\"\\n            import numpy as np\\n\\n            def myfunction(x):\\n                return np.sin(x)\\n            \"\"\"\\n            transactions.feature_engineering.create_udf_features(\\n                new_feature_name=\"sine_of_amount\",\\n                argument_names=[\"transactionAmount\"],\\n                function_name=\"myfunction\",\\n                function_file_contents=myfunction_contents,\\n                libraries=[\"numpy\"],\\n            )\\n\\n            ```\\n        *   **Rich UDFs:** Class-based UDFs that can maintain state and perform more complex operations (e.g., database lookups). Require an `__init__` method and a `func` method. The `dev_initializer_arguments` and `prod_initializer_arguments` are used for development and production environments, respectively.\\n        *   **User-Defined Aggregate Functions (UDAFs):** Custom aggregations defined in Python. Require implementing `create_state`, `accumulate`, `retract` (optional), `merge_states`, and `finish` methods.\\n            ```python\\n            function_file_contents = \"\"\"\\n            def create_state():\\n                return 0, 0\\n\\n            def accumulate(state, value, weight):\\n                if value is None or weight is None:\\n                    return state\\n                (s, w) = state\\n                s += value * weight\\n                w += weight\\n                return s, w\\n\\n            def retract(state, value, weight):\\n                if value is None or weight is None:\\n                    return state\\n                (s, w) = state\\n                s -= value * weight\\n                w -= weight\\n                return s, w\\n\\n            def merge_states(state_a, state_b):\\n                (s_a, w_a) = state_a\\n                (s_b, w_b) = state_b\\n                return s_a + s_b, w_a + w_b\\n\\n            def finish(state):\\n                (sum, weight) = state\\n                if weight == 0:\\n                    return None\\n                else:\\n                    return sum / weight\\n            \"\"\"\\n            transactions.feature_engineering.create_udaf_features(\\n                new_feature_name=\"weighted_avg\",\\n                column_to_operate=[\"transactionAmount\", \"transactionTime\"],\\n                function_name=\"weighted_avg\",\\n                return_type=\"DOUBLE\",\\n                function_file_contents=function_file_contents,\\n                column_to_group=[\"accountID\"],\\n                timestamp_column=\"timestamp\",\\n                window_duration=1,\\n                window_unit=\"hours\",\\n            )\\n            ```\\n    *   **Ibis Features:**\\n        *   Define complex streaming features using the Ibis DataFrame API.\\n        *   Supports Apache Flink and RisingWave backends for execution.\\n        *   Allows for more sophisticated feature engineering than simple SQL or aggregations.\\n        *   Example:\\n            ```python\\n            fe = tb.IbisFeatureEngineering()\\n            transactions = fe.get_ibis_table(\"transactions_stream\")\\n            # ... define features using Ibis expressions ...\\n            fe.materialize_features(\\n                transactions_with_frequency_score,\\n                \"transactions_with_frequency_score\",\\n                \"transactionID\",\\n                BackEnd.Flink, # Or BackEnd.Risingwave\\n                \"transactions_stream\",\\n            )\\n            ```\\n\\n*   **Feature Materialization:**\\n    *   `materialize_features()`: Submits feature definitions to the platform for continuous computation. Features are computed in real-time as new data arrives.\\n    *   `materialize_ibis_features()`: Submits Ibis feature definitions.\\n*   **Feature Retrieval:**\\n    *   `get_features()`: Retrieves a *snapshot* of the raw data stream (for experimentation). *Note:* The returned data is not guaranteed to be in the same order or size on each call.\\n    *   `get_local_features()`: Returns a DataFrame with the locally computed features (for debugging and experimentation).\\n    *   `get_materialized_features()`: Retrieves the *continuously computed* features from the platform.\\n    *   `retrieve_features()`: Computes feature values on ad-hoc data (not part of the stream).\\n*   **Timestamp Handling:**\\n    *   `register_timestamp()`: Registers a column as the timestamp for the dataset. Required for time-windowed aggregations.\\n    *   `get_timestamp_formats()`: Returns a list of supported timestamp format strings.\\n    *   `convert_timestamp()`: *This function is an internal utility, not a directly exposed user-facing API. It\\'s used within the feature engineering logic.*\\n* **Classes/Functions:**\\n    *   `FeatureEngineering`: Class for defining SQL and aggregation features on `OnlineDataset`.\\n    *   `LocalFeatureEngineering`: Class for defining features on `LocalDataset`.\\n    *   `IbisFeatureEngineering`: Class for defining features using the Ibis interface.\\n    *   `tb.register_source()`: Registers a data source configuration.\\n    *   `DataSource`: Defines where and how raw data is accessed.\\n        *    `FileSource`: Specifies a file-based data source (e.g., CSV, Parquet).\\n        *   `PostgresSource`: Specifies a PostgreSQL data source.\\n        *   `KafkaSource`: *Mentioned in the context, but not as a direct source for `DataSource` in the `intro`.*\\n        *   `FeatureGroupSource`: Specifies a feature group as a data source.\\n    *   `TimestampFormatConfig`: Configures timestamp format.\\n    *   `Watermark`: Defines watermark settings for streaming data.\\n    *   `TurboMLScalarFunction`: Base class for defining rich UDFs.\\n\\n**3. ML Modeling:**\\n\\n*   **Model Types:**\\n    *   **Supervised:** Models that learn from labeled data (e.g., classification, regression).\\n    *   **Unsupervised:** Models that learn from unlabeled data (e.g., anomaly detection).\\n*   **Input Specification:**\\n    *   Models require specifying the input features using:\\n        *   `numerical_fields`: List of numeric column names.\\n        *   `categorical_fields`: List of categorical column names.\\n        *   `textual_fields`: List of text column names.\\n        *   `imaginal_fields`: List of image column names (binary data).\\n        *   `time_field`: Name of the timestamp column (optional).\\n    *   Example:\\n        ```python\\n        numerical_fields = [\"transactionAmount\", \"localHour\"]\\n        categorical_fields = [\"digitalItemCount\", \"physicalItemCount\", \"isProxyIP\"]\\n        features = transactions.get_model_inputs(\\n            numerical_fields=numerical_fields, categorical_fields=categorical_fields\\n        )\\n        label = labels.get_model_labels(label_field=\"is_fraud\")\\n        ```\\n* **Supported Algorithms (Examples):**\\n    *   **Classification:**\\n        *   `HoeffdingTreeClassifier`: Incremental decision tree for classification.\\n        *   `AMFClassifier`: Aggregated Mondrian Forest classifier.\\n        *   `FFMClassifier`: Field-aware Factorization Machine classifier.\\n        *   `SGTClassifier`: Stochastic Gradient Tree classifier.\\n        *   `MultinomialNB`: Multinomial Naive Bayes.\\n        *   `GaussianNB`: Gaussian Naive Bayes.\\n    *   **Regression:**\\n        *   `HoeffdingTreeRegressor`: Incremental decision tree for regression.\\n        *   `AMFRegressor`: Aggregated Mondrian Forest regressor.\\n        *   `FFMRegressor`: Field-aware Factorization Machine regressor.\\n        *   `SGTRegressor`: Stochastic Gradient Tree regressor.\\n        *   `SNARIMAX`: Time series forecasting model.\\n    *   **Anomaly Detection:**\\n        *   `RCF`: Random Cut Forest.\\n        *   `HST`: Half-Space Trees.\\n        *   `MStream`: Multi-aspect stream anomaly detection.\\n    *   **Ensemble Methods:**\\n        *   `LeveragingBaggingClassifier`: Bagging with ADWIN for concept drift.\\n        *   `HeteroLeveragingBaggingClassifier`: Bagging with different base models.\\n        *   `AdaBoostClassifier`: AdaBoost.\\n        *   `HeteroAdaBoostClassifier`: AdaBoost with different base models.\\n        *   `BanditModelSelection`: Model selection using bandit algorithms.\\n        *   `ContextualBanditModelSelection`: Model selection using contextual bandits.\\n        *   `RandomSampler`: Random sampling for imbalanced datasets.\\n    *   **General Purpose:**\\n        *   `NeuralNetwork`: Configurable neural network.\\n        *   `ONN`: Online Neural Network.\\n        *   `AdaptiveXGBoost`: XGBoost with concept drift handling.\\n        *   `AdaptiveLGBM`: LightGBM with concept drift handling.\\n        *   `ONNX`: Deploy models from other frameworks (PyTorch, TensorFlow, Scikit-learn) using ONNX format (static models).\\n        *   `Python`: Define custom models using Python classes.\\n        *   `PythonEnsembleModel`: Define custom ensemble models using Python classes.\\n        *   `RestAPIClient`: Use custom models via REST API.\\n        *   `GRPCClient`: Use custom models via gRPC API.\\n*   **Bring Your Own Models (BYOM):**\\n    *   **ONNX:** Deploy models trained in other frameworks.\\n    *   **Python:** Define custom models with `learn_one` and `predict_one` methods. Requires defining a class with `init_imports`, `learn_one`, and `predict_one` methods. A virtual environment (`venv`) can be set up to manage dependencies.\\n    *   **gRPC:** Integrate models via gRPC.\\n    *   **REST API:** Integrate models via REST API.\\n*   **Model Composition:**\\n    *   Combine models using ensemble methods (e.g., `LeveragingBaggingClassifier`).\\n    *   Chain preprocessors with models (e.g., `MinMaxPreProcessor` + `HoeffdingTreeClassifier`).\\n*   **Preprocessors:**\\n    *   Transform input data before it\\'s passed to the model.\\n    *   Examples:\\n        *   `MinMaxPreProcessor`: Scales numerical features to [0, 1].\\n        *   `NormalPreProcessor`: Standardizes numerical features (zero mean, unit variance).\\n        *   `RobustPreProcessor`: Scales features using robust statistics (median, IQR).\\n        *   `LabelPreProcessor`: Converts strings to ordinal integers (textual fields).\\n        *   `OneHotPreProcessor`: Creates one-hot encoding for categorical features.\\n        *   `BinaryPreProcessor`: Creates binary encoding for categorical features.\\n        *   `FrequencyPreProcessor`: Encodes strings based on their frequency.\\n        *   `TargetPreProcessor`: Encodes strings based on the average target value.\\n        *   `LlamaCppPreProcessor`: Generates text embeddings using Llama.cpp models (GGUF format).\\n        *   `ClipEmbeddingPreprocessor`: Generates image embeddings using CLIP models (GGUF format).\\n        *   `ImageToNumericPreProcessor`: Converts binary image data to numerical data.\\n        *   `RandomProjectionEmbedding`: Dimensionality reduction using random projection.\\n        *   `LLAMAEmbedding`: Text embeddings using GGUF models.\\n        *   `ClipEmbedding`: Image embeddings using GGUF models.\\n        *   `EmbeddingModel`: Combines an embedding model with a base model.\\n        *   `OVR`: One-vs-the-rest multiclass strategy.\\n    *   Preprocessors are typically combined with a `base_model`.\\n* **Model Training:**\\n    *   **Batch Training:**\\n        *   Use the `learn()` method on a `Model` instance.\\n        *   Can be used to incrementally train a model on multiple batches of data.\\n        *   Example:\\n            ```python\\n            model = tb.HoeffdingTreeClassifier(n_classes=2)\\n            trained_model = model.learn(features, label)  # Train on initial data\\n            new_trained_model = trained_model.learn(new_features, new_label)  # Update with new data\\n            ```\\n    *   **Streaming Training:**\\n        *   Models are continuously updated as new data arrives.\\n        *   Enabled by default when deploying a model with `deploy()`.\\n        *   Can be configured with different update strategies:\\n            *   **Online:** Update on every data point.\\n            *   **Trigger-based:** Update based on volume, time, performance, or drift (*mentioned in `intro`, but specific configuration details are limited in the provided documentation*).\\n* **Model Deployment:**\\n    *   `model.deploy(name, input, labels, predict_only=False)`: Deploys a model to the TurboML platform.\\n        *   `name`: A unique name for the deployed model.\\n        *   `input`: An `OnlineInputs` object defining the input features.\\n        *   `labels`: An `OnlineLabels` object defining the target labels.\\n        *   `predict_only`: If `True`, the model will not be updated with new data (useful for batch-trained models).\\n    *   Returns a `DeployedModel` instance.\\n* **Model Retrieval:**\\n    *    `tb.retrieve_model(model_name: str)`: Fetches a reference to an already deployed model, allowing interaction in a new workspace/environment without redeployment.\\n\\n**4. Model Evaluation and Monitoring (MLOps):**\\n\\n*   **Evaluation Metrics:**\\n    *   **Built-in Metrics:**\\n        *   `WindowedAUC`: Area Under the ROC Curve (for classification).\\n        *   `WindowedAccuracy`: Accuracy (for classification).\\n        *   `WindowedMAE`: Mean Absolute Error (for regression).\\n        *   `WindowedMSE`: Mean Squared Error (for regression).\\n        *   `WindowedRMSE`: Root Mean Squared Error (for regression).\\n    *   **Custom Metrics:**\\n        *   Define custom aggregate metrics using Python classes that inherit from `ModelMetricAggregateFunction`.\\n        *   Implement `create_state`, `accumulate`, `retract` (optional), `merge_states`, and `finish` methods.\\n    *   **Continuous Evaluation:** Metrics are calculated continuously as new data arrives.\\n    *   **Functions:**\\n        *   `deployed_model.add_metric(metric_name)`: Registers a metric for a deployed model.\\n        *   `deployed_model.get_evaluation(metric_name, filter_expression=\"\", window_size=1000, limit=100000)`: Retrieves evaluation results for a specific metric. The `filter_expression` allows filtering data based on SQL expressions.\\n        *   `tb.evaluation_metrics()`: Lists available built-in metrics.\\n        *   `tb.register_custom_metric(metric_name, metric_class)`: Registers a custom metric.\\n        *   `tb.compare_model_metrics(models, metric)`: Compares multiple models on a given metric (generates a Plotly plot).\\n*   **Drift Detection:**\\n    *   **Univariate Drift:** Detects changes in the distribution of individual features.\\n        *   Uses the Adaptive Windowing (ADWIN) method by default.\\n        *   Register with `dataset.register_univariate_drift(numerical_field, label=None)`.\\n        *   Retrieve with `dataset.get_univariate_drift(label=None, numerical_field=None)`.\\n    *   **Multivariate Drift:** Detects changes in the joint distribution of multiple features.\\n        *   Uses PCA-based reconstruction by default.\\n        *   Register with `dataset.register_multivariate_drift(label, numerical_fields)`.\\n        *   Retrieve with `dataset.get_multivariate_drift(label)`.\\n    *   **Model Drift (Target Drift):** Detects changes in the relationship between input features and the target variable.\\n        *   Register with `deployed_model.add_drift()`.\\n        *   Retrieve with `deployed_model.get_drifts()`.\\n*   **Model Explanations:**\\n    *   Integration with the `iXAI` library for incremental model explanations.\\n    *   Provides insights into feature importance and model behavior.  The example shows using `IncrementalPFI` from `iXAI`.\\n*   **Model Management:**\\n    *   `deployed_model.pause()`: Pauses a running model.\\n    *   `deployed_model.resume()`: Resumes a paused model.\\n    *   `deployed_model.delete(delete_output_topic=True)`: Deletes a model and optionally its associated output data.\\n    *   `deployed_model.get_endpoints()`: Retrieves the API endpoints for a deployed model. These endpoints can be used for synchronous inference.\\n    *   `deployed_model.get_logs()`: Retrieves logs for a deployed model.\\n* **Inference:**\\n    *   **Async:**\\n        *   The data streamed from the input source is continuously fed to the model, and the outputs are streamed to another source.\\n        *   ```python\\n            outputs = deployed_model.get_outputs()\\n            ```\\n    *   **API:**\\n        *   A request-response model is used for inference on a single data point synchronously.\\n        *   The `/model_name/predict` endpoint is exposed for each deployed model where a REST API call can be made to get the outputs.\\n            ```python\\n            import requests\\n            resp = requests.post(model_endpoints[0], json=model_query_datapoint, headers=tb.common.api.headers)\\n            ```\\n    *   **Batch:**\\n        *   When you have multiple records you’d like to perform inference on, you can use the get_inference method as follows.\\n            ```python\\n            outputs = deployed_model.get_inference(query_df)\\n            ```\\n\\n**5. Advanced Features:**\\n\\n*   **Hyperparameter Tuning:**\\n    *   `tb.hyperparameter_tuning()`: Performs grid search to find the best combination of hyperparameters for a given model and dataset.\\n*   **Algorithm Tuning:**\\n    *   `tb.algorithm_tuning()`: Compares different models on a given dataset to identify the best-performing algorithm.\\n* **Local Model:**\\n    * The `LocalModel` class provides direct access to TurboML\\'s machine learning models in Python, allowing for local training and prediction without deploying to the platform.\\n    * Useful for offline experimentation and development.\\n\\n**Code Structure and Key Modules:**\\n\\n*   **`turboml`:** The main package, providing access to all core functionalities.\\n*   **`common`:**\\n    *   `api.py`: Handles communication with the TurboML backend API. Includes `ApiException`, `NotFoundException`, and retry logic.\\n    *   `dataloader.py`: Functions for data loading, serialization, and interaction with the Arrow server. Includes `get_proto_msgs`, `create_protobuf_from_row_tuple`, `upload_df`, and more.\\n    *   `datasets.py`: Defines dataset classes (`LocalDataset`, `OnlineDataset`, standard datasets like `FraudDetectionDatasetFeatures`).\\n    *   `feature_engineering.py`: Classes and functions for defining and managing features (`FeatureEngineering`, `LocalFeatureEngineering`, `IbisFeatureEngineering`).\\n    *   `internal.py`: Internal utilities (e.g., `TbPyArrow` for PyArrow helpers, `TbPandas` for Pandas helpers).\\n    *   `llm.py`: Functions for working with Large Language Models (LLMs), including acquiring models from Hugging Face and spawning LLM servers.\\n    *   `ml_algs.py`: Core ML algorithms, model deployment logic, and model classes (`Model`, `DeployedModel`, `LocalModel`). Includes functions like `ml_modelling`, `model_learn`, `model_predict`, `get_score_for_model`, and model classes for various algorithms.\\n    *   `models.py`: Pydantic models for data structures and API requests/responses. Defines core data structures like `DatasetSchema`, `InputSpec`, `ModelConfigStorageRequest`, etc.\\n    *   `namespaces.py`: Functions for managing namespaces.\\n    *   `pymodel.py`: Python bindings for the underlying C++ model implementation (using `turboml_bindings`).\\n    *   `pytypes.py`: Python type definitions (using `turboml_bindings`).\\n    *   `udf.py`: Base class for defining custom metric aggregation functions (`ModelMetricAggregateFunction`).\\n    *   `util.py`: General utility functions (e.g., type conversions, timestamp handling, Ibis utilities).\\n    *   `default_model_configs.py`: Default configurations for ML algorithms.\\n    *   `model_comparison.py`: Functions for comparing model performance (e.g., `compare_model_metrics`).\\n    *   `env.py`: Environment configuration (e.g., server addresses).\\n*   **`protos`:**\\n    *   Contains Protobuf definitions (`.proto` files) and generated Python code (`_pb2.py`, `_pb2.pyi`) for data structures used in communication with the backend. Key files include `config.proto`, `input.proto`, `output.proto`, `metrics.proto`.\\n*   **`wyo_models`:**\\n    *   Contains examples of how to implement custom models using Python.\\n*   **Subpackages for Algorithms:**\\n    *   `regression`, `classification`, `anomaly_detection`, `ensembles`, `general_purpose`, `forecasting`: Contain specific ML algorithm implementations.\\n*   **`pre_deployment_ml`:**\\n    *   Contains modules for pre-deployment tasks like hyperparameter tuning and algorithm tuning.\\n*   **`post_deployment_ml`:**\\n    *   Contains modules for post-deployment tasks like drift detection, model explanations, and custom metrics.\\n*   **`general_examples`:**\\n    *   Contains examples of using TurboML features.\\n*   **`non_numeric_inputs`:**\\n    *   Contains examples of using non-numeric inputs like images and text.\\n*   **`byo_models`:**\\n    *   Contains examples of bringing your own models.\\n*   **`llms`:**\\n    *   Contains modules and examples for using LLMs.\\n\\n**Conclusion:**\\nTurboML provides a robust and flexible platform for building and deploying real-time machine learning applications. Its strengths lie in its streaming-first architecture, support for diverse data sources and ML algorithms, comprehensive feature engineering capabilities (including SQL, aggregations, UDFs, and Ibis integration), and built-in MLOps features like model monitoring, drift detection, and evaluation. The platform\\'s Python SDK and support for BYOM (Bring Your Own Model) via ONNX, gRPC, REST API, and custom Python code make it adaptable to a wide range of use cases and existing workflows. The combination of ease of use, performance, and real-time capabilities makes TurboML a powerful tool for developing and managing data-driven applications.\\n\\n# What is TurboML?\\n@ TurboML - page_link: https://docs.turboml.com/intro/\\n<page_content>\\nIntroduction  \\nTurboML is a machine learning platform that’s reinvented for real-time. What does that mean? All the steps in the ML lifecycle, from data ingestion, to feature engineering, to ML modelling to post deployment steps like monitoring, are all designed so that in addition to batch data, they can also handle real-time data.  \\n## Data Ingestion [Permalink for this section](https://docs.turboml.com/intro/\\\\#data-ingestion)  \\nThe first step is to bring your data to the TurboML platform. There are two major ways to ingest your data. Pull-based and Push-based.  \\n### Pull-based ingestion [Permalink for this section](https://docs.turboml.com/intro/\\\\#pull-based-ingestion)  \\nWith this approach, you use TurboML’s prebuilt connectors to connect to your data source. The connectors will continuously pull data from your data source, and ingest it into TurboML.  \\n### Push-based ingestion [Permalink for this section](https://docs.turboml.com/intro/\\\\#push-based-ingestion)  \\nSometimes, you might not want to send data via an intermediate data source, but rather directly send the data. Push-based ingestion can be used for this, where data can be send either via REST API calls, or using more performant client SDKs. Here’s an example with a Pandas DataFrame  \\n```transactions = tb.PandasDataset(dataset_name=\"transactions\",dataframe=df, upload=True)\\ntransactions.configure_dataset(key_field=\"index\")\\n```  \\n## Feature Engineering [Permalink for this section](https://docs.turboml.com/intro/\\\\#feature-engineering)  \\nFeature engineering is perhaps the most important step for data scientists. TurboML provides several different interfaces to define features. We’ve designed the feature engineering experience in a way so that after you’ve defined a feature, you can see that feature computed for your local data. This should help debug and iterate faster. Once you’re confident about a feature definition, you can deploy it where it’ll be continuously computed on the real-time data. Once deployed, these features are automatically computed on the streaming data. And we have retrieval APIs to compute it for ad-hoc queries.  \\n### SQL Features [Permalink for this section](https://docs.turboml.com/intro/\\\\#sql-features)  \\nWriting SQL queries is one of the most common way to define ML features. TurboML supports writing arbitrary SQL expressions to enable such features. Here’s an example with a simple SQL feature.  \\n```transactions.feature_engineering.create_sql_features(\\nsql_definition=\\'\"transactionAmount\" + \"localHour\"\\',\\nnew_feature_name=\"my_sql_feat\",\\n)\\n```  \\nNotice that the column names are in quotes.  \\nAnd here’s a more complicated example  \\n```transactions.feature_engineering.create_sql_features(\\nsql_definition=\\'CASE WHEN \"paymentBillingCountryCode\" <> \"ipCountryCode\" THEN 1 ELSE 0 END \\',\\nnew_feature_name=\"country_code_match\",\\n)\\n```  \\n### Aggregate Features [Permalink for this section](https://docs.turboml.com/intro/\\\\#aggregate-features)  \\nA common template for real-time features is aggregating some value over some time window. To define such time-windowed aggregations, you first need to register a timestamp column for your dataset. This can be done as follows,  \\n```transactions.feature_engineering.register_timestamp(column_name=\"timestamp\", format_type=\"epoch_seconds\")\\n```  \\nThe supported formats can be found out using  \\n```tb.get_timestamp_formats()\\n```  \\nOnce the timestamp is registered, we can create a feature using  \\n```transactions.feature_engineering.create_aggregate_features(\\ncolumn_to_operate=\"transactionAmount\",\\ncolumn_to_group=\"accountID\",\\noperation=\"SUM\",\\nnew_feature_name=\"my_sum_feat\",\\ntimestamp_column=\"timestamp\",\\nwindow_duration=24,\\nwindow_unit=\"hours\"\\n)\\n```  \\n### User Defined Features [Permalink for this section](https://docs.turboml.com/intro/\\\\#user-defined-features)  \\nWe understand why data scientists love Python - the simplicity, the ecosystem - is unmatchable. Guess what? You can use native Python, importing any library, [to define features](https://docs.turboml.com/feature_engineering/udf/)!  \\n### IBIS Features [Permalink for this section](https://docs.turboml.com/intro/\\\\#ibis-features)  \\nFor streaming features that are more complex than just windowed aggregations, can be defined using the [ibis interface](https://docs.turboml.com/feature_engineering/advanced/ibis_feature_engineering/). They can then be executed using Apache Flink or RisingWave.  \\n### Feature Retrieval [Permalink for this section](https://docs.turboml.com/intro/\\\\#feature-retrieval)  \\nAs mentioned before, once deployed, the feature computation is automatically added to the real-time streaming pipeline. However, feature values can also be retrieved on ad-hoc data using the retrieval API. Here’s an example  \\n```features = tb.retrieve_features(\"transactions\", query_df)\\n```  \\n## ML Modelling - Basic concepts [Permalink for this section](https://docs.turboml.com/intro/\\\\#ml-modelling---basic-concepts)  \\n### Inputs and Labels [Permalink for this section](https://docs.turboml.com/intro/\\\\#inputs-and-labels)  \\nFor each model, we need to specify the Inputs and the Labels.  \\n### Types of fields [Permalink for this section](https://docs.turboml.com/intro/\\\\#types-of-fields)  \\nDifferent models can accept different types of input fields. The supported types of fields are, numeric, categoric, time series, text, and image.  \\n### TurboML algorithms [Permalink for this section](https://docs.turboml.com/intro/\\\\#turboml-algorithms)  \\nTurboML provides several algorithms out of the box. These algorithms are optimized for online predictions and learning, and have been tested on real-world settings.  \\n```model = tb.HoeffdingTreeClassifier(n_classes=2)\\n```  \\n### Pytorch/TensorFlow/Scikit-learn [Permalink for this section](https://docs.turboml.com/intro/\\\\#pytorchtensorflowscikit-learn)  \\nWe use ONNX to deploy trained models from [Pytorch](https://docs.turboml.com/byo_models/onnx_pytorch/), [TensorFlow](https://docs.turboml.com/byo_models/onnx_tensorflow/), [Scikit-learn](https://docs.turboml.com/byo_models/onnx_sklearn/) or other ONNX compatible frameworks. Example for these three frameworks can be found in the following notebooks.  \\nNote: These models are static, and are not updated automatically.  \\n### Python [Permalink for this section](https://docs.turboml.com/intro/\\\\#python)  \\nTurboML also supports writing arbitrary Python code to define your own algorithms, including any libraries. To add your own algorithms, you need to define a Python class with 2 methods defined with the following signature:  \\n```class Model:\\ndef learn_one(self, features, label):\\npass\\n\\ndef predict_one(self, features, output_data):\\npass\\n```  \\nExamples of using an incremental learning algorithm, as well as a batch-like algorithm, can be found [here](https://docs.turboml.com/wyo_models/native_python_model/) from the river library.  \\n### Combining models [Permalink for this section](https://docs.turboml.com/intro/\\\\#combining-models)  \\nModels can also be combined to create other models, e.g. ensembles. An example of an ensemble model is as follows  \\n```model = tb.LeveragingBaggingClassifier(n_classes=2, base_model = tb.HoeffdingTreeClassifier(n_classes=2))\\n```  \\nPreprocessors can also be chained and applied in a similar manner. E.g.  \\n```model = tb.MinMaxPreProcessor(base_model = model)\\n```  \\n## Model Training [Permalink for this section](https://docs.turboml.com/intro/\\\\#model-training)  \\nOnce we’ve defined a model, it can be trained in different ways.  \\n### Batch way [Permalink for this section](https://docs.turboml.com/intro/\\\\#batch-way)  \\nThe simplest way is to train the model in a batch way. This is similar to sklearn’s fit() method. However, internally the training is performed in an incremental manner. So, you can update an already trained model on some new data too. Here’s an example  \\n```old_trained_model = model.learn(old_features, old_label)\\nnew_trained_model = old_trained_model.learn(new_features, new_label)\\n```  \\nAny trained copy of the model can be then deployed to production.  \\n```deployed_model = new_trained_model.deploy(name = \"deployment_name\", input=features, labels=label, predict_only=True)\\n```  \\nSince this is a trained model, we can also invoke this model in a batch way to get predictions without actually deploying the mode.  \\n```outputs = new_trained_model.predict(query_features)\\n```  \\n### Streaming way [Permalink for this section](https://docs.turboml.com/intro/\\\\#streaming-way)  \\nThis is where the model, after deployment, is continuously trained on new data. The user can choose how to update the model. The choices are online updates (where the model is updated on every new datapoint), or trigger-based updates which can be volume-based, time-based, performance-based or drift-based. The default option is online updates.  \\n```deployed_model = model.deploy(name = \"deployment_name\", input=features, labels=label)\\n```  \\n## Deployment and MLOps [Permalink for this section](https://docs.turboml.com/intro/\\\\#deployment-and-mlops)  \\n### Inference [Permalink for this section](https://docs.turboml.com/intro/\\\\#inference)  \\nOnce you’ve deployed a mode, there are several different ways to perform inference.  \\n#### Async [Permalink for this section](https://docs.turboml.com/intro/\\\\#async)  \\nThe first one is the async method. The data that is streamed from the input source is continuously fed to the model, and the outputs are streamed to another source. This stream can be either be subscribed to directly be the end user application, or sinked to a database or other data sources.  \\n```outputs = deployed_model.get_outputs()\\n```  \\n#### API [Permalink for this section](https://docs.turboml.com/intro/\\\\#api)  \\nA request-response model is used for inference on a single data point synchronously. The `/model_name/predict` endpoint is exposed for each deployed model where a REST API call can be made to get the outputs.  \\n#### Batch [Permalink for this section](https://docs.turboml.com/intro/\\\\#batch)  \\nWhen you have multiple records you’d like to perform inference on, you can use the get\\\\_inference method as follows.  \\n```outputs = deployed_model.get_inference(query_df)\\n```  \\n### Evaluation [Permalink for this section](https://docs.turboml.com/intro/\\\\#evaluation)  \\nTurboML provides standard ML metrics out of the box to perform model evaluation. Multiple metrics can be registered for any deployed model. The metrics pipeline re-uses the labels extracted for model training.  \\n```deployed_model.add_metric(\"WindowedAUC\")\\nmodel_auc_scores = deployed_model.get_evaluation(\"WindowedAUC\")\\n```  \\nLast updated on January 24, 2025  \\n[Quickstart](https://docs.turboml.com/quickstart/ \"Quickstart\")  \\nWhat is TurboML? @ TurboML\\n</page_content>\\n\\n# TurboML Quickstart\\n@ TurboML - page_link: https://docs.turboml.com/quickstart/\\n<page_content>\\nQuickstart  \\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/quickstart.ipynb)  \\n```import turboml as tb\\n```  \\n## Inspecting Data [Permalink for this section](https://docs.turboml.com/quickstart/\\\\#inspecting-data)  \\nTurboML is built for real-time machine learning, and as such, deals with streams of data. This can be achieved by using connectors to continuously pull data from your data source (like S3 or postgres), or use push-based approaches using REST API or Client SDKs.  \\nFor the purpose of this tutorial, we can use simulate real-time data generation, with a batch-like setting using pandas dataframes. Let\\'s first load some pandas dataframes. In this example, we\\'re using a credit card fraud detection dataset.  \\n```transactions_df = tb.datasets.FraudDetectionDatasetFeatures().df\\nlabels_df = tb.datasets.FraudDetectionDatasetLabels().df\\n```  \\n```transactions_df\\n```  \\n```labels_df\\n```  \\nOur dataset has 201406 datapoints, each with a corresponding label. Since we don\\'t have a natural primary key in the dataset that can uniquely identify each row, we\\'ll use the inbuilt index that pandas provides.  \\n```transactions_df.head()\\n```  \\n```labels_df.head()\\n```  \\n## Data Ingestion [Permalink for this section](https://docs.turboml.com/quickstart/\\\\#data-ingestion)  \\nWe can now upload these dataframes to the TurboML platform, the **OnlineDataset** class can be used here. It takes in the dataframe, the primary key, and the name of the dataset that is to be created for the given dataframe as input.  \\n```# Attempt to create and upload dataset\\ntransactions = tb.OnlineDataset.from_pd(\\nid=\"qs_transactions\",\\ndf=transactions_df,\\nkey_field=\"transactionID\",\\nload_if_exists=True,\\n)\\nlabels = tb.OnlineDataset.from_pd(\\nid=\"qs_transaction_labels\",\\ndf=labels_df,\\nkey_field=\"transactionID\",\\nload_if_exists=True,\\n)\\n```  \\n## Feature Engineering [Permalink for this section](https://docs.turboml.com/quickstart/\\\\#feature-engineering)  \\nTurboML platform facilitates transformations on raw data to produce new features. You can use the jupyter notebook as a \"playground\" to explore different features. This involves 3 steps.  \\n- **fetch data**: Experimentation is easier on static data. Since TurboML works with continuous data streams, to enable experimentation we fetch a snapshot or a subset of data in the jupyter notebook.\\n- **add feature definitions**: Now that we have a static dataset, we can define multiple different features, and see their values on this dataset. Since we can observe their values, we can perform simple experiments and validations like correlations, plots and other exploratory analysis.\\n- **submit feature definitions**: Once we\\'re confident about the features we\\'ve defined, we can now submit the ones we want TurboML to compute continuously for the actual data stream.  \\n### Fetch data [Permalink for this section](https://docs.turboml.com/quickstart/\\\\#fetch-data)  \\nWe can use the **get\\\\_features** function to get a snapshot or subset of the data stream.  \\n**Note**: This size of the dataset returned by this function can change on each invocation. Also, the dataset is not guaranteed to be in the same order.  \\n### Add feature definitions [Permalink for this section](https://docs.turboml.com/quickstart/\\\\#add-feature-definitions)  \\nTo add feature definitions, we have a class from turboml package called **FeatureEngineering**. This allows us to define SQL-based and dynamic aggregation-based features.  \\nThe following cell shows how to define an SQL-based feature. The sql\\\\_definition parameter in the **create\\\\_sql\\\\_features** function takes in the SQL expression to be used to prepare the feature. It returns a dataframe with all the original columns, and another column which, on a high-level is defined as `SELECT sql_definition AS new_feature_name FROM dataframe`.  \\n```transactions.feature_engineering.create_sql_features(\\nsql_definition=\\'\"transactionAmount\" + \"localHour\"\\',\\nnew_feature_name=\"my_sql_feat\",\\n)\\n```  \\n```transactions.feature_engineering.get_local_features()\\n```  \\n```tb.get_timestamp_formats()\\n```  \\n```transactions.feature_engineering.register_timestamp(\\ncolumn_name=\"timestamp\", format_type=\"epoch_seconds\"\\n)\\n```  \\nThe following cell shows how to define an aggregation-based feature using the **create\\\\_aggregate\\\\_features** function. It returns a dataframe with all the original columns, and another column which, on a high-level is defined as `SELECT operation(column_to_operate) OVER (PARTITION BY column_to_group ORDER BY time_column RANGE BETWEEN INTERVAL window_duration PRECEDING AND CURRENT ROW) as new_feature_name from dataframe`.  \\n```transactions.feature_engineering.create_aggregate_features(\\ncolumn_to_operate=\"transactionAmount\",\\ncolumn_to_group=\"accountID\",\\noperation=\"SUM\",\\nnew_feature_name=\"my_sum_feat\",\\ntimestamp_column=\"timestamp\",\\nwindow_duration=24,\\nwindow_unit=\"hours\",\\n)\\n```  \\n```transactions.feature_engineering.get_local_features()\\n```  \\n### Submit feature definitions [Permalink for this section](https://docs.turboml.com/quickstart/\\\\#submit-feature-definitions)  \\nNow that we\\'ve seen the newly created features, and everything looks good, we can submit these feature definitions to the TurboML platform so that this can be computed continously for the input data stream.  \\nWe need to tell the platform to start computations for all pending features for the given dataset. This can be done by calling the **materialize\\\\_features** function.  \\n```transactions.feature_engineering.materialize_features([\"my_sql_feat\", \"my_sum_feat\"])\\n```  \\n```df_transactions = transactions.feature_engineering.get_materialized_features()\\ndf_transactions\\n```  \\n## Machine Learning Modelling [Permalink for this section](https://docs.turboml.com/quickstart/\\\\#machine-learning-modelling)  \\nTurboML provides out of the box algorithms, optimized for real-time ML, and supports bringing your own models and algorithms as well. In this tutorial, we\\'ll use the algorithms provided by TurboML.  \\n### Check the available algorithms [Permalink for this section](https://docs.turboml.com/quickstart/\\\\#check-the-available-algorithms)  \\nYou can check what are the available ML algorithms based on `tb.ml_algorithms(have_labels=True/False)` depending on supervised or unsupervised learning.  \\n```tb.ml_algorithms(have_labels=False)\\n```  \\nLet\\'s use the RandomCutForest (RCF) algorithm.  \\n### Create model [Permalink for this section](https://docs.turboml.com/quickstart/\\\\#create-model)  \\nNow that we\\'ve chosen an algorithm, we need to create a model.  \\n```model = tb.RCF(number_of_trees=50)\\n```  \\n### Run Streaming ML jobs [Permalink for this section](https://docs.turboml.com/quickstart/\\\\#run-streaming-ml-jobs)  \\nNow that we\\'ve instantiated the model, we can deploy it using the **deploy** function.\\nFor an unsupervised ML job, we need to provide a dataset from which the model can consume inputs. For each record in this dataset, the model will make a prediction, produce the prediction to an output dataset, and then perform unsupervised updates using this record.  \\nThere are four types of fields that can be used by any ML algorithm:  \\n- numerical\\\\_fields: This represents fields that we want our algorithm to treat as real-valued fields.\\n- categorical\\\\_fields: This represents fields that we want our algorithm to treat as categorical fields.\\n- time\\\\_field: This is used for time-series applications to capture the timestamp field.\\n- textual\\\\_fields: This represents fields that we want our algorithm to treat as text fields.  \\nThe input values from any of these fields are suitably converted to the desired type. String values are converted using the hashing trick.  \\nLet\\'s construct a model config using the following numerical fields, no categorical or time fields.  \\n```numerical_fields = [\\\\\\n\"transactionAmount\",\\\\\\n\"localHour\",\\\\\\n\"my_sum_feat\",\\\\\\n\"my_sql_feat\",\\\\\\n]\\nfeatures = transactions.get_model_inputs(numerical_fields=numerical_fields)\\nlabel = labels.get_model_labels(label_field=\"is_fraud\")\\n```  \\n```deployed_model_rcf = model.deploy(name=\"demo_model_rcf\", input=features, labels=label)\\n```  \\n### Inspect model outputs [Permalink for this section](https://docs.turboml.com/quickstart/\\\\#inspect-model-outputs)  \\nWe can now fetch the outputs that the model produced by calling the **get\\\\_outputs** function.  \\n**Note**: This size of the outputs returned by this function can change on each invocation, since the model is continuosly producing outputs.  \\n```outputs = deployed_model_rcf.get_outputs()\\n```  \\n```len(outputs)\\n```  \\n```sample_output = outputs[-1]\\nsample_output\\n```  \\nThe above output corresponds to an input with the key, or index, sample\\\\_output.key. Along with the anomaly score, the output also contains attributions to different features. We can see that the first numerical feature, i.e. \\'transactionAmount\\' is around sample\\\\_output.feature\\\\_score\\\\[0\\\\]\\\\*100% responsible for the anomaly score  \\n```import matplotlib.pyplot as plt\\n\\nplt.plot([output[\"record\"].score for output in outputs])\\n```  \\n### Model Endpoints [Permalink for this section](https://docs.turboml.com/quickstart/\\\\#model-endpoints)  \\nThe above method of interacting with the model was asynchronous. We were adding our datapoints to an input dataset, and getting the corresponding model outputs in an output dataset. In some scenarios, we need a synchronous method to query the model. This is where we can use the model endpoints that TurboML exposes.  \\n```model_endpoints = deployed_model_rcf.get_endpoints()\\nmodel_endpoints\\n```  \\nNow that we know what endpoint to send the request to, we now need to figure out the right format. Let\\'s try to make a prediction on the last row from our input dataset.  \\n```model_query_datapoint = transactions_df.iloc[-1].to_dict()\\nmodel_query_datapoint\\n```  \\n```import requests\\n\\nresp = requests.post(\\nmodel_endpoints[0], json=model_query_datapoint, headers=tb.common.api.headers\\n)\\n```  \\n```resp.json()\\n```  \\n### Batch Inference on Models [Permalink for this section](https://docs.turboml.com/quickstart/\\\\#batch-inference-on-models)  \\nWhile the above method is more suited for individual requests, we can also perform batch inference on the models. We use the **get\\\\_inference** function for this purpose.  \\n```outputs = deployed_model_rcf.get_inference(transactions_df)\\noutputs\\n```  \\n## Model Evaluation [Permalink for this section](https://docs.turboml.com/quickstart/\\\\#model-evaluation)  \\nSimilar to ML models, TurboML provides in-built metrics, and supports defining your own metrics. Let\\'s see the available metrics.  \\n```tb.evaluation_metrics()\\n```  \\nWe can select the AreaUnderCurve (AUC) metric to evaluate our anomaly detection model. The windowed prefix means we\\'re evaluating these metrics over a rolling window. By default, the window size is `1000`.  \\n```deployed_model_rcf.add_metric(\"WindowedAUC\")\\n```  \\nSimilar to steps like feature engineering and ML modelling, model evaluation is also a continuosly running job. We can look at the snapshot of the model metrics at any given instance by using the **get\\\\_evaluation** function.  \\n**Note**: This size of the outputs returned by this function can change on each invocation, since we\\'re continuously evaluating the model.  \\n```model_auc_scores = deployed_model_rcf.get_evaluation(\"WindowedAUC\")\\nmodel_auc_scores[-1]\\n```  \\n```import matplotlib.pyplot as plt\\n\\nplt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])\\n```  \\n### Model Evaluation with filter and custom window size [Permalink for this section](https://docs.turboml.com/quickstart/\\\\#model-evaluation-with-filter-and-custom-window-size)  \\nWe support running evaluation on filtered model data using valid SQL expression along with custom window size.  \\n```model_auc_scores = deployed_model_rcf.get_evaluation(\\n\"WindowedAUC\",\\nfilter_expression=\"input_data.transactionCurrencyCode != \\'USD\\' AND output_data.score > 0.6\",\\nwindow_size=200,\\n)\\nmodel_auc_scores[-1]\\n```  \\n```import matplotlib.pyplot as plt\\n\\nplt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])\\n```  \\n## Supervised Learning [Permalink for this section](https://docs.turboml.com/quickstart/\\\\#supervised-learning)  \\nLet\\'s now take an example with a supervised learning algorithm. First, let\\'s see what algorithms are supported out of the box.  \\n```tb.ml_algorithms(have_labels=True)\\n```  \\nWe can use HoeffdingTreeClassifier to try to classify fraudulent and normal activity on the same dataset. First, we need to instantiate a model.  \\n```htc_model = tb.HoeffdingTreeClassifier(n_classes=2)\\n```  \\nWe can use the same numerical fields in this model as well. However, let\\'s add some categorical fields as well.  \\n```categorical_fields = [\\\\\\n\"digitalItemCount\",\\\\\\n\"physicalItemCount\",\\\\\\n\"isProxyIP\",\\\\\\n]\\nfeatures = transactions.get_model_inputs(\\nnumerical_fields=numerical_fields, categorical_fields=categorical_fields\\n)\\nlabel = labels.get_model_labels(label_field=\"is_fraud\")\\n```  \\n### Run Supervised ML jobs [Permalink for this section](https://docs.turboml.com/quickstart/\\\\#run-supervised-ml-jobs)  \\nSame as before, we can deploy this model with the **deploy** function.  \\n```deployed_model_htc = htc_model.deploy(\"demo_classifier\", input=features, labels=label)\\n```  \\nWe can now inspect the outputs.  \\n```outputs = deployed_model_htc.get_outputs()\\n```  \\n```len(outputs)\\n```  \\n```sample_output = outputs[-1]\\nsample_output\\n```  \\nWe notice that since this is a classification model, we have some new attributes in the output, specifically `class_probabilities` and `predicted_class`. We also have the `score` attribute which, for classification, just shows us the probability for the last class.  \\n### Supervised Model Endpoints [Permalink for this section](https://docs.turboml.com/quickstart/\\\\#supervised-model-endpoints)  \\nPredict API for supervised models is exactly the same as unsupervised models.  \\n```model_endpoints = deployed_model_htc.get_endpoints()\\nmodel_endpoints\\n```  \\n```resp = requests.post(\\nmodel_endpoints[0], json=model_query_datapoint, headers=tb.common.api.headers\\n)\\nresp.json()\\n```  \\n### Supervised Model Evaluation [Permalink for this section](https://docs.turboml.com/quickstart/\\\\#supervised-model-evaluation)  \\nLet\\'s now evaluate our supervised ML model. The process is exactly the same as for unsupervised model evaluation.  \\n```deployed_model_htc.add_metric(\"WindowedAUC\")\\n```  \\nWe can use the same **get\\\\_evaluation** function to fetch the metrics for this model as well. Remember, this function retrieves the metric values present at that moment of time. So, if the number of records recieved seem low, just re-run this function.  \\n```model_auc_scores = deployed_model_htc.get_evaluation(\"WindowedAUC\")\\nmodel_auc_scores[-1]\\n```  \\n```plt.plot([model_auc_score.metric for model_auc_score in model_auc_scores])\\n```  \\n## Model Comparison [Permalink for this section](https://docs.turboml.com/quickstart/\\\\#model-comparison)  \\nNow that we have 2 models deployed, and we\\'ve registered metrics for both of them, we can compare them on real-time data. On each invocation, the following function will fetch the latest evaluations of the models and plot them.  \\n```tb.compare_model_metrics(\\nmodels=[deployed_model_rcf, deployed_model_htc], metric=\"WindowedAUC\"\\n)\\n```  \\n## Model Deletion [Permalink for this section](https://docs.turboml.com/quickstart/\\\\#model-deletion)  \\nWe can delete the models like this, by default the generated output is deleted. If you want to retain the output generated by model, use `delete_output_topic=False`.  \\n```deployed_model_rcf.delete()\\n```  \\nLast updated on January 24, 2025  \\n[Introduction](https://docs.turboml.com/intro/ \"Introduction\") [Batch API](https://docs.turboml.com/general_examples/batch_api/ \"Batch API\")\\n</page_content>\\n\\n# Batch APIs\\n@ TurboML - page_link: https://docs.turboml.com/general_examples/batch_api/\\n<page_content>\\nGeneral  \\nBatch API  \\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg) (opens in a new tab)](https://colab.research.google.com/github/TurboML-Inc/colab-notebooks/blob/main/batch_api.ipynb)  \\nThe main mode of operation in TurboML is streaming, with continuous updates to different components with fresh data. However, TurboML also supports the good ol\\' fashioned batch APIs. We\\'ve already seen examples of this for feature engineering in the quickstart notebook. In this notebook, we\\'ll focus primarily on batch APIs for ML modelling.  \\nTo make this more interesting, we\\'ll show how we can still have incremental training on batch data.  \\n```import turboml as tb\\n```  \\n```import pandas as pd\\nfrom sklearn import metrics\\n```  \\n## Dataset [Permalink for this section](https://docs.turboml.com/general_examples/batch_api/\\\\#dataset)  \\nWe\\'ll use our standard `FraudDetection` dataset again, but this time without pushing it to the platform. Interfaces like feature engineering and feature selection work in the exact same ways, just without being linked\\nto a platform-managed dataset.  \\n```transactions = tb.datasets.FraudDetectionDatasetFeatures()\\nlabels = tb.datasets.FraudDetectionDatasetLabels()\\n\\ntransactions_p1 = transactions[:100000]\\nlabels_p1 = labels[:100000]\\n\\ntransactions_p2 = transactions[100000:]\\nlabels_p2 = labels[100000:]\\n```  \\n```numerical_fields = [\\\\\\n\"transactionAmount\",\\\\\\n\"localHour\",\\\\\\n]\\ncategorical_fields = [\\\\\\n\"digitalItemCount\",\\\\\\n\"physicalItemCount\",\\\\\\n\"isProxyIP\",\\\\\\n]\\nfeatures = transactions_p1.get_model_inputs(\\nnumerical_fields=numerical_fields, categorical_fields=categorical_fields\\n)\\nlabel = labels_p1.get_model_labels(label_field=\"is_fraud\")\\n```  \\n## Training [Permalink for this section](https://docs.turboml.com/general_examples/batch_api/\\\\#training)  \\nWith the features and label defined, we can train a model in a batch way using the learn method.  \\n```model = tb.HoeffdingTreeClassifier(n_classes=2)\\n```  \\n```model_trained_100K = model.learn(features, label)\\n```  \\nWe\\'ve trained a model on the first 100K rows. Now, to update this model on the remaining data, we can create another batch dataset and call the `learn` method. Note that this time, learn is called on a trained model.  \\n```features = transactions_p2.get_model_inputs(\\nnumerical_fields=numerical_fields, categorical_fields=categorical_fields\\n)\\nlabel = labels_p2.get_model_labels(label_field=\"is_fraud\")\\n```  \\n```model_fully_trained = model_trained_100K.learn(features, label)\\n```  \\n## Inference [Permalink for this section](https://docs.turboml.com/general_examples/batch_api/\\\\#inference)  \\nWe\\'ve seen batch inference on deployed models in the quickstart notebook. We can also perform batch inference on these models using the `predict` method.  \\n```outputs = model_trained_100K.predict(features)\\nprint(metrics.roc_auc_score(labels_p2.df[\"is_fraud\"], outputs[\"score\"]))\\noutputs = model_fully_trained.predict(features)\\nprint(metrics.roc_auc_score(labels_p2.df[\"is_fraud\"], outputs[\"score\"]))\\n```  \\n## Deployment [Permalink for this section](https://docs.turboml.com/general_examples/batch_api/\\\\#deployment)  \\nSo far, we\\'ve only trained a model. We haven\\'t deployed it yet. Deploying a batch trained model is exactly like any other model deployment, except we\\'ll set the `predict_only` option to be True. This means the model won\\'t be updated automatically.  \\n```transactions = transactions.to_online(id=\"transactions10\", load_if_exists=True)\\nlabels = labels.to_online(id=\"transaction_labels\", load_if_exists=True)\\n```  \\n```features = transactions.get_model_inputs(\\nnumerical_fields=numerical_fields, categorical_fields=categorical_fields\\n)\\nlabel = labels.get_model_labels(label_field=\"is_fraud\")\\n```  \\n```deployed_model = model_fully_trained.deploy(\\nname=\"predict_only_model\", input=features, labels=label, predict_only=True\\n)\\n```  \\n```outputs = deployed_model.get_outputs()\\noutputs[-1]\\n```  \\n## Next Steps [Permalink for this section](https://docs.turboml.com/general_examples/batch_api/\\\\#next-steps)  \\nIn this notebook, we discussed how to train models in a batch paradigm and deploy them. In a separate notebook we\\'ll cover two different statregies to update models, (i) starting from a batch trained model and using continual learning, (ii) training models incrementally in a batch paradigm and updating the deployment with newer versions.  \\nLast updated on January 24, 2025  \\n[Quickstart](https://docs.turboml.com/quickstart/ \"Quickstart\") [Local Model](https://docs.turboml.com/general_examples/local_model/ \"Local Model\")\\n</page_content>',\n",
       " 'base_chunk': 'What is TurboML?',\n",
       " 'context_sections': ['summary',\n",
       "  'What is TurboML?',\n",
       "  'TurboML Quickstart',\n",
       "  'Batch APIs'],\n",
       " 'generation_timestamp_ns': 1740785987488093300}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q huggingface-hub datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"your_api_token_here\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e613f721a1b4527ac034d3c0cb2165d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hf_turboml_dataset.json:   0%|          | 0.00/74.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/DebopamC/TurboML_Synthetic_QnA_Dataset/commit/a281fb6903acbcce27ca098640888c169238a38f', commit_message='Upload hf_turboml_dataset.json with huggingface_hub', commit_description='', oid='a281fb6903acbcce27ca098640888c169238a38f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/DebopamC/TurboML_Synthetic_QnA_Dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='DebopamC/TurboML_Synthetic_QnA_Dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Upload main dataset file\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"TurboML_datasets/hf_turboml_dataset.json\",\n",
    "    path_in_repo=\"hf_turboml_dataset.json\",\n",
    "    repo_id=\"DebopamC/TurboML_Synthetic_QnA_Dataset\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "# # Upload README\n",
    "# api.upload_file(\n",
    "#     path_or_fileobj=\"README.md\",\n",
    "#     path_in_repo=\"README.md\",\n",
    "#     repo_id=\"DebopamC/TurboML_Synthetic_QnA_Dataset\",\n",
    "#     repo_type=\"dataset\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
