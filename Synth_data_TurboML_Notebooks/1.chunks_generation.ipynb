{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cxJZ4hGFX54n"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACo2gwGSocVO"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-text-splitters"
      ],
      "metadata": {
        "id": "RKg1YbKoos7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain_core.documents import Document"
      ],
      "metadata": {
        "id": "LDm4qP8owccA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These are for my reference - To check if all the chunks are present\n",
        "headers = [\n",
        "    \"# What is TurboML?\",\n",
        "    \"# TurboML Quickstart\",\n",
        "    \"# String Encoding\",\n",
        "    \"# AMF Regressor\",\n",
        "    \"# Multinomial Naive Bayes\",\n",
        "    \"# PreProcessors\",\n",
        "    \"# Python Model: Batch Example\",\n",
        "    \"# Image Processing (MNIST Example)\",\n",
        "    \"# Stream Dataset to Deployed Models\",\n",
        "    \"# LeveragingBaggingClassifier\",\n",
        "    \"# TF-IDF embedding example using gRPC Client\",\n",
        "    \"# EmbeddingModel\",\n",
        "    \"# BanditModelSelection\",\n",
        "    \"# HeteroLeveragingBaggingClassifier\",\n",
        "    \"# FFM Regressor\",\n",
        "    \"# Hyperparameter Tuning\",\n",
        "    \"# Algorithm Tuning\",\n",
        "    \"# Drift Detection\",\n",
        "    \"# MSTREAM\",\n",
        "    \"# RandomProjectionEmbedding\",\n",
        "    \"# Gaussian Naive Bayes\",\n",
        "    \"# Feature Engineering - Python UDAF\",\n",
        "    \"# TurboML LLM Tutorial\",\n",
        "    \"# AMF Classifier\",\n",
        "    \"# Image Processing (MNIST Example)\",\n",
        "    \"# Feature Engineering - Python UDFs\",\n",
        "    \"# Native Python Models\",\n",
        "    \"# ONNX tutorial with Scikit-Learn\",\n",
        "    \"# Adaptive LightGBM\",\n",
        "    \"# Hoeffding Tree Regressor\",\n",
        "    \"# Python Model: PySAD Example\",\n",
        "    \"# Batch APIs\",\n",
        "    \"# ContextualBanditModelSelection\",\n",
        "    \"# ONNX tutorial with TensorFlow\",\n",
        "    \"# Local Model\",\n",
        "    \"# Neural Network\",\n",
        "    \"# OCR example using RestAPI Client\",\n",
        "    \"# HeteroAdaBoostClassifier\",\n",
        "    \"# ONNX\",\n",
        "    \"# Resnet example using gRPC Client\",\n",
        "    \"# OVR (OnevsRestClassifier)\",\n",
        "    \"# Hoeffding Tree Classifier\",\n",
        "    \"# AdaBoostClassifier\",\n",
        "    \"# Random Sampler\",\n",
        "    \"# FFM Classifier\",\n",
        "    \"# SNARIMAX\",\n",
        "    \"# Feature Engineering - Complex Stream Processing\",\n",
        "    \"# TurboML Ibis Quickstart\",\n",
        "    \"# Performance Improvements\",\n",
        "    \"# Online Neural Network\",\n",
        "    \"# Ensembling Custom Python Models in TurboML\",\n",
        "    \"# Custom Evaluation Metric\",\n",
        "    \"# Adaptive XGBoost\",\n",
        "    \"# Random Cut Forest\",\n",
        "    \"# LLM Embeddings\",\n",
        "    \"# __init__.py\",\n",
        "    \"# api.py\",\n",
        "    \"# concurrent.py\",\n",
        "    \"# dataloader.py\",\n",
        "    \"# datasets.py\",\n",
        "    \"# default_model_configs.py\",\n",
        "    \"# env.py\",\n",
        "    \"# feature_engineering.py\",\n",
        "    \"# internal.py\",\n",
        "    \"# llm.py\",\n",
        "    \"# ml_algs.py\",\n",
        "    \"# models.py\",\n",
        "    \"# model_comparison.py\",\n",
        "    \"# namespaces.py\",\n",
        "    \"# pymodel.py\",\n",
        "    \"# pytypes.py\",\n",
        "    \"# types.py\",\n",
        "    \"# udf.py\",\n",
        "    \"# util.py\",\n",
        "    \"# config_pb2.py\",\n",
        "    \"# config_pb2.pyi\",\n",
        "    \"# flink_pb2.py\",\n",
        "    \"# flink_pb2.pyi\",\n",
        "    \"# flink_pb2_grpc.py\",\n",
        "    \"# input_pb2.py\",\n",
        "    \"# input_pb2.pyi\",\n",
        "    \"# metrics_pb2.py\",\n",
        "    \"# metrics_pb2.pyi\",\n",
        "    \"# ml_service_pb2.py\",\n",
        "    \"# ml_service_pb2_grpc.py\",\n",
        "    \"# output_pb2.py\",\n",
        "    \"# output_pb2.pyi\",\n",
        "    \"# sources_pb2.py\",\n",
        "    \"# sources.p2p.py\",\n",
        "]"
      ],
      "metadata": {
        "id": "zXgdkL9Toybs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
        "\n",
        "# Define the headers to split on (using level 1 headers)\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"section\")  # This matches all top-level headers (# Header)\n",
        "]\n",
        "\n",
        "# Create the markdown splitter\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(\n",
        "    headers_to_split_on=headers_to_split_on\n",
        ")\n",
        "\n",
        "# Load your markdown document\n",
        "with open(\"/content/knowledge_base.md\", \"r\") as f:\n",
        "    markdown_document = f.read()\n",
        "\n",
        "# Split the document\n",
        "chunks = markdown_splitter.split_text(markdown_document)\n",
        "\n",
        "# Each chunk contains the content and metadata with the header\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk {i+1}:\")\n",
        "    print(f\"  Section: {chunk.metadata.get('section')}\")\n",
        "    print(f\"  Content length: {len(chunk.page_content)} characters\")\n",
        "    print(\"-\" * 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jVe2HUHpRN3",
        "outputId": "a6521112-ca2e-4dd5-daba-8296096d519b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1:\n",
            "  Section: What is TurboML?\n",
            "  Content length: 10790 characters\n",
            "---\n",
            "Chunk 2:\n",
            "  Section: TurboML Quickstart\n",
            "  Content length: 15877 characters\n",
            "---\n",
            "Chunk 3:\n",
            "  Section: String Encoding\n",
            "  Content length: 2842 characters\n",
            "---\n",
            "Chunk 4:\n",
            "  Section: AMF Regressor\n",
            "  Content length: 2339 characters\n",
            "---\n",
            "Chunk 5:\n",
            "  Section: MultinomialNB\n",
            "  Content length: 1199 characters\n",
            "---\n",
            "Chunk 6:\n",
            "  Section: PreProcessors\n",
            "  Content length: 9334 characters\n",
            "---\n",
            "Chunk 7:\n",
            "  Section: Python Model: Batch Example\n",
            "  Content length: 3565 characters\n",
            "---\n",
            "Chunk 8:\n",
            "  Section: Image Processing (MNIST Example)\n",
            "  Content length: 4476 characters\n",
            "---\n",
            "Chunk 9:\n",
            "  Section: Stream Dataset to Deployed Models\n",
            "  Content length: 15743 characters\n",
            "---\n",
            "Chunk 10:\n",
            "  Section: LeveragingBaggingClassifier\n",
            "  Content length: 2459 characters\n",
            "---\n",
            "Chunk 11:\n",
            "  Section: TF-IDF embedding example using gRPC Client\n",
            "  Content length: 3298 characters\n",
            "---\n",
            "Chunk 12:\n",
            "  Section: EmbeddingModel\n",
            "  Content length: 1493 characters\n",
            "---\n",
            "Chunk 13:\n",
            "  Section: BanditModelSelection\n",
            "  Content length: 1649 characters\n",
            "---\n",
            "Chunk 14:\n",
            "  Section: HeteroLeveragingBaggingClassifier\n",
            "  Content length: 1898 characters\n",
            "---\n",
            "Chunk 15:\n",
            "  Section: FFM Regressor\n",
            "  Content length: 2657 characters\n",
            "---\n",
            "Chunk 16:\n",
            "  Section: Hyperparameter Tuning\n",
            "  Content length: 3579 characters\n",
            "---\n",
            "Chunk 17:\n",
            "  Section: Algorithm Tuning\n",
            "  Content length: 3270 characters\n",
            "---\n",
            "Chunk 18:\n",
            "  Section: Drift Detection\n",
            "  Content length: 2700 characters\n",
            "---\n",
            "Chunk 19:\n",
            "  Section: MSTREAM\n",
            "  Content length: 1812 characters\n",
            "---\n",
            "Chunk 20:\n",
            "  Section: RandomProjectionEmbedding\n",
            "  Content length: 1757 characters\n",
            "---\n",
            "Chunk 21:\n",
            "  Section: Gaussian Naive Bayes\n",
            "  Content length: 1081 characters\n",
            "---\n",
            "Chunk 22:\n",
            "  Section: Feature Engineering - Python UDAF\n",
            "  Content length: 4158 characters\n",
            "---\n",
            "Chunk 23:\n",
            "  Section: TurboML LLM Tutorial\n",
            "  Content length: 2943 characters\n",
            "---\n",
            "Chunk 24:\n",
            "  Section: AMF Classifier\n",
            "  Content length: 3555 characters\n",
            "---\n",
            "Chunk 25:\n",
            "  Section: Image Processing (MNIST Example)\n",
            "  Content length: 5452 characters\n",
            "---\n",
            "Chunk 26:\n",
            "  Section: Feature Engineering - Python UDFs\n",
            "  Content length: 4392 characters\n",
            "---\n",
            "Chunk 27:\n",
            "  Section: Native Python Models\n",
            "  Content length: 9176 characters\n",
            "---\n",
            "Chunk 28:\n",
            "  Section: ONNX tutorial with Scikit-Learn\n",
            "  Content length: 3603 characters\n",
            "---\n",
            "Chunk 29:\n",
            "  Section: Adaptive LightGBM\n",
            "  Content length: 2101 characters\n",
            "---\n",
            "Chunk 30:\n",
            "  Section: HoeffdingTreeRegressor\n",
            "  Content length: 1869 characters\n",
            "---\n",
            "Chunk 31:\n",
            "  Section: Python Model: PySAD Example\n",
            "  Content length: 3622 characters\n",
            "---\n",
            "Chunk 32:\n",
            "  Section: Batch APIs\n",
            "  Content length: 4650 characters\n",
            "---\n",
            "Chunk 33:\n",
            "  Section: ContextualBanditModelSelection\n",
            "  Content length: 1537 characters\n",
            "---\n",
            "Chunk 34:\n",
            "  Section: ONNX tutorial with TensorFlow\n",
            "  Content length: 3920 characters\n",
            "---\n",
            "Chunk 35:\n",
            "  Section: Local Model\n",
            "  Content length: 13190 characters\n",
            "---\n",
            "Chunk 36:\n",
            "  Section: Neural Network\n",
            "  Content length: 2391 characters\n",
            "---\n",
            "Chunk 37:\n",
            "  Section: OCR example using RestAPI Client\n",
            "  Content length: 4471 characters\n",
            "---\n",
            "Chunk 38:\n",
            "  Section: HeteroAdaBoostClassifier\n",
            "  Content length: 1292 characters\n",
            "---\n",
            "Chunk 39:\n",
            "  Section: ONNX\n",
            "  Content length: 1119 characters\n",
            "---\n",
            "Chunk 40:\n",
            "  Section: Resnet example using gRPC Client\n",
            "  Content length: 3899 characters\n",
            "---\n",
            "Chunk 41:\n",
            "  Section: OVR (OnevsRestClassifier)\n",
            "  Content length: 1169 characters\n",
            "---\n",
            "Chunk 42:\n",
            "  Section: Hoeffding Tree Classifier\n",
            "  Content length: 3339 characters\n",
            "---\n",
            "Chunk 43:\n",
            "  Section: AdaBoostClassifier\n",
            "  Content length: 2017 characters\n",
            "---\n",
            "Chunk 44:\n",
            "  Section: Random Sampler\n",
            "  Content length: 1596 characters\n",
            "---\n",
            "Chunk 45:\n",
            "  Section: FFM Classifier\n",
            "  Content length: 2603 characters\n",
            "---\n",
            "Chunk 46:\n",
            "  Section: SNARIMAX\n",
            "  Content length: 3300 characters\n",
            "---\n",
            "Chunk 47:\n",
            "  Section: LLM Embeddings\n",
            "  Content length: 4300 characters\n",
            "---\n",
            "Chunk 48:\n",
            "  Section: LLAMA Embedding\n",
            "  Content length: 1268 characters\n",
            "---\n",
            "Chunk 49:\n",
            "  Section: Model Explanations using iXAI\n",
            "  Content length: 5059 characters\n",
            "---\n",
            "Chunk 50:\n",
            "  Section: ONNX tutorial with PyTorch\n",
            "  Content length: 5427 characters\n",
            "---\n",
            "Chunk 51:\n",
            "  Section: Half-Space Trees (HST)\n",
            "  Content length: 1343 characters\n",
            "---\n",
            "Chunk 52:\n",
            "  Section: SGT Regressor\n",
            "  Content length: 3140 characters\n",
            "---\n",
            "Chunk 53:\n",
            "  Section: Ensembling Custom Python Models in TurboML\n",
            "  Content length: 6364 characters\n",
            "---\n",
            "Chunk 54:\n",
            "  Section: Custom Evaluation Metric\n",
            "  Content length: 5552 characters\n",
            "---\n",
            "Chunk 55:\n",
            "  Section: Adaptive XGBoost\n",
            "  Content length: 2203 characters\n",
            "---\n",
            "Chunk 56:\n",
            "  Section: Random Cut Forest\n",
            "  Content length: 1337 characters\n",
            "---\n",
            "Chunk 57:\n",
            "  Section: Feature Engineering - Complex Stream Processing\n",
            "  Content length: 10359 characters\n",
            "---\n",
            "Chunk 58:\n",
            "  Section: TurboML Ibis Quickstart\n",
            "  Content length: 3544 characters\n",
            "---\n",
            "Chunk 59:\n",
            "  Section: Performance Improvements\n",
            "  Content length: 5291 characters\n",
            "---\n",
            "Chunk 60:\n",
            "  Section: Online Neural Network\n",
            "  Content length: 1788 characters\n",
            "---\n",
            "Chunk 61:\n",
            "  Section: __init__.py\n",
            "  Content length: 15539 characters\n",
            "---\n",
            "Chunk 62:\n",
            "  Section: api.py\n",
            "  Content length: 5819 characters\n",
            "---\n",
            "Chunk 63:\n",
            "  Section: concurrent.py\n",
            "  Content length: 526 characters\n",
            "---\n",
            "Chunk 64:\n",
            "  Section: dataloader.py\n",
            "  Content length: 12463 characters\n",
            "---\n",
            "Chunk 65:\n",
            "  Section: datasets.py\n",
            "  Content length: 22927 characters\n",
            "---\n",
            "Chunk 66:\n",
            "  Section: default_model_configs.py\n",
            "  Content length: 7062 characters\n",
            "---\n",
            "Chunk 67:\n",
            "  Section: env.py\n",
            "  Content length: 874 characters\n",
            "---\n",
            "Chunk 68:\n",
            "  Section: feature_engineering.py\n",
            "  Content length: 44393 characters\n",
            "---\n",
            "Chunk 69:\n",
            "  Section: internal.py\n",
            "  Content length: 7007 characters\n",
            "---\n",
            "Chunk 70:\n",
            "  Section: llm.py\n",
            "  Content length: 2511 characters\n",
            "---\n",
            "Chunk 71:\n",
            "  Section: ml_algs.py\n",
            "  Content length: 55070 characters\n",
            "---\n",
            "Chunk 72:\n",
            "  Section: models.py\n",
            "  Content length: 25804 characters\n",
            "---\n",
            "Chunk 73:\n",
            "  Section: model_comparison.py\n",
            "  Content length: 1411 characters\n",
            "---\n",
            "Chunk 74:\n",
            "  Section: namespaces.py\n",
            "  Content length: 1157 characters\n",
            "---\n",
            "Chunk 75:\n",
            "  Section: pymodel.py\n",
            "  Content length: 99 characters\n",
            "---\n",
            "Chunk 76:\n",
            "  Section: pytypes.py\n",
            "  Content length: 99 characters\n",
            "---\n",
            "Chunk 77:\n",
            "  Section: types.py\n",
            "  Content length: 591 characters\n",
            "---\n",
            "Chunk 78:\n",
            "  Section: udf.py\n",
            "  Content length: 1720 characters\n",
            "---\n",
            "Chunk 79:\n",
            "  Section: util.py\n",
            "  Content length: 4766 characters\n",
            "---\n",
            "Chunk 80:\n",
            "  Section: config_pb2.py\n",
            "  Content length: 56247 characters\n",
            "---\n",
            "Chunk 81:\n",
            "  Section: flink_pb2.py\n",
            "  Content length: 15496 characters\n",
            "---\n",
            "Chunk 82:\n",
            "  Section: flink_pb2_grpc.py\n",
            "  Content length: 4053 characters\n",
            "---\n",
            "Chunk 83:\n",
            "  Section: input_pb2.py\n",
            "  Content length: 2502 characters\n",
            "---\n",
            "Chunk 84:\n",
            "  Section: metrics_pb2.py\n",
            "  Content length: 1593 characters\n",
            "---\n",
            "Chunk 85:\n",
            "  Section: ml_service_pb2.py\n",
            "  Content length: 1727 characters\n",
            "---\n",
            "Chunk 86:\n",
            "  Section: ml_service_pb2_grpc.py\n",
            "  Content length: 3930 characters\n",
            "---\n",
            "Chunk 87:\n",
            "  Section: output_pb2.py\n",
            "  Content length: 2651 characters\n",
            "---\n",
            "Chunk 88:\n",
            "  Section: sources_pb2.py\n",
            "  Content length: 11280 characters\n",
            "---\n",
            "Chunk 89:\n",
            "  Section: sources_p2p.py\n",
            "  Content length: 3065 characters\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(headers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwIuLuEjpXEd",
        "outputId": "4a5bb501-867e-41f8-89b1-691ae33bff53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "89"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in chunks:\n",
        "  header = chunk.metadata.get('section')\n",
        "  chunk.page_content = \"# \" + header + '\\n' + chunk.page_content"
      ],
      "metadata": {
        "id": "8FA4KpXitE5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chunks[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx8z4jssCAr-",
        "outputId": "7ec23ee8-b2fe-4c40-8285-07234147aba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# What is TurboML?\n",
            "@ TurboML - page_link: https://docs.turboml.com/intro/\n",
            "<page_content>\n",
            "Introduction  \n",
            "TurboML is a machine learning platform that’s reinvented for real-time. What does that mean? All the steps in the ML lifecycle, from data ingestion, to feature engineering, to ML modelling to post deployment steps like monitoring, are all designed so that in addition to batch data, they can also handle real-time data.  \n",
            "## Data Ingestion [Permalink for this section](https://docs.turboml.com/intro/\\#data-ingestion)  \n",
            "The first step is to bring your data to the TurboML platform. There are two major ways to ingest your data. Pull-based and Push-based.  \n",
            "### Pull-based ingestion [Permalink for this section](https://docs.turboml.com/intro/\\#pull-based-ingestion)  \n",
            "With this approach, you use TurboML’s prebuilt connectors to connect to your data source. The connectors will continuously pull data from your data source, and ingest it into TurboML.  \n",
            "### Push-based ingestion [Permalink for this section](https://docs.turboml.com/intro/\\#push-based-ingestion)  \n",
            "Sometimes, you might not want to send data via an intermediate data source, but rather directly send the data. Push-based ingestion can be used for this, where data can be send either via REST API calls, or using more performant client SDKs. Here’s an example with a Pandas DataFrame  \n",
            "```transactions = tb.PandasDataset(dataset_name=\"transactions\",dataframe=df, upload=True)\n",
            "transactions.configure_dataset(key_field=\"index\")\n",
            "```  \n",
            "## Feature Engineering [Permalink for this section](https://docs.turboml.com/intro/\\#feature-engineering)  \n",
            "Feature engineering is perhaps the most important step for data scientists. TurboML provides several different interfaces to define features. We’ve designed the feature engineering experience in a way so that after you’ve defined a feature, you can see that feature computed for your local data. This should help debug and iterate faster. Once you’re confident about a feature definition, you can deploy it where it’ll be continuously computed on the real-time data. Once deployed, these features are automatically computed on the streaming data. And we have retrieval APIs to compute it for ad-hoc queries.  \n",
            "### SQL Features [Permalink for this section](https://docs.turboml.com/intro/\\#sql-features)  \n",
            "Writing SQL queries is one of the most common way to define ML features. TurboML supports writing arbitrary SQL expressions to enable such features. Here’s an example with a simple SQL feature.  \n",
            "```transactions.feature_engineering.create_sql_features(\n",
            "sql_definition='\"transactionAmount\" + \"localHour\"',\n",
            "new_feature_name=\"my_sql_feat\",\n",
            ")\n",
            "```  \n",
            "Notice that the column names are in quotes.  \n",
            "And here’s a more complicated example  \n",
            "```transactions.feature_engineering.create_sql_features(\n",
            "sql_definition='CASE WHEN \"paymentBillingCountryCode\" <> \"ipCountryCode\" THEN 1 ELSE 0 END ',\n",
            "new_feature_name=\"country_code_match\",\n",
            ")\n",
            "```  \n",
            "### Aggregate Features [Permalink for this section](https://docs.turboml.com/intro/\\#aggregate-features)  \n",
            "A common template for real-time features is aggregating some value over some time window. To define such time-windowed aggregations, you first need to register a timestamp column for your dataset. This can be done as follows,  \n",
            "```transactions.feature_engineering.register_timestamp(column_name=\"timestamp\", format_type=\"epoch_seconds\")\n",
            "```  \n",
            "The supported formats can be found out using  \n",
            "```tb.get_timestamp_formats()\n",
            "```  \n",
            "Once the timestamp is registered, we can create a feature using  \n",
            "```transactions.feature_engineering.create_aggregate_features(\n",
            "column_to_operate=\"transactionAmount\",\n",
            "column_to_group=\"accountID\",\n",
            "operation=\"SUM\",\n",
            "new_feature_name=\"my_sum_feat\",\n",
            "timestamp_column=\"timestamp\",\n",
            "window_duration=24,\n",
            "window_unit=\"hours\"\n",
            ")\n",
            "```  \n",
            "### User Defined Features [Permalink for this section](https://docs.turboml.com/intro/\\#user-defined-features)  \n",
            "We understand why data scientists love Python - the simplicity, the ecosystem - is unmatchable. Guess what? You can use native Python, importing any library, [to define features](https://docs.turboml.com/feature_engineering/udf/)!  \n",
            "### IBIS Features [Permalink for this section](https://docs.turboml.com/intro/\\#ibis-features)  \n",
            "For streaming features that are more complex than just windowed aggregations, can be defined using the [ibis interface](https://docs.turboml.com/feature_engineering/advanced/ibis_feature_engineering/). They can then be executed using Apache Flink or RisingWave.  \n",
            "### Feature Retrieval [Permalink for this section](https://docs.turboml.com/intro/\\#feature-retrieval)  \n",
            "As mentioned before, once deployed, the feature computation is automatically added to the real-time streaming pipeline. However, feature values can also be retrieved on ad-hoc data using the retrieval API. Here’s an example  \n",
            "```features = tb.retrieve_features(\"transactions\", query_df)\n",
            "```  \n",
            "## ML Modelling - Basic concepts [Permalink for this section](https://docs.turboml.com/intro/\\#ml-modelling---basic-concepts)  \n",
            "### Inputs and Labels [Permalink for this section](https://docs.turboml.com/intro/\\#inputs-and-labels)  \n",
            "For each model, we need to specify the Inputs and the Labels.  \n",
            "### Types of fields [Permalink for this section](https://docs.turboml.com/intro/\\#types-of-fields)  \n",
            "Different models can accept different types of input fields. The supported types of fields are, numeric, categoric, time series, text, and image.  \n",
            "### TurboML algorithms [Permalink for this section](https://docs.turboml.com/intro/\\#turboml-algorithms)  \n",
            "TurboML provides several algorithms out of the box. These algorithms are optimized for online predictions and learning, and have been tested on real-world settings.  \n",
            "```model = tb.HoeffdingTreeClassifier(n_classes=2)\n",
            "```  \n",
            "### Pytorch/TensorFlow/Scikit-learn [Permalink for this section](https://docs.turboml.com/intro/\\#pytorchtensorflowscikit-learn)  \n",
            "We use ONNX to deploy trained models from [Pytorch](https://docs.turboml.com/byo_models/onnx_pytorch/), [TensorFlow](https://docs.turboml.com/byo_models/onnx_tensorflow/), [Scikit-learn](https://docs.turboml.com/byo_models/onnx_sklearn/) or other ONNX compatible frameworks. Example for these three frameworks can be found in the following notebooks.  \n",
            "Note: These models are static, and are not updated automatically.  \n",
            "### Python [Permalink for this section](https://docs.turboml.com/intro/\\#python)  \n",
            "TurboML also supports writing arbitrary Python code to define your own algorithms, including any libraries. To add your own algorithms, you need to define a Python class with 2 methods defined with the following signature:  \n",
            "```class Model:\n",
            "def learn_one(self, features, label):\n",
            "pass\n",
            "\n",
            "def predict_one(self, features, output_data):\n",
            "pass\n",
            "```  \n",
            "Examples of using an incremental learning algorithm, as well as a batch-like algorithm, can be found [here](https://docs.turboml.com/wyo_models/native_python_model/) from the river library.  \n",
            "### Combining models [Permalink for this section](https://docs.turboml.com/intro/\\#combining-models)  \n",
            "Models can also be combined to create other models, e.g. ensembles. An example of an ensemble model is as follows  \n",
            "```model = tb.LeveragingBaggingClassifier(n_classes=2, base_model = tb.HoeffdingTreeClassifier(n_classes=2))\n",
            "```  \n",
            "Preprocessors can also be chained and applied in a similar manner. E.g.  \n",
            "```model = tb.MinMaxPreProcessor(base_model = model)\n",
            "```  \n",
            "## Model Training [Permalink for this section](https://docs.turboml.com/intro/\\#model-training)  \n",
            "Once we’ve defined a model, it can be trained in different ways.  \n",
            "### Batch way [Permalink for this section](https://docs.turboml.com/intro/\\#batch-way)  \n",
            "The simplest way is to train the model in a batch way. This is similar to sklearn’s fit() method. However, internally the training is performed in an incremental manner. So, you can update an already trained model on some new data too. Here’s an example  \n",
            "```old_trained_model = model.learn(old_features, old_label)\n",
            "new_trained_model = old_trained_model.learn(new_features, new_label)\n",
            "```  \n",
            "Any trained copy of the model can be then deployed to production.  \n",
            "```deployed_model = new_trained_model.deploy(name = \"deployment_name\", input=features, labels=label, predict_only=True)\n",
            "```  \n",
            "Since this is a trained model, we can also invoke this model in a batch way to get predictions without actually deploying the mode.  \n",
            "```outputs = new_trained_model.predict(query_features)\n",
            "```  \n",
            "### Streaming way [Permalink for this section](https://docs.turboml.com/intro/\\#streaming-way)  \n",
            "This is where the model, after deployment, is continuously trained on new data. The user can choose how to update the model. The choices are online updates (where the model is updated on every new datapoint), or trigger-based updates which can be volume-based, time-based, performance-based or drift-based. The default option is online updates.  \n",
            "```deployed_model = model.deploy(name = \"deployment_name\", input=features, labels=label)\n",
            "```  \n",
            "## Deployment and MLOps [Permalink for this section](https://docs.turboml.com/intro/\\#deployment-and-mlops)  \n",
            "### Inference [Permalink for this section](https://docs.turboml.com/intro/\\#inference)  \n",
            "Once you’ve deployed a mode, there are several different ways to perform inference.  \n",
            "#### Async [Permalink for this section](https://docs.turboml.com/intro/\\#async)  \n",
            "The first one is the async method. The data that is streamed from the input source is continuously fed to the model, and the outputs are streamed to another source. This stream can be either be subscribed to directly be the end user application, or sinked to a database or other data sources.  \n",
            "```outputs = deployed_model.get_outputs()\n",
            "```  \n",
            "#### API [Permalink for this section](https://docs.turboml.com/intro/\\#api)  \n",
            "A request-response model is used for inference on a single data point synchronously. The `/model_name/predict` endpoint is exposed for each deployed model where a REST API call can be made to get the outputs.  \n",
            "#### Batch [Permalink for this section](https://docs.turboml.com/intro/\\#batch)  \n",
            "When you have multiple records you’d like to perform inference on, you can use the get\\_inference method as follows.  \n",
            "```outputs = deployed_model.get_inference(query_df)\n",
            "```  \n",
            "### Evaluation [Permalink for this section](https://docs.turboml.com/intro/\\#evaluation)  \n",
            "TurboML provides standard ML metrics out of the box to perform model evaluation. Multiple metrics can be registered for any deployed model. The metrics pipeline re-uses the labels extracted for model training.  \n",
            "```deployed_model.add_metric(\"WindowedAUC\")\n",
            "model_auc_scores = deployed_model.get_evaluation(\"WindowedAUC\")\n",
            "```  \n",
            "Last updated on January 24, 2025  \n",
            "[Quickstart](https://docs.turboml.com/quickstart/ \"Quickstart\")  \n",
            "What is TurboML? @ TurboML\n",
            "</page_content>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert chunks to a list of dictionaries\n",
        "chunk_data = [\n",
        "    {\"section\": chunk.metadata.get(\"section\"), \"content\": chunk.page_content}\n",
        "    for chunk in chunks\n",
        "]\n",
        "\n",
        "# Save to a JSON file\n",
        "with open(\"chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(chunk_data, f, indent=4, ensure_ascii=False)\n"
      ],
      "metadata": {
        "id": "qHYG8h6HC39Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load from JSON file\n",
        "with open(\"chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    loaded_data = json.load(f)\n",
        "\n",
        "# Convert back to Document objects\n",
        "documents = [\n",
        "    Document(page_content=item[\"content\"], metadata={\"section\": item[\"section\"]})\n",
        "    for item in loaded_data\n",
        "]\n",
        "\n",
        "# Check loaded documents\n",
        "for doc in documents:\n",
        "    print(f\"Section: {doc.metadata['section']}\")\n",
        "    print(f\"Content Character Count: {len(doc.page_content)}\")  # Print first 100 chars\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAushmjiFj1c",
        "outputId": "c310a4f5-9541-4186-e0d3-8abe947d2f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Section: What is TurboML?\n",
            "Content Character Count: 10809\n",
            "--------------------------------------------------\n",
            "Section: TurboML Quickstart\n",
            "Content Character Count: 15898\n",
            "--------------------------------------------------\n",
            "Section: String Encoding\n",
            "Content Character Count: 2860\n",
            "--------------------------------------------------\n",
            "Section: AMF Regressor\n",
            "Content Character Count: 2355\n",
            "--------------------------------------------------\n",
            "Section: MultinomialNB\n",
            "Content Character Count: 1215\n",
            "--------------------------------------------------\n",
            "Section: PreProcessors\n",
            "Content Character Count: 9350\n",
            "--------------------------------------------------\n",
            "Section: Python Model: Batch Example\n",
            "Content Character Count: 3595\n",
            "--------------------------------------------------\n",
            "Section: Image Processing (MNIST Example)\n",
            "Content Character Count: 4511\n",
            "--------------------------------------------------\n",
            "Section: Stream Dataset to Deployed Models\n",
            "Content Character Count: 15779\n",
            "--------------------------------------------------\n",
            "Section: LeveragingBaggingClassifier\n",
            "Content Character Count: 2489\n",
            "--------------------------------------------------\n",
            "Section: TF-IDF embedding example using gRPC Client\n",
            "Content Character Count: 3343\n",
            "--------------------------------------------------\n",
            "Section: EmbeddingModel\n",
            "Content Character Count: 1510\n",
            "--------------------------------------------------\n",
            "Section: BanditModelSelection\n",
            "Content Character Count: 1672\n",
            "--------------------------------------------------\n",
            "Section: HeteroLeveragingBaggingClassifier\n",
            "Content Character Count: 1934\n",
            "--------------------------------------------------\n",
            "Section: FFM Regressor\n",
            "Content Character Count: 2673\n",
            "--------------------------------------------------\n",
            "Section: Hyperparameter Tuning\n",
            "Content Character Count: 3603\n",
            "--------------------------------------------------\n",
            "Section: Algorithm Tuning\n",
            "Content Character Count: 3289\n",
            "--------------------------------------------------\n",
            "Section: Drift Detection\n",
            "Content Character Count: 2718\n",
            "--------------------------------------------------\n",
            "Section: MSTREAM\n",
            "Content Character Count: 1822\n",
            "--------------------------------------------------\n",
            "Section: RandomProjectionEmbedding\n",
            "Content Character Count: 1785\n",
            "--------------------------------------------------\n",
            "Section: Gaussian Naive Bayes\n",
            "Content Character Count: 1104\n",
            "--------------------------------------------------\n",
            "Section: Feature Engineering - Python UDAF\n",
            "Content Character Count: 4194\n",
            "--------------------------------------------------\n",
            "Section: TurboML LLM Tutorial\n",
            "Content Character Count: 2966\n",
            "--------------------------------------------------\n",
            "Section: AMF Classifier\n",
            "Content Character Count: 3572\n",
            "--------------------------------------------------\n",
            "Section: Image Processing (MNIST Example)\n",
            "Content Character Count: 5487\n",
            "--------------------------------------------------\n",
            "Section: Feature Engineering - Python UDFs\n",
            "Content Character Count: 4428\n",
            "--------------------------------------------------\n",
            "Section: Native Python Models\n",
            "Content Character Count: 9199\n",
            "--------------------------------------------------\n",
            "Section: ONNX tutorial with Scikit-Learn\n",
            "Content Character Count: 3637\n",
            "--------------------------------------------------\n",
            "Section: Adaptive LightGBM\n",
            "Content Character Count: 2121\n",
            "--------------------------------------------------\n",
            "Section: HoeffdingTreeRegressor\n",
            "Content Character Count: 1894\n",
            "--------------------------------------------------\n",
            "Section: Python Model: PySAD Example\n",
            "Content Character Count: 3652\n",
            "--------------------------------------------------\n",
            "Section: Batch APIs\n",
            "Content Character Count: 4663\n",
            "--------------------------------------------------\n",
            "Section: ContextualBanditModelSelection\n",
            "Content Character Count: 1570\n",
            "--------------------------------------------------\n",
            "Section: ONNX tutorial with TensorFlow\n",
            "Content Character Count: 3952\n",
            "--------------------------------------------------\n",
            "Section: Local Model\n",
            "Content Character Count: 13204\n",
            "--------------------------------------------------\n",
            "Section: Neural Network\n",
            "Content Character Count: 2408\n",
            "--------------------------------------------------\n",
            "Section: OCR example using RestAPI Client\n",
            "Content Character Count: 4506\n",
            "--------------------------------------------------\n",
            "Section: HeteroAdaBoostClassifier\n",
            "Content Character Count: 1319\n",
            "--------------------------------------------------\n",
            "Section: ONNX\n",
            "Content Character Count: 1126\n",
            "--------------------------------------------------\n",
            "Section: Resnet example using gRPC Client\n",
            "Content Character Count: 3934\n",
            "--------------------------------------------------\n",
            "Section: OVR (OnevsRestClassifier)\n",
            "Content Character Count: 1197\n",
            "--------------------------------------------------\n",
            "Section: Hoeffding Tree Classifier\n",
            "Content Character Count: 3367\n",
            "--------------------------------------------------\n",
            "Section: AdaBoostClassifier\n",
            "Content Character Count: 2038\n",
            "--------------------------------------------------\n",
            "Section: Random Sampler\n",
            "Content Character Count: 1613\n",
            "--------------------------------------------------\n",
            "Section: FFM Classifier\n",
            "Content Character Count: 2620\n",
            "--------------------------------------------------\n",
            "Section: SNARIMAX\n",
            "Content Character Count: 3311\n",
            "--------------------------------------------------\n",
            "Section: LLM Embeddings\n",
            "Content Character Count: 4317\n",
            "--------------------------------------------------\n",
            "Section: LLAMA Embedding\n",
            "Content Character Count: 1286\n",
            "--------------------------------------------------\n",
            "Section: Model Explanations using iXAI\n",
            "Content Character Count: 5091\n",
            "--------------------------------------------------\n",
            "Section: ONNX tutorial with PyTorch\n",
            "Content Character Count: 5456\n",
            "--------------------------------------------------\n",
            "Section: Half-Space Trees (HST)\n",
            "Content Character Count: 1368\n",
            "--------------------------------------------------\n",
            "Section: SGT Regressor\n",
            "Content Character Count: 3156\n",
            "--------------------------------------------------\n",
            "Section: Ensembling Custom Python Models in TurboML\n",
            "Content Character Count: 6409\n",
            "--------------------------------------------------\n",
            "Section: Custom Evaluation Metric\n",
            "Content Character Count: 5579\n",
            "--------------------------------------------------\n",
            "Section: Adaptive XGBoost\n",
            "Content Character Count: 2222\n",
            "--------------------------------------------------\n",
            "Section: Random Cut Forest\n",
            "Content Character Count: 1357\n",
            "--------------------------------------------------\n",
            "Section: Feature Engineering - Complex Stream Processing\n",
            "Content Character Count: 10409\n",
            "--------------------------------------------------\n",
            "Section: TurboML Ibis Quickstart\n",
            "Content Character Count: 3570\n",
            "--------------------------------------------------\n",
            "Section: Performance Improvements\n",
            "Content Character Count: 5318\n",
            "--------------------------------------------------\n",
            "Section: Online Neural Network\n",
            "Content Character Count: 1812\n",
            "--------------------------------------------------\n",
            "Section: __init__.py\n",
            "Content Character Count: 15553\n",
            "--------------------------------------------------\n",
            "Section: api.py\n",
            "Content Character Count: 5828\n",
            "--------------------------------------------------\n",
            "Section: concurrent.py\n",
            "Content Character Count: 542\n",
            "--------------------------------------------------\n",
            "Section: dataloader.py\n",
            "Content Character Count: 12479\n",
            "--------------------------------------------------\n",
            "Section: datasets.py\n",
            "Content Character Count: 22941\n",
            "--------------------------------------------------\n",
            "Section: default_model_configs.py\n",
            "Content Character Count: 7089\n",
            "--------------------------------------------------\n",
            "Section: env.py\n",
            "Content Character Count: 883\n",
            "--------------------------------------------------\n",
            "Section: feature_engineering.py\n",
            "Content Character Count: 44418\n",
            "--------------------------------------------------\n",
            "Section: internal.py\n",
            "Content Character Count: 7021\n",
            "--------------------------------------------------\n",
            "Section: llm.py\n",
            "Content Character Count: 2520\n",
            "--------------------------------------------------\n",
            "Section: ml_algs.py\n",
            "Content Character Count: 55083\n",
            "--------------------------------------------------\n",
            "Section: models.py\n",
            "Content Character Count: 25816\n",
            "--------------------------------------------------\n",
            "Section: model_comparison.py\n",
            "Content Character Count: 1433\n",
            "--------------------------------------------------\n",
            "Section: namespaces.py\n",
            "Content Character Count: 1173\n",
            "--------------------------------------------------\n",
            "Section: pymodel.py\n",
            "Content Character Count: 112\n",
            "--------------------------------------------------\n",
            "Section: pytypes.py\n",
            "Content Character Count: 112\n",
            "--------------------------------------------------\n",
            "Section: types.py\n",
            "Content Character Count: 602\n",
            "--------------------------------------------------\n",
            "Section: udf.py\n",
            "Content Character Count: 1729\n",
            "--------------------------------------------------\n",
            "Section: util.py\n",
            "Content Character Count: 4776\n",
            "--------------------------------------------------\n",
            "Section: config_pb2.py\n",
            "Content Character Count: 56263\n",
            "--------------------------------------------------\n",
            "Section: flink_pb2.py\n",
            "Content Character Count: 15511\n",
            "--------------------------------------------------\n",
            "Section: flink_pb2_grpc.py\n",
            "Content Character Count: 4073\n",
            "--------------------------------------------------\n",
            "Section: input_pb2.py\n",
            "Content Character Count: 2517\n",
            "--------------------------------------------------\n",
            "Section: metrics_pb2.py\n",
            "Content Character Count: 1610\n",
            "--------------------------------------------------\n",
            "Section: ml_service_pb2.py\n",
            "Content Character Count: 1747\n",
            "--------------------------------------------------\n",
            "Section: ml_service_pb2_grpc.py\n",
            "Content Character Count: 3955\n",
            "--------------------------------------------------\n",
            "Section: output_pb2.py\n",
            "Content Character Count: 2667\n",
            "--------------------------------------------------\n",
            "Section: sources_pb2.py\n",
            "Content Character Count: 11297\n",
            "--------------------------------------------------\n",
            "Section: sources_p2p.py\n",
            "Content Character Count: 3082\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################\n",
        "\n",
        "#              INTERVENTION\n",
        "\n",
        "#################################################"
      ],
      "metadata": {
        "id": "S6ZPbpIpdlHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is the markdown of the Notebook that I ran on RunPod -\n",
        "\n",
        "### `Later switched to colab again to save money. You can minimize to skip seeing the code`"
      ],
      "metadata": {
        "id": "cxJZ4hGFX54n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> to create context (Group related pages and internal code implementation) for Robust as well as Diverse Synthetic Question Generation, I used BGE-m3 for best performance - had to use 48-GB A40 GPU to do it effectively.\n",
        "\n",
        "Here's a concise, structured summary of the actions performed:\n",
        "\n",
        "1.  **Setup:** Installed necessary libraries (FAISS, Langchain, and related components).\n",
        "\n",
        "2.  **Data Loading:**\n",
        "    *   Loaded pre-processed document chunks from a JSON file (chunks.json) which we exported from this colab notebook, converting them into `Document` objects with content and section metadata.\n",
        "    *   Loaded a `summary document` that contains summary of `TurboML Docs & Implementations` from a Markdown file `summary.md`, also as a `Document` object.\n",
        "\n",
        "3.  **Vector Store Initialization:**\n",
        "    *   Initialized a HuggingFace BGE embeddings model for generating text embeddings.\n",
        "    *   Created a FAISS (Facebook AI Similarity Search) index for efficient similarity search, using the embedding dimension.\n",
        "    *   Initialized a Langchain FAISS vector store, linking the embeddings, FAISS index, and an in-memory document store.\n",
        "\n",
        "4.  **Vector Store Population:**\n",
        "    *   Added the loaded document chunks to the FAISS vector store, generating unique IDs for each.\n",
        "\n",
        "5.  **Context Merging:**\n",
        "    *   For each document chunk:\n",
        "        *   Performed a similarity search in the vector store to find related chunks.\n",
        "        *   Filtered out self-matches.\n",
        "        *   Selected the top similar chunks, the original chunk, and the summary document.\n",
        "        *   Merged the content of these selected documents, creating a single, context-rich text.\n",
        "        *   Generated an embedding for this merged context.\n",
        "        *   Stored the base chunk's section, the sections of the merged documents, the merged content, and its embedding.\n",
        "\n",
        "6.  **Output:**\n",
        "    *   Saved the generated merged contexts (including base chunk, context sections, merged content, and embedding) to a JSON file.\n",
        "    *   Verified the output by printing samples of the merged contexts, showing the base chunk, merged sections, and content length.\n",
        "\n",
        "\n",
        "```python\n",
        "!pip install -qU faiss-cpu langchain langchain_community langchain-huggingface\n",
        "```\n",
        "\n",
        "\n",
        "```python\n",
        "import json\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "import faiss\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from uuid import uuid4\n",
        "```\n",
        "\n",
        "1. Load Prepared Chunks\n",
        "\n",
        "\n",
        "```python\n",
        "# Load prepared chunks from JSON file\n",
        "with open(\"chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    chunk_data = json.load(f)\n",
        "\n",
        "# Convert to Document format with metadata\n",
        "documents = [\n",
        "    Document(page_content=item[\"content\"], metadata={\"section\": item[\"section\"]})\n",
        "    for item in chunk_data\n",
        "]\n",
        "\n",
        "print(f\"Loaded {len(documents)} document chunks.\")\n",
        "\n",
        "```\n",
        "\n",
        "    Loaded 89 document chunks.\n",
        "\n",
        "\n",
        "2. Load Summary Document\n",
        "\n",
        "\n",
        "```python\n",
        "# Load summary content\n",
        "with open(\"summary.md\", \"r\") as f:\n",
        "    summary_content = f.read()\n",
        "\n",
        "# Create a Document object for the summary\n",
        "summary_doc = Document(\n",
        "    page_content=summary_content,\n",
        "    metadata={\"section\": \"summary\", \"source\": \"summary.md\"}\n",
        ")\n",
        "\n",
        "print(\"Summary document loaded successfully.\")\n",
        "\n",
        "```\n",
        "\n",
        "    Summary document loaded successfully.\n",
        "\n",
        "\n",
        "3. Initialize Embeddings & FAISS Vector Store\n",
        "\n",
        "\n",
        "```python\n",
        "# Define model\n",
        "model_name = \"BAAI/bge-m3\"\n",
        "\n",
        "# Initialize embeddings\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Determine the embedding dimension\n",
        "dimension = len(embeddings.embed_query(\"test\"))\n",
        "\n",
        "# Initialize FAISS index\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "print(f\"Initialized FAISS index with dimension: {dimension}\")\n",
        "\n",
        "```\n",
        "\n",
        "    /tmp/ipykernel_686/1907406924.py:5: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
        "      embeddings = HuggingFaceBgeEmbeddings(\n",
        "\n",
        "\n",
        "    Initialized FAISS index with dimension: 1024\n",
        "\n",
        "\n",
        "4. Create Vector Store with Documents\n",
        "\n",
        "\n",
        "```python\n",
        "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
        "\n",
        "vector_store = FAISS(\n",
        "    embedding_function=embeddings,\n",
        "    index=index,\n",
        "    docstore=InMemoryDocstore(),\n",
        "    index_to_docstore_id={},\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "```python\n",
        "# Combine summary with all document chunks\n",
        "all_docs = documents\n",
        "\n",
        "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "vector_store.add_documents(documents=documents, ids=uuids)\n",
        "```\n",
        "\n",
        "\n",
        "```python\n",
        "print(f\"FAISS vector store created with {len(all_docs)} documents.\")\n",
        "\n",
        "```\n",
        "\n",
        "    FAISS vector store created with 89 documents.\n",
        "\n",
        "\n",
        "5. Generate Merged Contexts\n",
        "\n",
        "```python\n",
        "merged_contexts = []\n",
        "\n",
        "for idx, doc in enumerate(documents):\n",
        "    try:\n",
        "        # Find similar chunks (excluding self and summary)\n",
        "        similar_docs = vector_store.similarity_search(\n",
        "            query=doc.page_content,\n",
        "            k=4,  # Fetch extra in case of self-match\n",
        "        )\n",
        "        \n",
        "        # Remove accidental self-matches\n",
        "        filtered_docs = [d for d in similar_docs if d.metadata[\"section\"] != doc.metadata[\"section\"]]\n",
        "        \n",
        "        # Select top 2 similar documents + current doc + summary\n",
        "        selected_docs = [summary_doc, doc] + filtered_docs[:2]\n",
        "        \n",
        "        # Merge content with section headers\n",
        "        merged_content = \"\\n\\n\".join([\n",
        "            d.page_content\n",
        "            for d in selected_docs\n",
        "        ])\n",
        "        \n",
        "        # Store merged context data\n",
        "        merged_contexts.append({\n",
        "            \"base_chunk\": doc.metadata[\"section\"],\n",
        "            \"context_sections\": [d.metadata[\"section\"] for d in selected_docs],\n",
        "            \"content\": merged_content,\n",
        "            \"embedding\": embeddings.embed_query(merged_content)\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error processing chunk {idx}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "print(f\"Generated {len(merged_contexts)} merged contexts.\")\n",
        "\n",
        "```\n",
        "\n",
        "    Generated 89 merged contexts.\n",
        "\n",
        "\n",
        "6. Save Merged Contexts to File\n",
        "\n",
        "\n",
        "```python\n",
        "# Save merged contexts as JSON file\n",
        "with open(\"merged_contexts.json\", \"w\") as f:\n",
        "    json.dump(merged_contexts, f, indent=2)\n",
        "\n",
        "print(\"Merged contexts saved successfully.\")\n",
        "\n",
        "```\n",
        "\n",
        "    Merged contexts saved successfully.\n",
        "\n",
        "\n",
        "7. Verify Sample Output\n",
        "\n",
        "\n",
        "```python\n",
        "for i, sample in enumerate(merged_contexts):\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"Base chunk: {sample['base_chunk']}\")\n",
        "    print(f\"Merged sections: {sample['context_sections']}\")\n",
        "    print(f\"Content length: {len(sample['content'])} characters\")\n",
        "    print(\"=\" * 50)  # Separator for readability\n",
        "\n",
        "```\n",
        "\n",
        "    Sample 1:\n",
        "    Base chunk: What is TurboML?\n",
        "    Merged sections: ['summary', 'What is TurboML?', 'TurboML Quickstart', 'Batch APIs']\n",
        "    Content length: 58376 characters\n",
        "    ==================================================\n",
        "    Sample 2:\n",
        "    Base chunk: TurboML Quickstart\n",
        "    Merged sections: ['summary', 'TurboML Quickstart', 'What is TurboML?', 'Batch APIs']\n",
        "    Content length: 58376 characters\n",
        "    ==================================================\n",
        "    Sample 3:\n",
        "    Base chunk: String Encoding\n",
        "    Merged sections: ['summary', 'String Encoding', 'Hyperparameter Tuning', 'Algorithm Tuning']\n",
        "    Content length: 36758 characters\n",
        "    ==================================================\n",
        "    Sample 4:\n",
        "    Base chunk: AMF Regressor\n",
        "    Merged sections: ['summary', 'AMF Regressor', 'AMF Classifier', 'FFM Regressor']\n",
        "    Content length: 35606 characters\n",
        "    ==================================================\n",
        "    Sample 5:\n",
        "    Base chunk: MultinomialNB\n",
        "    Merged sections: ['summary', 'MultinomialNB', 'Gaussian Naive Bayes', 'HeteroAdaBoostClassifier']\n",
        "    Content length: 30644 characters\n",
        "    ==================================================\n",
        "    Sample 6:\n",
        "    Base chunk: PreProcessors\n",
        "    Merged sections: ['summary', 'PreProcessors', 'EmbeddingModel', 'What is TurboML?']\n",
        "    Content length: 48675 characters\n",
        "    ==================================================\n",
        "    Sample 7:\n",
        "    Base chunk: Python Model: Batch Example\n",
        "    Merged sections: ['summary', 'Python Model: Batch Example', 'Python Model: PySAD Example', 'Batch APIs']\n",
        "    Content length: 38916 characters\n",
        "    ==================================================\n",
        "    Sample 8:\n",
        "    Base chunk: Image Processing (MNIST Example)\n",
        "    Merged sections: ['summary', 'Image Processing (MNIST Example)', 'LLM Embeddings', 'What is TurboML?']\n",
        "    Content length: 46643 characters\n",
        "    ==================================================\n",
        "    Sample 9:\n",
        "    Base chunk: Stream Dataset to Deployed Models\n",
        "    Merged sections: ['summary', 'Stream Dataset to Deployed Models', 'TurboML Quickstart', 'datasets.py']\n",
        "    Content length: 81624 characters\n",
        "    ==================================================\n",
        "    Sample 10:\n",
        "    Base chunk: LeveragingBaggingClassifier\n",
        "    Merged sections: ['summary', 'LeveragingBaggingClassifier', 'HeteroLeveragingBaggingClassifier', 'HeteroAdaBoostClassifier']\n",
        "    Content length: 32748 characters\n",
        "    ==================================================\n",
        "    Sample 11:\n",
        "    Base chunk: TF-IDF embedding example using gRPC Client\n",
        "    Merged sections: ['summary', 'TF-IDF embedding example using gRPC Client', 'Resnet example using gRPC Client', 'Image Processing (MNIST Example)']\n",
        "    Content length: 38794 characters\n",
        "    ==================================================\n",
        "    Sample 12:\n",
        "    Base chunk: EmbeddingModel\n",
        "    Merged sections: ['summary', 'EmbeddingModel', 'PreProcessors', 'RandomProjectionEmbedding']\n",
        "    Content length: 39651 characters\n",
        "    ==================================================\n",
        "    Sample 13:\n",
        "    Base chunk: BanditModelSelection\n",
        "    Merged sections: ['summary', 'BanditModelSelection', 'ContextualBanditModelSelection', 'AdaBoostClassifier']\n",
        "    Content length: 32286 characters\n",
        "    ==================================================\n",
        "    Sample 14:\n",
        "    Base chunk: HeteroLeveragingBaggingClassifier\n",
        "    Merged sections: ['summary', 'HeteroLeveragingBaggingClassifier', 'HeteroAdaBoostClassifier', 'LeveragingBaggingClassifier']\n",
        "    Content length: 32748 characters\n",
        "    ==================================================\n",
        "    Sample 15:\n",
        "    Base chunk: FFM Regressor\n",
        "    Merged sections: ['summary', 'FFM Regressor', 'FFM Classifier', 'AMF Regressor']\n",
        "    Content length: 34654 characters\n",
        "    ==================================================\n",
        "    Sample 16:\n",
        "    Base chunk: Hyperparameter Tuning\n",
        "    Merged sections: ['summary', 'Hyperparameter Tuning', 'Algorithm Tuning', 'Performance Improvements']\n",
        "    Content length: 39216 characters\n",
        "    ==================================================\n",
        "    Sample 17:\n",
        "    Base chunk: Algorithm Tuning\n",
        "    Merged sections: ['summary', 'Algorithm Tuning', 'Hyperparameter Tuning', 'Performance Improvements']\n",
        "    Content length: 39216 characters\n",
        "    ==================================================\n",
        "    Sample 18:\n",
        "    Base chunk: Drift Detection\n",
        "    Merged sections: ['summary', 'Drift Detection', 'Performance Improvements', 'TurboML Quickstart']\n",
        "    Content length: 50940 characters\n",
        "    ==================================================\n",
        "    Sample 19:\n",
        "    Base chunk: MSTREAM\n",
        "    Merged sections: ['summary', 'MSTREAM', 'Random Cut Forest', 'Half-Space Trees (HST)']\n",
        "    Content length: 31553 characters\n",
        "    ==================================================\n",
        "    Sample 20:\n",
        "    Base chunk: RandomProjectionEmbedding\n",
        "    Merged sections: ['summary', 'RandomProjectionEmbedding', 'EmbeddingModel', 'Random Sampler']\n",
        "    Content length: 31914 characters\n",
        "    ==================================================\n",
        "    Sample 21:\n",
        "    Base chunk: Gaussian Naive Bayes\n",
        "    Merged sections: ['summary', 'Gaussian Naive Bayes', 'MultinomialNB', 'HeteroLeveragingBaggingClassifier']\n",
        "    Content length: 31259 characters\n",
        "    ==================================================\n",
        "    Sample 22:\n",
        "    Base chunk: Feature Engineering - Python UDAF\n",
        "    Merged sections: ['summary', 'Feature Engineering - Python UDAF', 'Feature Engineering - Python UDFs', 'udf.py']\n",
        "    Content length: 37357 characters\n",
        "    ==================================================\n",
        "    Sample 23:\n",
        "    Base chunk: TurboML LLM Tutorial\n",
        "    Merged sections: ['summary', 'TurboML LLM Tutorial', 'LLM Embeddings', 'llm.py']\n",
        "    Content length: 36809 characters\n",
        "    ==================================================\n",
        "    Sample 24:\n",
        "    Base chunk: AMF Classifier\n",
        "    Merged sections: ['summary', 'AMF Classifier', 'AMF Regressor', 'Hoeffding Tree Classifier']\n",
        "    Content length: 36300 characters\n",
        "    ==================================================\n",
        "    Sample 25:\n",
        "    Base chunk: Image Processing (MNIST Example)\n",
        "    Merged sections: ['summary', 'Image Processing (MNIST Example)', 'Algorithm Tuning', 'TurboML Quickstart']\n",
        "    Content length: 51680 characters\n",
        "    ==================================================\n",
        "    Sample 26:\n",
        "    Base chunk: Feature Engineering - Python UDFs\n",
        "    Merged sections: ['summary', 'Feature Engineering - Python UDFs', 'Feature Engineering - Python UDAF', 'Feature Engineering - Complex Stream Processing']\n",
        "    Content length: 46037 characters\n",
        "    ==================================================\n",
        "    Sample 27:\n",
        "    Base chunk: Native Python Models\n",
        "    Merged sections: ['summary', 'Native Python Models', 'Ensembling Custom Python Models in TurboML', 'Python Model: Batch Example']\n",
        "    Content length: 46209 characters\n",
        "    ==================================================\n",
        "    Sample 28:\n",
        "    Base chunk: ONNX tutorial with Scikit-Learn\n",
        "    Merged sections: ['summary', 'ONNX tutorial with Scikit-Learn', 'ONNX tutorial with TensorFlow', 'ONNX tutorial with PyTorch']\n",
        "    Content length: 40051 characters\n",
        "    ==================================================\n",
        "    Sample 29:\n",
        "    Base chunk: Adaptive LightGBM\n",
        "    Merged sections: ['summary', 'Adaptive LightGBM', 'Adaptive XGBoost', 'SGT Regressor']\n",
        "    Content length: 34505 characters\n",
        "    ==================================================\n",
        "    Sample 30:\n",
        "    Base chunk: HoeffdingTreeRegressor\n",
        "    Merged sections: ['summary', 'HoeffdingTreeRegressor', 'Hoeffding Tree Classifier', 'SGT Regressor']\n",
        "    Content length: 35423 characters\n",
        "    ==================================================\n",
        "    Sample 31:\n",
        "    Base chunk: Python Model: PySAD Example\n",
        "    Merged sections: ['summary', 'Python Model: PySAD Example', 'Python Model: Batch Example', 'Native Python Models']\n",
        "    Content length: 43452 characters\n",
        "    ==================================================\n",
        "    Sample 32:\n",
        "    Base chunk: Batch APIs\n",
        "    Merged sections: ['summary', 'Batch APIs', 'What is TurboML?', 'TurboML Quickstart']\n",
        "    Content length: 58376 characters\n",
        "    ==================================================\n",
        "    Sample 33:\n",
        "    Base chunk: ContextualBanditModelSelection\n",
        "    Merged sections: ['summary', 'ContextualBanditModelSelection', 'BanditModelSelection', 'LeveragingBaggingClassifier']\n",
        "    Content length: 32737 characters\n",
        "    ==================================================\n",
        "    Sample 34:\n",
        "    Base chunk: ONNX tutorial with TensorFlow\n",
        "    Merged sections: ['summary', 'ONNX tutorial with TensorFlow', 'ONNX tutorial with Scikit-Learn', 'ONNX tutorial with PyTorch']\n",
        "    Content length: 40051 characters\n",
        "    ==================================================\n",
        "    Sample 35:\n",
        "    Base chunk: Local Model\n",
        "    Merged sections: ['summary', 'Local Model', 'TurboML Quickstart', 'ONNX tutorial with PyTorch']\n",
        "    Content length: 61564 characters\n",
        "    ==================================================\n",
        "    Sample 36:\n",
        "    Base chunk: Neural Network\n",
        "    Merged sections: ['summary', 'Neural Network', 'Online Neural Network', 'ONNX']\n",
        "    Content length: 32352 characters\n",
        "    ==================================================\n",
        "    Sample 37:\n",
        "    Base chunk: OCR example using RestAPI Client\n",
        "    Merged sections: ['summary', 'OCR example using RestAPI Client', 'Resnet example using gRPC Client', 'TF-IDF embedding example using gRPC Client']\n",
        "    Content length: 38789 characters\n",
        "    ==================================================\n",
        "    Sample 38:\n",
        "    Base chunk: HeteroAdaBoostClassifier\n",
        "    Merged sections: ['summary', 'HeteroAdaBoostClassifier', 'HeteroLeveragingBaggingClassifier', 'AdaBoostClassifier']\n",
        "    Content length: 32297 characters\n",
        "    ==================================================\n",
        "    Sample 39:\n",
        "    Base chunk: ONNX\n",
        "    Merged sections: ['summary', 'ONNX', 'ONNX tutorial with TensorFlow', 'ONNX tutorial with Scikit-Learn']\n",
        "    Content length: 35721 characters\n",
        "    ==================================================\n",
        "    Sample 40:\n",
        "    Base chunk: Resnet example using gRPC Client\n",
        "    Merged sections: ['summary', 'Resnet example using gRPC Client', 'TF-IDF embedding example using gRPC Client', 'OCR example using RestAPI Client']\n",
        "    Content length: 38789 characters\n",
        "    ==================================================\n",
        "    Sample 41:\n",
        "    Base chunk: OVR (OnevsRestClassifier)\n",
        "    Merged sections: ['summary', 'OVR (OnevsRestClassifier)', 'Random Sampler', 'PreProcessors']\n",
        "    Content length: 39166 characters\n",
        "    ==================================================\n",
        "    Sample 42:\n",
        "    Base chunk: Hoeffding Tree Classifier\n",
        "    Merged sections: ['summary', 'Hoeffding Tree Classifier', 'HoeffdingTreeRegressor', 'AMF Classifier']\n",
        "    Content length: 35839 characters\n",
        "    ==================================================\n",
        "    Sample 43:\n",
        "    Base chunk: AdaBoostClassifier\n",
        "    Merged sections: ['summary', 'AdaBoostClassifier', 'HeteroAdaBoostClassifier', 'BanditModelSelection']\n",
        "    Content length: 32035 characters\n",
        "    ==================================================\n",
        "    Sample 44:\n",
        "    Base chunk: Random Sampler\n",
        "    Merged sections: ['summary', 'Random Sampler', 'EmbeddingModel', 'RandomProjectionEmbedding']\n",
        "    Content length: 31914 characters\n",
        "    ==================================================\n",
        "    Sample 45:\n",
        "    Base chunk: FFM Classifier\n",
        "    Merged sections: ['summary', 'FFM Classifier', 'FFM Regressor', 'AMF Classifier']\n",
        "    Content length: 35871 characters\n",
        "    ==================================================\n",
        "    Sample 46:\n",
        "    Base chunk: SNARIMAX\n",
        "    Merged sections: ['summary', 'SNARIMAX', 'What is TurboML?', 'ONNX tutorial with Scikit-Learn']\n",
        "    Content length: 44763 characters\n",
        "    ==================================================\n",
        "    Sample 47:\n",
        "    Base chunk: LLM Embeddings\n",
        "    Merged sections: ['summary', 'LLM Embeddings', 'Image Processing (MNIST Example)', 'TurboML LLM Tutorial']\n",
        "    Content length: 38800 characters\n",
        "    ==================================================\n",
        "    Sample 48:\n",
        "    Base chunk: LLAMA Embedding\n",
        "    Merged sections: ['summary', 'LLAMA Embedding', 'LLM Embeddings', 'HeteroAdaBoostClassifier']\n",
        "    Content length: 33928 characters\n",
        "    ==================================================\n",
        "    Sample 49:\n",
        "    Base chunk: Model Explanations using iXAI\n",
        "    Merged sections: ['summary', 'Model Explanations using iXAI', 'What is TurboML?', 'TurboML Quickstart']\n",
        "    Content length: 58804 characters\n",
        "    ==================================================\n",
        "    Sample 50:\n",
        "    Base chunk: ONNX tutorial with PyTorch\n",
        "    Merged sections: ['summary', 'ONNX tutorial with PyTorch', 'ONNX tutorial with Scikit-Learn', 'ONNX tutorial with TensorFlow']\n",
        "    Content length: 40051 characters\n",
        "    ==================================================\n",
        "    Sample 51:\n",
        "    Base chunk: Half-Space Trees (HST)\n",
        "    Merged sections: ['summary', 'Half-Space Trees (HST)', 'MSTREAM', 'Random Cut Forest']\n",
        "    Content length: 31553 characters\n",
        "    ==================================================\n",
        "    Sample 52:\n",
        "    Base chunk: SGT Regressor\n",
        "    Merged sections: ['summary', 'SGT Regressor', 'HoeffdingTreeRegressor', 'AMF Regressor']\n",
        "    Content length: 34411 characters\n",
        "    ==================================================\n",
        "    Sample 53:\n",
        "    Base chunk: Ensembling Custom Python Models in TurboML\n",
        "    Merged sections: ['summary', 'Ensembling Custom Python Models in TurboML', 'Native Python Models', 'Python Model: Batch Example']\n",
        "    Content length: 46209 characters\n",
        "    ==================================================\n",
        "    Sample 54:\n",
        "    Base chunk: Custom Evaluation Metric\n",
        "    Merged sections: ['summary', 'Custom Evaluation Metric', 'Ensembling Custom Python Models in TurboML', 'Feature Engineering - Python UDAF']\n",
        "    Content length: 43188 characters\n",
        "    ==================================================\n",
        "    Sample 55:\n",
        "    Base chunk: Adaptive XGBoost\n",
        "    Merged sections: ['summary', 'Adaptive XGBoost', 'Adaptive LightGBM', 'Model Explanations using iXAI']\n",
        "    Content length: 36440 characters\n",
        "    ==================================================\n",
        "    Sample 56:\n",
        "    Base chunk: Random Cut Forest\n",
        "    Merged sections: ['summary', 'Random Cut Forest', 'MSTREAM', 'AMF Regressor']\n",
        "    Content length: 32540 characters\n",
        "    ==================================================\n",
        "    Sample 57:\n",
        "    Base chunk: Feature Engineering - Complex Stream Processing\n",
        "    Merged sections: ['summary', 'Feature Engineering - Complex Stream Processing', 'TurboML Ibis Quickstart', 'datasets.py']\n",
        "    Content length: 63926 characters\n",
        "    ==================================================\n",
        "    Sample 58:\n",
        "    Base chunk: TurboML Ibis Quickstart\n",
        "    Merged sections: ['summary', 'TurboML Ibis Quickstart', 'Feature Engineering - Complex Stream Processing', 'TurboML Quickstart']\n",
        "    Content length: 56883 characters\n",
        "    ==================================================\n",
        "    Sample 59:\n",
        "    Base chunk: Performance Improvements\n",
        "    Merged sections: ['summary', 'Performance Improvements', 'Algorithm Tuning', 'Hyperparameter Tuning']\n",
        "    Content length: 39216 characters\n",
        "    ==================================================\n",
        "    Sample 60:\n",
        "    Base chunk: Online Neural Network\n",
        "    Merged sections: ['summary', 'Online Neural Network', 'ONNX', 'Neural Network']\n",
        "    Content length: 32352 characters\n",
        "    ==================================================\n",
        "    Sample 61:\n",
        "    Base chunk: __init__.py\n",
        "    Merged sections: ['summary', '__init__.py', 'models.py', 'ml_algs.py']\n",
        "    Content length: 123458 characters\n",
        "    ==================================================\n",
        "    Sample 62:\n",
        "    Base chunk: api.py\n",
        "    Merged sections: ['summary', 'api.py', 'namespaces.py', 'dataloader.py']\n",
        "    Content length: 46486 characters\n",
        "    ==================================================\n",
        "    Sample 63:\n",
        "    Base chunk: concurrent.py\n",
        "    Merged sections: ['summary', 'concurrent.py', 'internal.py', 'types.py']\n",
        "    Content length: 35171 characters\n",
        "    ==================================================\n",
        "    Sample 64:\n",
        "    Base chunk: dataloader.py\n",
        "    Merged sections: ['summary', 'dataloader.py', 'models.py', 'ml_algs.py']\n",
        "    Content length: 120384 characters\n",
        "    ==================================================\n",
        "    Sample 65:\n",
        "    Base chunk: datasets.py\n",
        "    Merged sections: ['summary', 'datasets.py', 'Stream Dataset to Deployed Models', 'feature_engineering.py']\n",
        "    Content length: 110144 characters\n",
        "    ==================================================\n",
        "    Sample 66:\n",
        "    Base chunk: default_model_configs.py\n",
        "    Merged sections: ['summary', 'default_model_configs.py', 'config_pb2.py', 'models.py']\n",
        "    Content length: 116174 characters\n",
        "    ==================================================\n",
        "    Sample 67:\n",
        "    Base chunk: env.py\n",
        "    Merged sections: ['summary', 'env.py', 'pymodel.py', 'pytypes.py']\n",
        "    Content length: 28113 characters\n",
        "    ==================================================\n",
        "    Sample 68:\n",
        "    Base chunk: feature_engineering.py\n",
        "    Merged sections: ['summary', 'feature_engineering.py', 'models.py', 'datasets.py']\n",
        "    Content length: 120181 characters\n",
        "    ==================================================\n",
        "    Sample 69:\n",
        "    Base chunk: internal.py\n",
        "    Merged sections: ['summary', 'internal.py', 'ml_algs.py', 'dataloader.py']\n",
        "    Content length: 101589 characters\n",
        "    ==================================================\n",
        "    Sample 70:\n",
        "    Base chunk: llm.py\n",
        "    Merged sections: ['summary', 'llm.py', 'TurboML LLM Tutorial', 'LLM Embeddings']\n",
        "    Content length: 36809 characters\n",
        "    ==================================================\n",
        "    Sample 71:\n",
        "    Base chunk: ml_algs.py\n",
        "    Merged sections: ['summary', 'ml_algs.py', 'models.py', '__init__.py']\n",
        "    Content length: 123458 characters\n",
        "    ==================================================\n",
        "    Sample 72:\n",
        "    Base chunk: models.py\n",
        "    Merged sections: ['summary', 'models.py', 'ml_algs.py', '__init__.py']\n",
        "    Content length: 123458 characters\n",
        "    ==================================================\n",
        "    Sample 73:\n",
        "    Base chunk: model_comparison.py\n",
        "    Merged sections: ['summary', 'model_comparison.py', '__init__.py', 'Python Model: Batch Example']\n",
        "    Content length: 47587 characters\n",
        "    ==================================================\n",
        "    Sample 74:\n",
        "    Base chunk: namespaces.py\n",
        "    Merged sections: ['summary', 'namespaces.py', 'api.py', 'env.py']\n",
        "    Content length: 34890 characters\n",
        "    ==================================================\n",
        "    Sample 75:\n",
        "    Base chunk: pymodel.py\n",
        "    Merged sections: ['summary', 'pymodel.py', 'pytypes.py', 'types.py']\n",
        "    Content length: 27832 characters\n",
        "    ==================================================\n",
        "    Sample 76:\n",
        "    Base chunk: pytypes.py\n",
        "    Merged sections: ['summary', 'pytypes.py', 'pymodel.py', 'types.py']\n",
        "    Content length: 27832 characters\n",
        "    ==================================================\n",
        "    Sample 77:\n",
        "    Base chunk: types.py\n",
        "    Merged sections: ['summary', 'types.py', 'pytypes.py', 'pymodel.py']\n",
        "    Content length: 27832 characters\n",
        "    ==================================================\n",
        "    Sample 78:\n",
        "    Base chunk: udf.py\n",
        "    Merged sections: ['summary', 'udf.py', 'Feature Engineering - Python UDAF', 'Custom Evaluation Metric']\n",
        "    Content length: 38508 characters\n",
        "    ==================================================\n",
        "    Sample 79:\n",
        "    Base chunk: util.py\n",
        "    Merged sections: ['summary', 'util.py', 'internal.py', 'api.py']\n",
        "    Content length: 44631 characters\n",
        "    ==================================================\n",
        "    Sample 80:\n",
        "    Base chunk: config_pb2.py\n",
        "    Merged sections: ['summary', 'config_pb2.py', 'output_pb2.py', 'default_model_configs.py']\n",
        "    Content length: 93025 characters\n",
        "    ==================================================\n",
        "    Sample 81:\n",
        "    Base chunk: flink_pb2.py\n",
        "    Merged sections: ['summary', 'flink_pb2.py', 'sources_pb2.py', 'models.py']\n",
        "    Content length: 79630 characters\n",
        "    ==================================================\n",
        "    Sample 82:\n",
        "    Base chunk: flink_pb2_grpc.py\n",
        "    Merged sections: ['summary', 'flink_pb2_grpc.py', 'ml_service_pb2_grpc.py', 'flink_pb2.py']\n",
        "    Content length: 50545 characters\n",
        "    ==================================================\n",
        "    Sample 83:\n",
        "    Base chunk: input_pb2.py\n",
        "    Merged sections: ['summary', 'input_pb2.py', 'output_pb2.py', 'metrics_pb2.py']\n",
        "    Content length: 33800 characters\n",
        "    ==================================================\n",
        "    Sample 84:\n",
        "    Base chunk: metrics_pb2.py\n",
        "    Merged sections: ['summary', 'metrics_pb2.py', 'input_pb2.py', 'output_pb2.py']\n",
        "    Content length: 33800 characters\n",
        "    ==================================================\n",
        "    Sample 85:\n",
        "    Base chunk: ml_service_pb2.py\n",
        "    Merged sections: ['summary', 'ml_service_pb2.py', 'metrics_pb2.py', 'input_pb2.py']\n",
        "    Content length: 32880 characters\n",
        "    ==================================================\n",
        "    Sample 86:\n",
        "    Base chunk: ml_service_pb2_grpc.py\n",
        "    Merged sections: ['summary', 'ml_service_pb2_grpc.py', 'flink_pb2_grpc.py', 'ml_service_pb2.py']\n",
        "    Content length: 36781 characters\n",
        "    ==================================================\n",
        "    Sample 87:\n",
        "    Base chunk: output_pb2.py\n",
        "    Merged sections: ['summary', 'output_pb2.py', 'input_pb2.py', 'metrics_pb2.py']\n",
        "    Content length: 33800 characters\n",
        "    ==================================================\n",
        "    Sample 88:\n",
        "    Base chunk: sources_pb2.py\n",
        "    Merged sections: ['summary', 'sources_pb2.py', 'sources_p2p.py', 'flink_pb2.py']\n",
        "    Content length: 56896 characters\n",
        "    ==================================================\n",
        "    Sample 89:\n",
        "    Base chunk: sources_p2p.py\n",
        "    Merged sections: ['summary', 'sources_p2p.py', 'sources_pb2.py', 'input_pb2.py']\n",
        "    Content length: 43902 characters\n",
        "    =================================================="
      ],
      "metadata": {
        "id": "x3U5o574a2HE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continued to Colab to save money"
      ],
      "metadata": {
        "id": "DxWFwOb_bC5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-google-genai"
      ],
      "metadata": {
        "id": "2nggZj8gxPXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, HarmBlockThreshold, HarmCategory\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field"
      ],
      "metadata": {
        "id": "u_zBn2aHbGqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Define Pydantic Model for Structured Output"
      ],
      "metadata": {
        "id": "vi9v5MOGxfsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pydantic Model for Structured Output\n",
        "class GeneratedQuestions(BaseModel):\n",
        "    questions: list[str] = Field(\n",
        "        description=\"List of 10 to 15 generated questions about TurboML, covering code implementation, debugging, architecture, and optimization\",\n",
        "        min_items=2,  # Use min_items instead of min_length for lists\n",
        "        max_items=45  # Use max_items instead of max_length for lists\n",
        "    )"
      ],
      "metadata": {
        "id": "0RKmsA1fxfCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Load Merged Contexts"
      ],
      "metadata": {
        "id": "oVIDFCnxxmHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"merged_contexts.json\", \"r\") as f:\n",
        "    merged_contexts = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(merged_contexts)} merged contexts\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqScHdlgxkI_",
        "outputId": "07cc861e-d596-41ea-ad23-0ac6c7550a2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 89 merged contexts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Initialize Gemini Model with Safety Settings"
      ],
      "metadata": {
        "id": "kDkpOqHBxq3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "PwQG9hsGyT9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=1,  # High for diversity\n",
        "    max_retries=3,\n",
        "    safety_settings={\n",
        "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "    }\n",
        ")\n",
        "# Apply structured output directly to the LLM\n",
        "structured_llm = llm.with_structured_output(GeneratedQuestions)"
      ],
      "metadata": {
        "id": "l5zjp9FUxoLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Define Prompt Template"
      ],
      "metadata": {
        "id": "P68sF0iFzKNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create prompt template\n",
        "# question_prompt = ChatPromptTemplate.from_messages([\n",
        "#     (\"system\", \"\"\"You are a TurboML expert creating training data. Generate 25-30 questions each unique from each other, from the context below:\n",
        "\n",
        "# {context}\n",
        "\n",
        "# Guidelines:\n",
        "# 1. 40% code implementation (Python examples)\n",
        "# 2. 30% debugging/troubleshooting\n",
        "# 3. 20% architecture decisions\n",
        "# 4. 10% performance optimization\n",
        "# 5. Vary question types (how, why, compare, fix-this-code)\n",
        "# 6. Include specific class/method names from context\"\"\"),\n",
        "#     (\"human\", \"Generate questions now. Return exactly 25-30 questions.\")\n",
        "# ])"
      ],
      "metadata": {
        "id": "W5KD0y_byeWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"# TurboML Question Generation Task\n",
        "You are a senior ML engineer creating high-quality training data for TurboML experts. Generate **10-15 distinct, technical questions** based EXCLUSIVELY on this context:\n",
        "\n",
        "# Context\n",
        "{context}\n",
        "\n",
        "## Question Requirements\n",
        "1. **Technical Depth**:\n",
        "   - 40% Code Implementation:\n",
        "     - \"Write Python code to [...] using <SpecificClass> from context\"\n",
        "     - \"Implement a <FeatureType> that combines [...]\"\n",
        "     - Example: \"Write code to deploy an ONNX model with custom preprocessing using TurboML's PyTorch integration\"\n",
        "\n",
        "2. **Troubleshooting** (30%):\n",
        "   - Include actual error patterns from ML systems:\n",
        "     - \"Why does <Class.method()> throw <SpecificError> when [...]?\"\n",
        "     - \"How to debug <Symptom> in <Component>?\"\n",
        "     - Example: \"My LeveragingBaggingClassifier fails with 'Input dimension mismatch' - how to resolve?\"\n",
        "\n",
        "3. **Architecture** (20%):\n",
        "   - Compare system components:\n",
        "     - \"When would you choose <ComponentA> over <ComponentB> for [...]?\"\n",
        "     - \"Design a pipeline that [...]\"\n",
        "     - Example: \"Architect a real-time fraud detection system using TurboML's streaming features\"\n",
        "\n",
        "4. **Performance** (10%):\n",
        "   - Focus on measurable optimizations:\n",
        "     - \"Reduce latency of <Operation> by [...]\"\n",
        "     - \"Optimize memory usage for <UseCase>\"\n",
        "     - Example: \"Improve inference speed for AdaptiveXGBoost on high-cardinality categorical data\"\n",
        "\n",
        "## Diversity Guidelines (for 10-15 questions):\n",
        "- 4-6 code implementation questions\n",
        "- 3-5 troubleshooting questions\n",
        "- 2-3 architecture questions\n",
        "- 1-2 performance questions\n",
        "\n",
        "## Strict Guidelines\n",
        "\n",
        "** Avoid hallucination at all cost, don't ask any question that cannot be solved by the data in the context. **\n",
        "\n",
        "- **Context Compliance**: Questions MUST be answerable USING ONLY THE PROVIDED CONTEXT\n",
        "- **Terminology**: Use EXACT class/method names (e.g., `FeatureEngineering.create_sql_features()`)\n",
        "- **Question Types**:\n",
        "  1. How-to implementation\n",
        "  2. Why-based reasoning\n",
        "  3. Comparative analysis\n",
        "  4. Debugging scenarios\n",
        "  5. Optimization challenges\n",
        "  6. Architecture diagrams → mermaid code\n",
        "  7. Error message interpretation\n",
        "\n",
        "\"\"\"),\n",
        "(\"human\", \"\"\"Generate 10-15 questions following ALL requirements above.\n",
        "Context Sections Available: {section_list}\"\"\")\n",
        "])"
      ],
      "metadata": {
        "id": "wHxmwbBGBNOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Generate Questions for Each Context"
      ],
      "metadata": {
        "id": "758N4bI041fG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to store successful and failed contexts\n",
        "all_questions = []\n",
        "failed_contexts = []  # Track failed context indices for retry\n",
        "\n",
        "# Process all contexts\n",
        "for idx, context in enumerate(merged_contexts):\n",
        "    try:\n",
        "        print(f\"Processing context {idx+1}/{len(merged_contexts)}\")\n",
        "\n",
        "        # Format the prompt\n",
        "        formatted_prompt = question_prompt.format_messages(\n",
        "            context=context[\"content\"],\n",
        "            section_list=context[\"context_sections\"]\n",
        "        )\n",
        "\n",
        "        # Generate questions with structured output\n",
        "        start_time = time.time()\n",
        "        response = structured_llm.invoke(formatted_prompt)\n",
        "\n",
        "        # Verify response\n",
        "        if response is None or not hasattr(response, 'questions'):\n",
        "            raise ValueError(f\"Invalid response object: {response}\")\n",
        "\n",
        "        # Store with metadata\n",
        "        all_questions.append({\n",
        "            \"context_id\": idx,\n",
        "            \"base_sections\": context[\"context_sections\"],\n",
        "            \"questions\": response.questions,\n",
        "            \"generation_time\": time.time() - start_time\n",
        "        })\n",
        "\n",
        "        print(f\"Successfully generated {len(response.questions)} questions\")\n",
        "\n",
        "        # Rate limiting\n",
        "        time.sleep(0.1)  # Avoid hitting rate limits\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing context {idx}: {str(e)}\")\n",
        "\n",
        "        # Add to failed contexts list\n",
        "        failed_contexts.append({\n",
        "            \"index\": idx,\n",
        "            \"context\": context,\n",
        "            \"error\": str(e),\n",
        "            \"timestamp\": time.time()\n",
        "        })\n",
        "\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "\n",
        "        # Try to print the raw response for debugging\n",
        "        try:\n",
        "            raw_response = llm.invoke(formatted_prompt)\n",
        "            print(f\"Raw LLM response type: {type(raw_response)}\")\n",
        "            print(f\"Raw LLM response (first 300 chars): {raw_response.content[:300]}...\")\n",
        "        except Exception as debug_e:\n",
        "            print(f\"Failed to get raw response: {str(debug_e)}\")\n",
        "\n",
        "        # Continue with next context\n",
        "        continue\n",
        "\n",
        "print(f\"Generated {sum(len(q['questions']) for q in all_questions)} questions from {len(all_questions)} contexts\")\n",
        "print(f\"Failed to generate questions for {len(failed_contexts)} contexts\")"
      ],
      "metadata": {
        "id": "yPJbWLDl5Y7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d54e6e49-ed46-48cf-ab97-05e867ce7242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing context 1/89\n",
            "Successfully generated 14 questions\n",
            "Processing context 2/89\n",
            "Successfully generated 16 questions\n",
            "Processing context 3/89\n",
            "Successfully generated 16 questions\n",
            "Processing context 4/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 5/89\n",
            "Successfully generated 22 questions\n",
            "Processing context 6/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 7/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 8/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 9/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 10/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 11/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 12/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 13/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 14/89\n",
            "Successfully generated 16 questions\n",
            "Processing context 15/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 16/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 17/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 18/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 19/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 20/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 21/89\n",
            "Successfully generated 16 questions\n",
            "Processing context 22/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 23/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 24/89\n",
            "Successfully generated 18 questions\n",
            "Processing context 25/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 26/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 27/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 28/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 29/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 30/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 31/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 32/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 33/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 34/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 35/89\n",
            "Successfully generated 16 questions\n",
            "Processing context 36/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 37/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 38/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 39/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 40/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 41/89\n",
            "Successfully generated 14 questions\n",
            "Processing context 42/89\n",
            "Successfully generated 16 questions\n",
            "Processing context 43/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 44/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 45/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 46/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 47/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 48/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 49/89\n",
            "Successfully generated 14 questions\n",
            "Processing context 50/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 51/89\n",
            "Successfully generated 13 questions\n",
            "Processing context 52/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 53/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 54/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 55/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 56/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 57/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 58/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 59/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 60/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 61/89\n",
            "Successfully generated 16 questions\n",
            "Processing context 62/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 63/89\n",
            "Successfully generated 14 questions\n",
            "Processing context 64/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 65/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 66/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 67/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 68/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 69/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 70/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 71/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 72/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 73/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 74/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 75/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 76/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 77/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 78/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 79/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 80/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 81/89\n",
            "Successfully generated 14 questions\n",
            "Processing context 82/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 83/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 84/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 85/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 86/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 87/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 88/89\n",
            "Successfully generated 15 questions\n",
            "Processing context 89/89\n",
            "Successfully generated 15 questions\n",
            "Generated 1345 questions from 89 contexts\n",
            "Failed to generate questions for 0 contexts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save progress\n",
        "# with open(\"generated_questions.json\", \"w\") as f:\n",
        "#     json.dump(all_questions, f, indent=2)\n",
        "\n",
        "with open(\"generated_questions_smaller_set.json\", \"w\") as f:\n",
        "    json.dump(all_questions, f, indent=2)\n",
        "\n",
        "# # Save failed contexts for later retry\n",
        "# with open(\"failed_contexts.json\", \"w\") as f:\n",
        "#     json.dump(failed_contexts, f, indent=2)"
      ],
      "metadata": {
        "id": "J6r7GHCK-k0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retry failed contexts\n",
        "def retry_failed_contexts():\n",
        "    # Load the failed contexts\n",
        "    try:\n",
        "        with open(\"failed_contexts.json\", \"r\") as f:\n",
        "            failed_contexts = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(\"No failed contexts file found.\")\n",
        "        return\n",
        "\n",
        "    if not failed_contexts:\n",
        "        print(\"No failed contexts to retry.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Retrying {len(failed_contexts)} failed contexts...\")\n",
        "\n",
        "    # Load existing results\n",
        "    try:\n",
        "        with open(\"generated_questions.json\", \"r\") as f:\n",
        "            all_questions = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        all_questions = []\n",
        "\n",
        "    newly_succeeded = []\n",
        "    still_failed = []\n",
        "\n",
        "    # Retry each failed context\n",
        "    for failed_item in failed_contexts:\n",
        "        idx = failed_item[\"index\"]\n",
        "        context = failed_item[\"context\"]\n",
        "\n",
        "        print(f\"Retrying context {idx+1}/{len(merged_contexts)}\")\n",
        "\n",
        "        try:\n",
        "            # Format the prompt\n",
        "            formatted_prompt = question_prompt.format_messages(\n",
        "                context=context[\"content\"],\n",
        "                section_list=context[\"context_sections\"]\n",
        "            )\n",
        "\n",
        "            # Generate questions with structured output\n",
        "            start_time = time.time()\n",
        "            response = structured_llm.invoke(formatted_prompt)\n",
        "\n",
        "            if response is None or not hasattr(response, 'questions'):\n",
        "                raise ValueError(f\"Invalid response object in retry: {response}\")\n",
        "\n",
        "            # Add to successful list\n",
        "            all_questions.append({\n",
        "                \"context_id\": idx,\n",
        "                \"base_sections\": context[\"context_sections\"],\n",
        "                \"questions\": response.questions,\n",
        "                \"generation_time\": time.time() - start_time,\n",
        "                \"retry\": True\n",
        "            })\n",
        "\n",
        "            newly_succeeded.append(idx)\n",
        "            print(f\"Successfully generated {len(response.questions)} questions on retry\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Still failed on retry for context {idx}: {str(e)}\")\n",
        "            still_failed.append({\n",
        "                \"index\": idx,\n",
        "                \"context\": context,\n",
        "                \"error\": str(e),\n",
        "                \"timestamp\": time.time(),\n",
        "                \"retry_attempted\": True\n",
        "            })\n",
        "\n",
        "        # Rate limiting\n",
        "        time.sleep(5)\n",
        "\n",
        "    # Update files\n",
        "    with open(\"generated_questions.json\", \"w\") as f:\n",
        "        json.dump(all_questions, f, indent=2)\n",
        "\n",
        "    if still_failed:\n",
        "        with open(\"failed_contexts_after_retry.json\", \"w\") as f:\n",
        "            json.dump(still_failed, f, indent=2)\n",
        "\n",
        "    print(f\"Retry results: {len(newly_succeeded)} succeeded, {len(still_failed)} still failed\")\n",
        "    return newly_succeeded, still_failed"
      ],
      "metadata": {
        "id": "wlOIPUOCGLY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this after the initial processing completes\n",
        "print(\"Starting retry process for failed contexts...\")\n",
        "newly_succeeded, still_failed = retry_failed_contexts()\n",
        "\n",
        "# Final summary\n",
        "with open(\"generated_questions_smaller_set.json\", \"r\") as f:\n",
        "    final_questions = json.load(f)\n",
        "\n",
        "print(\"\\nFinal results:\")\n",
        "print(f\"Total successful contexts: {len(final_questions)}\")\n",
        "print(f\"Total questions generated: {sum(len(q['questions']) for q in final_questions)}\")\n",
        "print(f\"Contexts that still failed: {len(still_failed)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne-y64hWHiEp",
        "outputId": "20474325-bb89-4a79-922a-99c34b614f64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final results:\n",
            "Total successful contexts: 89\n",
            "Total questions generated: 1345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now the important as well as time - consuming part - `Phase 3: Answer Generation with Full KnowledgeBase Context`"
      ],
      "metadata": {
        "id": "C-d8FlAnSu8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "import re\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "qIkVaJ3dHxla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/knowledge_base.md\", \"r\", encoding=\"utf-8\") as f:\n",
        "    full_kb = f.read()\n",
        "\n",
        "print(f\"Loaded knowledge base ({len(full_kb)} characters)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyK4ixCPTFGR",
        "outputId": "e62f947b-cec5-4bf9-d2fc-aa07e2862b04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded knowledge base (619144 characters)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load generated questions\n",
        "with open(\"generated_questions.json\", \"r\") as f:\n",
        "    all_questions = json.load(f)\n",
        "\n",
        "print(f\"Loaded {sum(len(ctx['questions']) for ctx in all_questions)} total questions\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUuXvFofTZRA",
        "outputId": "ed701647-7dee-438b-b7ac-4e5aa9080d6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2608 total questions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Gemini\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-pro-exp-02-05\",\n",
        "    temperature=0.1,\n",
        "    max_retries=3,\n",
        "    safety_settings={\n",
        "        \"HARM_CATEGORY_DANGEROUS_CONTENT\": \"BLOCK_NONE\",\n",
        "        \"HARM_CATEGORY_HATE_SPEECH\": \"BLOCK_NONE\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "7k6nhw4gTrG_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}