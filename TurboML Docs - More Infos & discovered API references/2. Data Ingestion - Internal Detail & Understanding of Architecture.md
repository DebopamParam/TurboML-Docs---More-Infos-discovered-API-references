![image](https://gist.github.com/user-attachments/assets/3e298ae1-eed3-46af-a0db-ebb68da28456)

### Knowing more about `tb.OnlineDataset`
In Notebook execute
```
help(tb.OnlineDataset)
```
Explanation of the output of Help:
---

## **1. Class Overview**
### **Class Name:** `OnlineDataset`
- **Module:** `turboml.common.datasets`
- **Base Class:** `builtins.object`
- **Purpose:** Represents a dataset managed and stored by the TurboML platform.
- **Key Features:**
  - Supports operations available on `LocalDataset`
  - Enables materialization of engineered features
  - Supports drift monitoring and registration
  - Can be used to serve models based on the stored data

---

## **2. Constructor (`__init__` Method)**
### **Signature:**
```python
__init__(self, dataset_id: str, init_key: object, key_field: str, 
         protobuf_cls: type[message.Message], 
         registered_schema: RegisteredSchema, 
         fe: LocalFeatureEngineering | None = None)
```
### **Parameters:**
- `dataset_id (str)`: Unique identifier for the dataset.
- `init_key (object)`: Initialization key for dataset creation.
- `key_field (str)`: Primary key field of the dataset.
- `protobuf_cls (type[message.Message])`: Class for handling protobuf messages.
- `registered_schema (RegisteredSchema)`: Schema registered for the dataset.
- `fe (LocalFeatureEngineering | None)`: Optional feature engineering instance.

---

## **3. Methods**
### **Instance Methods**
| Method | Description |
|--------|-------------|
| `__repr__()` | Returns a string representation of the object. |
| `add_pd(df: pd.DataFrame)` | Adds a Pandas DataFrame to the dataset. |
| `add_row_dict(row: dict)` | Adds a single row (dictionary format) to the dataset. |
| `get_model_inputs(numerical_fields, categorical_fields, textual_fields, imaginal_fields, time_field)` | Retrieves model input features based on specified field types. |
| `get_model_labels(label_field: str)` | Retrieves model labels based on the given field name. |
| `get_multivariate_drift(label, numerical_fields, limit=-1)` | Computes multivariate drift for numerical fields. |
| `get_univariate_drift(label, numerical_field, limit=-1)` | Computes univariate drift for a numerical field. |
| `register_multivariate_drift(numerical_fields, label)` | Registers multivariate drift monitoring for specified fields. |
| `register_univariate_drift(numerical_field, label=None)` | Registers univariate drift monitoring for a single field. |
| `sync_features()` | Synchronizes engineered features with the dataset. |
| `to_ibis()` | Converts the dataset into an Ibis table. |

---

### **Static Methods**
| Method | Description |
|--------|-------------|
| `from_local_dataset(dataset: LocalDataset, dataset_id: str, load_if_exists: bool = False) -> OnlineDataset` | Creates an `OnlineDataset` from a `LocalDataset`. |
| `from_pd(df: pd.DataFrame, id: str, key_field: str, load_if_exists: bool = False) -> OnlineDataset` | Creates an `OnlineDataset` from a Pandas DataFrame. |
| `load(dataset_id: str) -> OnlineDataset | None` | Loads an existing `OnlineDataset` by ID. |

---

## **4. Properties**
### **Readonly Properties**
| Property | Description |
|----------|-------------|
| `feature_engineering` | Returns the feature engineering instance. |
| `key_field` | Returns the primary key field of the dataset. |
| `preview_df` | Provides a preview of the dataset as a Pandas DataFrame. |
| `schema` | Returns the registered schema of the dataset. |

---

## **5. Data Descriptors**
| Descriptor | Description |
|------------|-------------|
| `__dict__` | Dictionary storing instance variables. |
| `__weakref__` | List of weak references to the object. |

---

### **Summary**
- The `OnlineDataset` class is a key component of TurboML, designed for managing datasets in an online environment.
- It provides methods to:
  - Add and retrieve data
  - Register and compute feature drift
  - Sync features and serve models
  - Convert datasets into an Ibis table
- It has static methods to create datasets from Pandas DataFrames or local datasets.



# **Understanding Drift in Machine Learning**  

Drift refers to changes in data distribution over time, which can negatively impact model performance. It occurs when the statistical properties of input features or labels change between training and production (real-world inference).  

There are **two main types of drift**:  

---

## **1. Univariate Drift**  
Univariate drift occurs when **a single feature** (variable) experiences a shift in its distribution over time. It focuses on detecting changes in individual columns of data.  

### **Example of Univariate Drift:**
- In a fraud detection system, suppose the feature **"transaction amount"** had an average value of **$100** during model training.  
- However, in production, the average transaction amount rises to **$500**.  
- This indicates a drift in the **"transaction amount"** feature, meaning the model may need retraining.  

### **Methods to Detect Univariate Drift:**
- **Kolmogorov-Smirnov (KS) Test**: Measures the difference between two probability distributions.  
- **Population Stability Index (PSI)**: Quantifies the shift between training and current distributions.  
- **Z-score, Mean, and Variance Analysis**: Tracks statistical changes in feature distributions.  

---

## **2. Multivariate Drift**  
Multivariate drift occurs when **multiple features** change in a way that impacts their relationships with each other or with the target variable. It analyzes the joint distribution of multiple variables.  

### **Example of Multivariate Drift:**
- In a credit risk model, during training:
  - **"income"** and **"loan amount"** were positively correlated (higher income → higher loan).  
- In production:
  - The relationship changes due to economic shifts, where lower-income users are now applying for higher loans.  
- This could signal **multivariate drift**, affecting the model’s accuracy.  

### **Methods to Detect Multivariate Drift:**
- **Mahalanobis Distance**: Measures how different a new data point is from the original distribution.  
- **Jensen-Shannon Divergence (JSD)**: Measures how two probability distributions differ.  
- **PCA (Principal Component Analysis) and T-SNE**: Visualize feature distribution shifts in lower dimensions.  
- **Drift Detection via Autoencoders**: Neural networks that identify distributional shifts in multiple variables.  

---

### **Key Differences Between Univariate and Multivariate Drift**
| Feature | Univariate Drift | Multivariate Drift |
|---------|----------------|----------------|
| **Focus** | Single feature | Multiple features |
| **Detection** | Feature-by-feature | Joint feature interactions |
| **Examples** | Change in "Age" distribution | Change in "Age vs. Income" relationship |
| **Common Tests** | KS-Test, PSI, Z-score | Mahalanobis Distance, PCA, JSD |

---

### **How to Handle Drift**
1. **Monitor Data Regularly**: Track feature distributions over time.  
2. **Retrain the Model**: If drift is detected, retrain with updated data.  
3. **Feature Engineering Adjustments**: Modify how features are processed to reduce sensitivity to drift.  
4. **Adaptive Learning Models**: Use online learning models that adjust to new data distributions.  

---

### **Application in TurboML (`OnlineDataset`)**
- **`get_univariate_drift()`**: Checks drift in a **single feature**.  
- **`get_multivariate_drift()`**: Checks drift across **multiple features**.  
- **`register_univariate_drift()`**: Registers monitoring for a specific feature.  
- **`register_multivariate_drift()`**: Registers monitoring for multiple features together.

# After uploading lets go back to the console - and see the dataset that we just uploaded

![image](https://gist.github.com/user-attachments/assets/5d08ba15-1baf-42b4-9e6d-28eb232eceeb)
![image](https://gist.github.com/user-attachments/assets/ba73c1a6-575a-4a78-8bb2-ffa08d037fa1)
![image](https://gist.github.com/user-attachments/assets/bf0f7c1f-99d6-48fb-b76a-bc45f4f3b175)

## Understanfing the above images
**Understanding TurboML and Kafka**

Based on the text you provided, TurboML is a platform designed for real-time machine learning.  A key aspect is its ability to handle streaming data throughout the ML lifecycle.  The text also explicitly states: **"Additional Information: TurboML uses Kafka Internally."**

This is crucial. Kafka is a distributed streaming platform widely used for building real-time data pipelines and streaming applications.  TurboML leverages Kafka as a core component for:

* **Data Ingestion:**  Kafka acts as the backbone for ingesting data streams into TurboML. This is evident in the "inputs.qs_transactions" topic name, suggesting it's an input data stream.
* **Real-time Processing:** Kafka's pub/sub nature allows TurboML to process data in real-time for feature engineering, model training, and inference.
* **Data Storage (Potentially):** While TurboML might use other storage solutions like Iceberg (as hinted at by the consumer group name), Kafka itself can also store data for a configured retention period.

**Image Breakdown and Explanation**

Let's go through each image tab by tab:

**Image 1: Overview Tab**

This tab provides a high-level overview of the `inputs.qs_transactions` topic.

* **`name`**: `_turboml.inputs.qs_transactions` - This is the fully qualified name of the Kafka topic within TurboML. The `_turboml` prefix likely indicates it's managed internally by the TurboML platform. `inputs.qs_transactions` clearly suggests this topic is designed to receive transaction data as input, likely for the "Quick Start" (QS) example as indicated by `qs`.
* **`partitions`**: `1` - This topic is configured with **one partition**.  Partitions are fundamental to Kafka for parallelism and scalability. Having only one partition means:
    * **Limited Parallelism:**  Consumers reading from this topic will be limited to one consumer instance for optimal ordering within the partition.  You won't get parallel processing at the consumer level.
    * **Ordering Guarantee:** Kafka guarantees message ordering within a partition.  With one partition, messages are strictly ordered as they arrive.
* **`replication_factor`**: `1` - This topic has a **replication factor of 1**.  Replication is crucial for fault tolerance in Kafka. A replication factor of 1 means:
    * **No Redundancy:**  If the Kafka broker (server) that holds this partition fails, data might be lost, and the topic becomes unavailable until the broker recovers.  **This is NOT recommended for production environments.** In a real-world scenario, you'd want a replication factor of at least 2 or 3 for data durability.
* **`urp`**: `0` -  **Under-Replicated Partitions**. This is currently `0`, which is good. It means there are no partitions that are under-replicated. If this number was greater than 0, it would indicate a problem with replication, potentially due to broker failures.
* **`in_sync_replicas`**: `1` -  **In-Sync Replicas**. This is `1`, matching the replication factor. It means the single replica is currently in sync and healthy.
* **`total_replicas`**: `1` -  Again, this is `1`, reflecting the replication factor.
* **`cleanup_policy`**: `delete` - This is the **topic's data retention policy**. `delete` means that when data in the topic reaches a certain size or time limit (configured elsewhere, likely in the 'Settings' tab), older segments of data will be deleted to free up space. This is a common policy for topics that hold transient or event-based data.
* **`segment_size`**: `1073741824` (1 GB) - This is the **maximum size of each segment file** on disk for this topic. Kafka stores topic data in segments. When a segment reaches this size, Kafka rolls over to a new segment.
* **`segment_count`**: `10` - This indicates there are currently **10 segment files** for this topic. This, combined with the `segment_size`, gives you an idea of the current storage used by the topic (roughly 10 GB in this case, although it might be less if segments aren't full).

**Image 2: Consumers Tab**

This tab shows the consumers currently active and reading from the `inputs.qs_transactions` topic.

* **`Consumer Group ID`**: `kafka_iceberg_ingest` - This is the name of the **consumer group**. In Kafka, consumers are organized into groups.  Key points about consumer groups:
    * **Scalability and Parallelism:**  Within a consumer group, Kafka distributes partitions among the consumers. In this case, with only one partition, only one consumer within this group will be actively reading at a time.
    * **Offset Management:** Kafka tracks the progress of each consumer group by storing the "offset" (position) within each partition. This allows consumers to resume reading from where they left off, even after failures or restarts.
    * **`kafka_iceberg_ingest` Naming:** The name strongly suggests this consumer group is responsible for ingesting data from this Kafka topic into an Apache Iceberg data lake or table within TurboML. Iceberg is a popular open-source data lake format, often used for large-scale analytical workloads.
* **`Active Consumers`**: `1` -  There is **one active consumer instance** within the `kafka_iceberg_ingest` group. This is expected given there's only one partition in the topic.
* **`State`**: `STABLE` - The consumer group's state is `STABLE`. This is a good sign. It means the consumer group is healthy, and partition assignment is stable (consumers are consistently assigned to partitions). Other states might include `REBALANCING` (during consumer group membership changes).

**Images 3 & 4: Settings Tab**

This tab displays detailed Kafka topic configuration settings as key-value pairs. These settings control various aspects of the topic's behavior. Let's highlight some important ones:

* **`compression.type`**: `producer` -  **Compression is enabled at the producer level**. This is a good practice.  Messages are compressed by the producer before being sent to Kafka. Common compression types are `gzip`, `snappy`, `lz4`.  Producer-side compression saves bandwidth and storage space.
* **`leader.replication.throttled.replicas` & `follower.replication.throttled.replicas`**:  These settings are related to **replication throttling**.  They are likely set to default or not explicitly configured in this basic setup. Throttling is used to control the bandwidth used by replication to avoid overwhelming the network or brokers.
* **`remote.storage.enable`**:  This setting might be specific to TurboML's infrastructure or Kafka distribution. It could relate to tiering data to remote storage (like cloud storage) for cost optimization, but without more context, it's hard to say definitively.
* **`message.downconversion.enable`**:  This setting is likely related to Kafka's message format evolution. It might control whether message format conversion is enabled if consumers are using older Kafka clients than the brokers.
* **`min.insync.replicas`**: `1` -  **Minimum In-Sync Replicas**. This is set to `1`.  Combined with the replication factor of 1, this means that even if the single replica is not in sync, the producer will still be able to write to the topic. **This setting should be carefully considered in production with higher replication factors.**  Setting it higher (e.g., to the replication factor minus 1) provides stronger data durability guarantees but can reduce write availability if replicas become out of sync.
* **`segment.jitter.ms`**: `-2` - This is likely a special value or default. `segment.jitter.ms` is used to add randomness to segment rolling to avoid all partitions rolling segments simultaneously.  A negative value might indicate it's disabled or using a default internal value.
* **`local.retention.ms`**: `-2` -  Similar to `segment.jitter.ms`, a negative value might indicate a default or platform-managed setting for local retention.  Local retention might refer to retention policies on the broker's local disk before data is potentially tiered to remote storage (if `remote.storage.enable` is relevant).
* **`cleanup.policy`**: `delete` -  Reiterates the `delete` cleanup policy seen in the Overview tab.
* **`flush.ms` & `flush.messages`**: These settings control how frequently Kafka **flushes data to disk**.  Lower values increase data durability (less data in memory if a broker fails) but can decrease write throughput. High values improve throughput but slightly reduce durability. The very large values (9223372036854776000) suggest these settings are effectively disabled or set to very infrequent flushing, likely prioritizing throughput in this setup.
* **`segment.bytes`**: `1073741824` -  Again, the segment size (1 GB) is confirmed here.
* **`retention.ms`**: `604800000` (7 days) -  **Data retention is set to 7 days (604,800,000 milliseconds)** based on time.  Data older than 7 days will be eligible for deletion based on the `cleanup.policy`.
* **`flush.messages`**: `9223372036854776000` -  As mentioned earlier, this very high value effectively disables message-based flushing, making time-based flushing (`flush.ms` if enabled and not similarly high) or segment rolling (`segment.bytes`) the primary triggers for disk synchronization.
* **`message.format.version`**: `3.0-IV1` - This indicates the **Kafka message format version** being used.  `3.0-IV1` likely refers to a specific version related to Kafka 3.0 or an internal TurboML version based on Kafka 3.0. Message format versions evolve in Kafka to introduce new features and optimizations.
* **`max.compaction.lag.ms` & `min.compaction.lag.ms`**: These settings are related to **Kafka log compaction**. Log compaction is a feature that can be enabled on Kafka topics to retain only the latest value for each key, effectively creating a key-value store within Kafka. These settings control the timing and eligibility of log compaction. They are likely set to default or not actively used for this `inputs.qs_transactions` topic, as compaction is typically used for stateful data, not raw event streams.
* **`file.delete.delay.ms`**: `60000` (60 seconds) -  This is the **delay before Kafka physically deletes segment files** after they are marked for deletion due to retention policies. It's a safety mechanism to allow for potential recovery or auditing.
* **`max.message.bytes`**: `1048588` (approximately 1 MB) - This is the **maximum size of a single message** that can be sent to this topic.  It's a standard Kafka setting to prevent excessively large messages from overwhelming the system.
* **`min.compaction.lag.ms`**: `0` -  Again, related to log compaction, likely set to a default value.
* **`message.timestamp.type`**: `Createtime` -  **Message timestamps are based on creation time (when the message is received by the broker)**.  The alternative is `LogAppendTime` (when the message is appended to the broker's log). `Createtime` is generally used when the timestamp is set by the producer and should be preserved.
* **`local.retention.ms`**: `-2` - Repeated setting, likely a configuration detail or default value.
* **`preallocate`**:  Likely a boolean setting (not shown as value here).  `preallocate` controls whether Kafka should pre-allocate disk space for segment files. Pre-allocation can improve write performance by reducing disk fragmentation but might consume disk space even if not fully used.
* **`index.interval.bytes`**: `4096` (4 KB) - This setting controls the **frequency of index markers in Kafka index files**. Indexes help Kafka quickly locate messages within segments.
* **`min.cleanable.dirty.ratio`**: `0.5` -  This setting is related to **log compaction**. It specifies the minimum ratio of "dirty" (outdated or compacted) data in a log segment before it becomes eligible for compaction.
* **`unclean.leader.election.enable`**:  Likely a boolean setting (not shown as value here). **`unclean.leader.election.enable` is a critical Kafka setting related to data consistency vs. availability.**  If enabled (which is often the default for older Kafka versions, but usually disabled in production now), it allows a non-in-sync replica to become the leader in case of a leader failure. This can lead to data loss if the out-of-sync replica is significantly behind.  **Disabling it (recommended for data consistency) prioritizes data consistency over availability.** In that case, if no in-sync replica is available to become leader, the partition becomes unavailable until an in-sync replica is restored.
* **`retention.bytes`**: `-1` - **Data retention based on size is disabled** (`-1` typically means unlimited or disabled).  Retention is solely controlled by `retention.ms` (time-based retention) in this configuration.
* **`delete.retention.ms`**: `86400000` (1 day) - This is the **delay before Kafka actually deletes log segments** that are marked for deletion due to retention policies. It's another safety buffer, often set to 24 hours.
* **`message.timestamp.after.max.ms` & `message.timestamp.before.max.ms` & `message.timestamp.difference.max.ms`**: These settings are related to **message timestamp validation**. They control how Kafka handles messages with timestamps that are too far in the future or past, or have too large a difference from the broker's time. They are likely set to very large values (9223372036854776000) effectively disabling these timestamp validations in this configuration.
* **`segment.ms`**:  Likely missing a value in the image. `segment.ms` would control **time-based segment rolling** (rolling to a new segment after a certain time, in addition to `segment.bytes`).
* **`segment.index.bytes`**: `10485760` (10 MB) - This is the **maximum size of the index file** for each segment.

**Summary**

In essence, these images show you the configuration of a Kafka topic named `inputs.qs_transactions` within the TurboML platform.  This topic is designed to receive transaction data and appears to be part of a Quick Start example.

**Key takeaways:**

* **Kafka as the Foundation:** TurboML relies heavily on Kafka for data ingestion and real-time processing.
* **Basic Topic Configuration:** The topic configuration shown is relatively basic, with only one partition and a replication factor of 1, likely for development or demonstration purposes. **Production setups would require higher replication and potentially more partitions for scalability and fault tolerance.**
* **Data Ingestion Pipeline:** The consumer group `kafka_iceberg_ingest` indicates that data from this Kafka topic is being ingested into an Iceberg data lake within TurboML for further processing and analysis.
* **Retention Policy:** The topic uses a `delete` cleanup policy with a 7-day time-based retention.
* **Detailed Settings:** The 'Settings' tab provides a comprehensive view of Kafka topic configurations, allowing fine-grained control over topic behavior.

By understanding these images and the underlying Kafka concepts, you gain insight into how TurboML manages and processes real-time data streams for machine learning.